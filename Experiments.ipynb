{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8ca520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "0\n",
      "Random Init Utilized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "loss: tensor(350.0442, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(49.1581, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "2\n",
      "loss: tensor(297.2805, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(16.6147, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "3\n",
      "loss: tensor(276.6905, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(13.7142, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "4\n",
      "loss: tensor(256.4194, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(12.5797, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "5\n",
      "loss: tensor(239.7066, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(13.1582, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "6\n",
      "loss: tensor(219.7606, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(12.9421, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "7\n",
      "loss: tensor(201.7312, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(14.5659, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "8\n",
      "loss: tensor(185.6647, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(13.3476, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "9\n",
      "loss: tensor(173.3830, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(18.6385, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "10\n",
      "loss: tensor(151.2297, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(13.0648, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "Component 0 Accuracy: 901.0/1000\n",
      "Component 1 Accuracy: 907.0/1000\n",
      "Component 2 Accuracy: 895.0/1000\n",
      "Component 3 Accuracy: 893.0/1000\n",
      "Component 4 Accuracy: 897.0/1000\n",
      "Component 5 Accuracy: 895.0/1000\n",
      "Component 6 Accuracy: 888.0/1000\n",
      "Component 7 Accuracy: 906.0/1000\n",
      "Component 8 Accuracy: 900.0/1000\n",
      "Component 9 Accuracy: 895.0/1000\n",
      "Posterior Mean Accuracy: 912.0/1000\n",
      "Ensemble Accuracy: 906/1000\n"
     ]
    }
   ],
   "source": [
    "#matplotlib inline\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "\n",
    "import importlib\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPUs are used!\")\n",
    "else:\n",
    "    print(\"CPUs are used!\")\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 50\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "\n",
    "#The net does not like to get larger at a given layer??\n",
    "l1shape=(256, 10)\n",
    "l2shape=(10, 10)\n",
    "l3shape=(10, 10)\n",
    "l4shape=(10, 5)\n",
    "layershapes = [l1shape, l2shape, l3shape, l4shape]\n",
    "\n",
    "epochs = 10\n",
    "trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net = FVMF.BayesianNetwork(layershapes=layershapes,BN='notbatchnorm',VD='Gaussian',\n",
    "                               dtrain=dtrain,dtest=dtest,BATCH_SIZE = 100,classification = 'classification').to(DEVICE)\n",
    "    #net = VMF.BayesianNetwork(l1=l1shape, l2=l2shape, l3=l3shape,l4=l4shape,BN='notbatchnorm').to(DEVICE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.007)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,shape = (0,256,256,257))\n",
    "        #print(net.l1.weight_mu.mean())\n",
    "\n",
    "    res = test_ensemble.test_ensemble(net,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = (0,256,256,257))\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f26701",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mu_nodewise = []\n",
    "b_mu = []\n",
    "for i,layer in enumerate(net.layers):\n",
    "    w_mu_nodewise += [layer.weight_mu]\n",
    "    b_mu += [layer.bias_mu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0470924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "FVMF RELOADED\n",
      "0\n",
      "Random Init Utilized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "loss: tensor(208.9301, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(53.8483, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(4.3046, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.0372, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "2\n",
      "loss: tensor(198.2865, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(17.0525, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(4.9646, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(17.5934, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "3\n",
      "loss: tensor(169.0435, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(19.6990, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(5.8137, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(18.7951, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "4\n",
      "loss: tensor(159.2988, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(15.4887, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(6.0600, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(19.6949, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "5\n",
      "loss: tensor(151.0041, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(14.3832, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(5.7620, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(20.1119, device='cuda:1', grad_fn=<NormBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 98\u001b[0m\n\u001b[1;32m     93\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(net2\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.14\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 98\u001b[0m     trtimes[epoch] \u001b[38;5;241m=\u001b[39m \u001b[43mFVMF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAMPLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m257\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax:\u001b[39m\u001b[38;5;124m'\u001b[39m,net2\u001b[38;5;241m.\u001b[39mweight_mu[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm:\u001b[39m\u001b[38;5;124m'\u001b[39m,torch\u001b[38;5;241m.\u001b[39mnorm(net2\u001b[38;5;241m.\u001b[39mweight_mu[\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[0;32m~/projects/BNN/FVMF.py:845\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, dtrain, SAMPLES, optimizer, epoch, i, shape, BATCH_SIZE, CLASSES)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;66;03m#print(target)\u001b[39;00m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;66;03m#print('\\n','target:',target,'\\n')\u001b[39;00m\n\u001b[1;32m    844\u001b[0m net\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 845\u001b[0m loss, log_prior, log_variational_posterior, negative_log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_elbo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNUM_BATCHES\u001b[49m\u001b[43m,\u001b[49m\u001b[43mSAMPLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43mCLASSES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m#start = time.time()\u001b[39;00m\n\u001b[1;32m    847\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/projects/BNN/FVMF.py:776\u001b[0m, in \u001b[0;36mBayesianNetwork.sample_elbo\u001b[0;34m(self, input, target, NUM_BATCHES, samples, CLASSES)\u001b[0m\n\u001b[1;32m    774\u001b[0m log_variational_posteriors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(samples)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(samples):\n\u001b[0;32m--> 776\u001b[0m     outputs[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m     log_priors[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_prior()\n\u001b[1;32m    778\u001b[0m     log_variational_posteriors[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_variational_posterior()\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/BNN/FVMF.py:745\u001b[0m, in \u001b[0;36mBayesianNetwork.forward\u001b[0;34m(self, x, sample)\u001b[0m\n\u001b[1;32m    743\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, viewstop)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 745\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    746\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/BNN/FVMF.py:618\u001b[0m, in \u001b[0;36mvMF_NodeWise.forward\u001b[0;34m(self, input, sample, calculate_log_probs)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_variational_posterior \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mlog_prob(bias)\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features):\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;66;03m#norm_w_mu = torch.distributions.normal.Normal(1,0.5).log_prob(norm(self.weight_mu[i]))[0].exp()\u001b[39;00m\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;66;03m#print('norm_w_mu:',norm_w_mu,'\\n')\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_variational_posterior \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_prior,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_variational_posterior \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/projects/BNN/FVMF.py:231\u001b[0m, in \u001b[0;36mvMF.log_prob\u001b[0;34m(self, x, utc)\u001b[0m\n\u001b[1;32m    229\u001b[0m     logliks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkappa \u001b[38;5;241m*\u001b[39m dotp\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     logC \u001b[38;5;241m=\u001b[39m \u001b[43mvMFLogPartition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkappa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     logliks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkappa \u001b[38;5;241m*\u001b[39m dotp \u001b[38;5;241m+\u001b[39m logC\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m#print(torch.distributions.normal.Normal(1,0.5).log_prob(norm(x))[0].exp())\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/BNN/FVMF.py:89\u001b[0m, in \u001b[0;36mvMFLogPartition.forward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m     87\u001b[0m mp_kappa \u001b[38;5;241m=\u001b[39m mpmath\u001b[38;5;241m.\u001b[39mmpf(\u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m*\u001b[39m kappa\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     88\u001b[0m mp_logI \u001b[38;5;241m=\u001b[39m vMFLogPartition\u001b[38;5;241m.\u001b[39mlog(vMFLogPartition\u001b[38;5;241m.\u001b[39mbesseli(s, mp_kappa))\n\u001b[0;32m---> 89\u001b[0m logI \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp_logI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkappa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (logI \u001b[38;5;241m!=\u001b[39m logI)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# there is nan\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNaN is detected from the output of log-besseli()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import importlib\n",
    "#import os\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "#import VMF\n",
    "\n",
    "#import importlib\n",
    "#importlib.reload(VMF)\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "batch_size = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 50\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "epochs = 10\n",
    "trtimes  = np.zeros(epochs)\n",
    "#w_mu = [w_mu1, w_mu2, w_mu3, w_mu4]\n",
    "#b_mu = [b_mu1, b_mu2, b_mu3, b_mu4]\n",
    "\n",
    "#w_mu_nodewise = [w_mu1_nodewise,w_mu2_nodewise,w_mu3_nodewise,w_mu4_nodewise]\n",
    "#b_mu_nodewise = [b_mu1_nodewise,b_mu2_nodewise,b_mu3_nodewise,b_mu4_nodewise]\n",
    "# make inference on 10 networks\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net2 = FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='vmf',\n",
    "                                b_kappa=torch.Tensor(1).uniform_(4,4.1),\n",
    "                                w_kappa=torch.Tensor(1).uniform_(6.5,6.6),\n",
    "                                Temper = 1, normalize = 'No',classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #p.requires_grad_(False)\n",
    "    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net2.parameters(), lr=0.14)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net2, dtrain, SAMPLES, optimizer, epoch, i,shape = (0,256,256,257),BATCH_SIZE = 100)\n",
    "        print('max:',net2.weight_mu[1].max())\n",
    "        print('norm:',torch.norm(net2.weight_mu[1]))\n",
    "\n",
    "    res = test_ensemble.test_ensemble(net2,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = (0,256,256,257))\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfaf3686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "0\n",
      "Random Init Utilized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "loss: tensor(-36.8351, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(171.1453, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(1.3029, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(6.3531, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "2\n",
      "loss: tensor(-72.8562, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(160.2231, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(1.3919, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(7.0493, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "3\n",
      "loss: tensor(-90.4584, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(159.4871, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(1.5872, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(7.5766, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "4\n",
      "loss: tensor(-103.7574, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(159.0381, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(1.6848, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(7.8330, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "5\n",
      "loss: tensor(-112.0691, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(161.1703, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(1.6836, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.1061, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "6\n",
      "loss: tensor(-118.9507, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(163.1211, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(1.7269, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.2751, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "7\n",
      "loss: tensor(-129.7426, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(159.9929, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(1.6417, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.2249, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "8\n",
      "loss: tensor(-137.0771, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(159.4409, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(1.5833, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.1833, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "9\n",
      "loss: tensor(-142.3340, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(160.2686, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(1.6870, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.2882, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "10\n",
      "loss: tensor(-148.4332, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(159.6992, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "max: tensor(1.7219, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.3768, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "Component 0 Accuracy: 257.0/1000\n",
      "Component 1 Accuracy: 246.0/1000\n",
      "Component 2 Accuracy: 260.0/1000\n",
      "Component 3 Accuracy: 257.0/1000\n",
      "Component 4 Accuracy: 253.0/1000\n",
      "Component 5 Accuracy: 255.0/1000\n",
      "Component 6 Accuracy: 257.0/1000\n",
      "Component 7 Accuracy: 257.0/1000\n",
      "Component 8 Accuracy: 257.0/1000\n",
      "Component 9 Accuracy: 257.0/1000\n",
      "Posterior Mean Accuracy: 257.0/1000\n",
      "Ensemble Accuracy: 257/1000\n"
     ]
    }
   ],
   "source": [
    "#matplotlib inline\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "\n",
    "import importlib\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPUs are used!\")\n",
    "else:\n",
    "    print(\"CPUs are used!\")\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 50\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "\n",
    "#The net does not like to get larger at a given layer??\n",
    "l1shape=(256, 10)\n",
    "l2shape=(10, 10)\n",
    "l3shape=(10, 10)\n",
    "l4shape=(10, 5)\n",
    "layershapes = [l1shape, l2shape, l3shape, l4shape]\n",
    "\n",
    "epochs = 10\n",
    "trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net = FVMF.BayesianNetwork(layershapes=layershapes,BN='notbatchnorm',VD='ProjGaus',\n",
    "                               dtrain=dtrain,dtest=dtest,BATCH_SIZE = 100,classification = 'classification').to(DEVICE)\n",
    "    #net = VMF.BayesianNetwork(l1=l1shape, l2=l2shape, l3=l3shape,l4=l4shape,BN='notbatchnorm').to(DEVICE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.07)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,shape = (0,256,256,257))\n",
    "        #print(net.l1.weight_mu.mean())\n",
    "        print('max:',net.weight_mu[1].max())\n",
    "        print('norm:',torch.norm(net.weight_mu[1]))\n",
    "    res = test_ensemble.test_ensemble(net,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = (0,256,256,257))\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24216e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(42069)\n",
    "torch.manual_seed(42069)\n",
    "\n",
    "PL = [torch.Tensor([1-3, 1-3]),torch.Tensor([5-3, 1-3]),torch.Tensor([1-3, 5-3]),torch.Tensor([5-3, 5-3]),torch.Tensor([3-3, 3-3])]\n",
    "cov = torch.eye(2)\n",
    "n = 250\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=500)\n",
    "distrib = []\n",
    "\n",
    "DF = torch.zeros((len(PL),n,1,3)) #multiple of 4\n",
    "for i, MU in enumerate(PL): #enumerate starts from and including 0.\n",
    "    distrib.append(torch.distributions.multivariate_normal.MultivariateNormal(loc=MU, covariance_matrix=cov))\n",
    "    DATA_ = distrib[i].sample((n,1))\n",
    "\n",
    "    DATA  = torch.zeros([n, 1, 3])\n",
    "    DATA[:,:,:2] = DATA_\n",
    "    DATA[:,:,2]  = i\n",
    "    DF[i,:,:,:] = DATA\n",
    "    \n",
    "    x = DATA[:,0,0]\n",
    "    y = DATA[:,0,1]\n",
    "    plt.plot(x,y,'.',markersize=1.25)\n",
    "plt.show()\n",
    "\n",
    "C = int(3*n/4)\n",
    "\n",
    "#DATA_train = torch.zeros((len(PL)*C,3))\n",
    "#DATA_test = torch.zeros((len(PL)*(n-C),3))\n",
    "\n",
    "DATA = DF.reshape(len(PL)*n,3)\n",
    "#print('DATA:',DATA,'len(DATA):',len(DATA),'mean dtrain:',DATA.mean(axis=0)[2])\n",
    "\n",
    "#data_mean = DATA.mean(axis=1)[0:2]\n",
    "#data_std = DATA.std(axis=1)[0:2]\n",
    "\n",
    "#DATA[:,0:2] = (DATA[:,0:2]  - data_mean)/data_std\n",
    "#print('DATA normalized:',DATA,'len(DATA) normalized:',len(DATA),'mean dtrain normalized:',DATA.mean(axis=0)[2])\n",
    "tr_ids = np.random.choice(n*5, n*4, replace = False)\n",
    "\n",
    "\n",
    "\n",
    "dtrain = DATA[tr_ids,:]\n",
    "dtest = DATA[-tr_ids,:]\n",
    "\n",
    "#print('\\n','dtrain:',dtrain, 'len(dtrain):',len(dtrain),'mean dtrain:',dtrain.mean(axis=0)[2])\n",
    "#print('\\n','dtest:',dtest, 'len(dtest):',len(dtest),'mean dtest:',dtest.mean(axis=0)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47628290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "\n",
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "# define the parameters\n",
    "\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 50\n",
    "TEST_BATCH_SIZE = 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "l1shape=(2, 5)\n",
    "l2shape=(5, 5)\n",
    "l3shape=(5, 5)\n",
    "l4shape=(5, 5)\n",
    "l5shape=(5, 5)\n",
    "l6shape=(5, 5)\n",
    "layershapes = [l1shape, l2shape, l3shape, l4shape]\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "data_shape = (0,2,2,3)\n",
    "\n",
    "epochs = 30\n",
    "trtimes  = np.zeros(epochs)\n",
    "#w_mu = [w_mu1, w_mu2, w_mu3, w_mu4]\n",
    "#b_mu = [b_mu1, b_mu2, b_mu3, b_mu4]\n",
    "\n",
    "#w_mu_nodewise = [w_mu1_nodewise,w_mu2_nodewise,w_mu3_nodewise,w_mu4_nodewise]\n",
    "#b_mu_nodewise = [b_mu1_nodewise,b_mu2_nodewise,b_mu3_nodewise,b_mu4_nodewise]\n",
    "# make inference on 10 networks\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net3 = FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='Gaussian',\n",
    "                                #b_kappa=torch.Tensor(1).uniform_(4,4.1),\n",
    "                                #w_kappa=torch.Tensor(1).uniform_(6,6.1),\n",
    "                                Temper = 1,classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net3.parameters(), lr=0.04)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net3, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100, shape = data_shape)\n",
    "        print('max:',net3.weight_mu[1].max())\n",
    "        print('norm:',torch.norm(net3.weight_mu[1]))\n",
    "\n",
    "    res = test_ensemble.test_ensemble(net3,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3171283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net4 = FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='vmf',\n",
    "                                b_kappa=torch.Tensor(1).uniform_(4.0,4.1),\n",
    "                                w_kappa=torch.Tensor(1).uniform_(6.0,6.1),\n",
    "                                Temper = 1,classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net4.parameters(), lr=0.14)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net4, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100, shape = data_shape)\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    #res = test_ensemble.test_ensemble(net4,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GC_intensity = 1\n",
    "\n",
    "#Kanskje er det en god idee å også arve kappaene, men halvere dem? Hva om vi også kjører på med litt normalizering av mu-ene her?\n",
    "#Hva med adaptiv kappa per lag? Det er jo bare en kappa per bias-lag.\n",
    "\n",
    "w_mu = []\n",
    "for i in range(len(net4.weight_mu)):\n",
    "    #print('\\n','torch.norm(net4.weight_mu[i]):',torch.norm(net4.weight_mu[i]))\n",
    "    w_mu.append(net4.weight_mu[i]/torch.norm(net4.weight_mu[i]))\n",
    "    #print('\\n','norm w_mu[i]',torch.norm(w_mu[i]))\n",
    "    \n",
    "b_mu = []\n",
    "for i in range(len(net4.bias_mu)):\n",
    "    #print('\\n','torch.norm(net4.bais_mu[i]):',torch.norm(net4.bias_mu[i]))\n",
    "    b_mu.append(net4.bias_mu[i]/torch.norm(net4.bias_mu[i]))\n",
    "    #print('\\n','norm b_mu[i]',torch.norm(b_mu[i]))\n",
    "    \n",
    "b_rho= []\n",
    "for i in range(len(net4.bias_rho)):\n",
    "    b_rho.append(net4.bias_rho[i]*GC_intensity)\n",
    "\n",
    "w_rho= []\n",
    "for i in range(len(net4.weight_rho)):\n",
    "    w_rho.append(net4.weight_rho[i]*GC_intensity)\n",
    "\n",
    "#print('b_rho:',b_rho, '\\n','w_rho:',b_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net4_GC = FVMF.BayesianNetwork(w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='vmf',\n",
    "                                b_kappa= b_rho, #torch.Tensor(1).uniform_(1.0,3.1), \n",
    "                                w_kappa= w_rho, #torch.Tensor(1).uniform_(2.0,4.1), \n",
    "                                Temper = 1,classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net4_GC.parameters(), lr=0.07)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net4_GC, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100, shape = data_shape)\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    #res = test_ensemble.test_ensemble(net4_GC,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "#print('This is the loss with the Gradient Capture method/Normalized Initialization Inheritance')\n",
    "\n",
    "#Ok, så kanskje drit i gradient-capture-method. Men! Husk hva du har vist her nå med kun normalisering av vektene, Du får trent nettet\n",
    "#Videre, og det fungerer faktisk. I tilfellet hvor du ikke gjør det ender du faktisk ikke bare opp med dårlig ytelse, men\n",
    "#hele nettet brekker!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "GC_intensity = 1\n",
    "\n",
    "#Kanskje er det en god idee å også arve kappaene, men halvere dem? Hva om vi også kjører på med litt normalizering av mu-ene her?\n",
    "#Hva med adaptiv kappa per lag? Det er jo bare en kappa per bias-lag. GC fungerer rett og slett ikke.\n",
    "\n",
    "w_mu = []\n",
    "for i in range(len(net4_GC.weight_mu)):\n",
    "    #print('\\n','torch.norm(net4.weight_mu[i]):',torch.norm(net4.weight_mu[i]))\n",
    "    w_mu.append(net4_GC.weight_mu[i]/torch.norm(net4_GC.weight_mu[i]))\n",
    "    #print('\\n','norm w_mu[i]',torch.norm(w_mu[i]))\n",
    "    \n",
    "b_mu = []\n",
    "for i in range(len(net4_GC.bias_mu)):\n",
    "    #print('\\n','torch.norm(net4.bais_mu[i]):',torch.norm(net4.bias_mu[i]))\n",
    "    b_mu.append(net4_GC.bias_mu[i]/torch.norm(net4_GC.bias_mu[i]))\n",
    "    #print('\\n','norm b_mu[i]',torch.norm(b_mu[i]))\n",
    "    \n",
    "b_rho= []\n",
    "for i in range(len(net4_GC.bias_rho)):\n",
    "    b_rho.append(net4_GC.bias_rho[i]*GC_intensity)\n",
    "\n",
    "w_rho= []\n",
    "for i in range(len(net4_GC.weight_rho)):\n",
    "    w_rho.append(net4_GC.weight_rho[i]*GC_intensity)\n",
    "\n",
    "#print('b_rho:',b_rho, '\\n','w_rho:',b_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe30de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net4_GC2 = FVMF.BayesianNetwork(w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='vmf',\n",
    "                                b_kappa= b_rho, #torch.Tensor(1).uniform_(1.0,3.1), \n",
    "                                w_kappa= w_rho, #torch.Tensor(1).uniform_(2.0,4.1), \n",
    "                                Temper = 1,classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net4_GC2.parameters(), lr=0.035)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net4_GC2, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100, shape = data_shape)\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    res = test_ensemble.test_ensemble(net4_GC2,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "print('This is the loss with the Gradient Capture method/Normalized Initialization Inheritance, applied twice')\n",
    "\n",
    "#Ok, så kanskje drit i gradient-capture-method. Men! Husk hva du har vist her nå med kun normalisering av vektene, Du får trent nettet\n",
    "#Videre, og det fungerer faktisk. I tilfellet hvor du ikke gjør det ender du faktisk ikke bare opp med dårlig ytelse, men\n",
    "#hele nettet brekker!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84652247",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 1028 #Ca. Full HD resolution. If you struggle with memory adjust this parameter down. 64 for example already gives a decent view.\n",
    "enums = 10\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "Temps = torch.zeros((RES**2,3))\n",
    "i, j = 0, 0\n",
    "\n",
    "x = torch.linspace(-5,5,RES)\n",
    "\n",
    "Temps[:,:2] = torch.cartesian_prod(x, x)\n",
    "probs = torch.zeros((1,5)).to(DEVICE)\n",
    "\n",
    "for j in range(enums):\n",
    "    probs = probs + torch.exp(net4_GC2(Temps[:,0:2])) #This is done enums times, each one will be different due to stochastic.\n",
    "probs = probs/enums\n",
    "\n",
    "Temps[:,2] = -torch.sum(probs * torch.log(probs),dim = 1)\n",
    "print(torch.max(Temps[:,2]),torch.min(Temps[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fe0b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('vMF Uncertainty Plot 42epocsLR0.1 3*10epocs, NII in between.')\n",
    "#xy = [0,0.2,0.4,0.6,0.8,1]\n",
    "#z = xy\n",
    "plot = plt.scatter(Temps[:,0].detach().numpy(), Temps[:,1].detach().numpy(),vmin = 0, vmax = 1.4, s=221000/(RES**2), c=Temps[:,2].detach().numpy(), cmap='RdYlBu', marker='s')\n",
    "plt.gcf().set_size_inches((14, 12))\n",
    "plt.colorbar(plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca69ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RES = 1028\n",
    "enums = 10\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "Temps2 = torch.zeros((RES**2,3))\n",
    "i, j = 0, 0\n",
    "\n",
    "x = torch.linspace(-5,5,RES)\n",
    "\n",
    "Temps2[:,:2] = torch.cartesian_prod(x, x)\n",
    "probs = torch.zeros((1,5)).to(DEVICE)\n",
    "\n",
    "for j in range(enums):\n",
    "    probs = probs + torch.exp(net3(Temps2[:,0:2])) #This is done enums times, each one will be different due to stochastic.\n",
    "probs = probs/enums\n",
    "\n",
    "Temps2[:,2] = -torch.sum(probs * torch.log(probs),dim = 1)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065f450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('Gaussian Uncertainty Plot')\n",
    "#xy = [0,0.2,0.4,0.6,0.8,1]\n",
    "#z = xy\n",
    "plot2 = plt.scatter(Temps2[:,0].detach().numpy(), Temps2[:,1].detach().numpy(),vmin = 0, vmax = 1.4, s=221000/(RES**2), c=Temps2[:,2].detach().numpy(), cmap='RdYlBu', marker='s')\n",
    "plt.gcf().set_size_inches((14, 12))\n",
    "plt.colorbar(plot2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of the variables should be defined earlier, to be certain we are comparing the same things in our plots.\n",
    "RES = 1028\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "Temps4 = torch.zeros((RES**2,3))\n",
    "\n",
    "x = torch.linspace(-5,5,RES)\n",
    "Temps4[:,:2] = torch.cartesian_prod(x, x)\n",
    "probs = torch.zeros((RES**2, 5)).to(DEVICE)\n",
    "\n",
    "\n",
    "#print(distributions[i].log_prob(Temps4[:,0:2]).shape)\n",
    "sfm = torch.nn.Softmax(dim = -1)\n",
    "\n",
    "VAL = torch.zeros((RES**2,5)).to(DEVICE)\n",
    "for i in range(len(distrib)):\n",
    "    VAL[:,i] = distrib[i].log_prob(Temps4[:,0:2]) #Does the log_prob method really do exactly what I want?\n",
    "\n",
    "VAL2 = torch.zeros((RES**2,5)).to(DEVICE)\n",
    "VAL2 = sfm(VAL)\n",
    "#mc = torch.min(VAL,dim = 1).values    \n",
    "#nc = torch.sum(VAL-mc,dim = 1)\n",
    "\n",
    "      \n",
    "#for i in range(len(distrib)):\n",
    "#    VAL[:,i] =  VAL[:,i]-mc)/nc[i]\n",
    "    \n",
    "Temps4[:,2] = -torch.sum(torch.log(VAL2) * VAL2,dim = 1) #We don't log the probs here, since they were already logged.\n",
    "\n",
    "\n",
    "#print(VAL.shape)\n",
    "#print(torch.exp(net3(Temps2[:,0:2])).shape)\n",
    "print(max(torch.exp(distrib[i].log_prob(Temps4[:,0:2]))),min(torch.exp(distrib[i].log_prob(Temps4[:,0:2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8862240",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('True Uncertainty Plot')\n",
    "#xy = [0,0.2,0.4,0.6,0.8,1]\n",
    "#z = xy\n",
    "plot4 = plt.scatter(Temps4[:,0].detach().numpy(), Temps4[:,1].detach().numpy(), s=221000/(RES**2), c=Temps4[:,2].detach().numpy(), cmap='RdYlBu', marker='s')\n",
    "plt.gcf().set_size_inches((14, 12))\n",
    "plt.colorbar(plot4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879386ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net4.state_dict()\n",
    "#print(net4.state_dict()['layers.3.weight_mu'])\n",
    "#get a bar for the range of the entropy scalar.\n",
    "#Center data around 0.\n",
    "\n",
    "#Simuler marginalfordeling for en vekt fra vMF. OG sammenlign med en marginal vekt fra multivariat Gaussisk som har blitt normalisert.\n",
    "#og hva skjer i posterioren etter det er trent. trekk vekter fra HU, gjør forwardpass predictions som simulert data, tren på det og sjekk om vektene blir som i starten.\n",
    "#Hva med å initialisere fra HU!!!???\n",
    "\n",
    "#Siden W_kappa initialiseres så høyt, er det jo ikke så rart at vMF-en generelt er mye mindre selvsikker enn den Gaussiske.\n",
    "#Da er det også forståelig, at ettersom vi øker w_kappa, så må vi også øke læringsraten, siden forandringen i likelihood\n",
    "#loss ved weight_mu justering blir mindre intens. Kanskje er også noe av grunnen til at vi blir tvunget til så høye kappa,\n",
    "#At nettet ikke klarer å finne noen gradient descent ellers? Siden hvis vi er langt unna der W-mu skulle vært, så får vi nesten\n",
    "#ikke noen forandring i loss? Hva med en tostegs-trening, hvor vi først trener med høy kappa. Og så initialiserer vi fra de foregående\n",
    "#W og b muene, men med nye tilfeldige kappa som er mye mindre?? Det høres ut som en god ide!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ee75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "DATA = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00448/carbon_nanotubes.csv',sep = ';', decimal=\",\")\n",
    "#This is the carbon nanotubes data.\n",
    "DATA = DATA.astype(float)\n",
    "#print(DATA)\n",
    "#print(DATA)\n",
    "DATA = DATA.to_numpy()\n",
    "DATA = torch.from_numpy(DATA)\n",
    "#print(DATA)\n",
    "np.random.seed(42069)\n",
    "#torch.manual_seed(42069)\n",
    "\n",
    "DATA = DATA.type(torch.float32)\n",
    "\n",
    "data_mean = DATA.mean(axis=1)[0:8]\n",
    "data_std = DATA.std(axis=1)[0:8]\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "\n",
    "#DATA[:,7] = 5*DATA[:,1] + 3*DATA[:,6] + 2*DATA[:,3] + 1*DATA[:,2] \n",
    "DATA[:,0:8] = (DATA[:,0:8]  - data_mean)/data_std\n",
    "#print('DATA normalized:',DATA,'len(DATA) normalized:',len(DATA),'mean dtrain normalized:',DATA.mean(axis=0)[2])\n",
    "tr_ids = np.random.choice(10721, 6000, replace = False)\n",
    "\n",
    "dTRAIN_CARBON = DATA[tr_ids,:]\n",
    "dTEST_CARBON = DATA[-tr_ids,:]\n",
    "\n",
    "print(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa13ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "COND_OPT = False\n",
    "CLASSES = 1\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 30\n",
    "epochs =10\n",
    "TEST_BATCH_SIZE = 6000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "data_shape = (0,7,7,8)\n",
    "l1shape=(7, 7)\n",
    "l2shape=(7, 7)\n",
    "l3shape=(7, 7)\n",
    "l4shape=(7, 1)\n",
    "layershapes = [l1shape, l2shape, l3shape, l4shape]\n",
    "\n",
    "trtimes  = np.zeros(epochs)\n",
    "\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net5= FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='Gaussian',\n",
    "                                #b_kappa=torch.Tensor(1).uniform_(3,3.1),\n",
    "                                #w_kappa=torch.Tensor(1).uniform_(5,5.1),\n",
    "                                Temper = 0.1,\n",
    "                                classification='Regression')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net5.parameters(), lr=0.0007)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net5, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(net5,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50521eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "COND_OPT = False\n",
    "CLASSES = 1\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 30\n",
    "epochs = 10\n",
    "TEST_BATCH_SIZE = 6000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "data_shape = (0,7,7,8)\n",
    "l1shape=(7, 7)\n",
    "l2shape=(7, 7)\n",
    "l3shape=(7, 7)\n",
    "l4shape=(7, 1) #So the vMF mathematically does not support having a layer that ends in 1. This is the cause of the bug.\n",
    "#However, should we even have the last layer as a vmf for regression? I don't really think so. Lets try the layerwise vmf\n",
    "#for the last one. Wait... why is the layerwise x_dim=out*in for its weights while the nodewise is just x_dim=in? Yeah cause I\n",
    "#list stuff together inside the vmf_nodewise, so that it makes sense. Maybe it will work better to just have a frequenstist last layer?\n",
    "#The loss is not calculated correctly, MSE criterion 0.92, is not representative of the network missing by 42000 on a 0-1 regression task.\n",
    "#There was some PyTorch funny business with the lr being so low that the los becomes jumpy and the pedictions rubbish.... No\n",
    "#immediate mathematical interpretation of this strange behaviour as far as I can see, so I think it is a technical probelm\n",
    "#with regard to loss or gradient calculation in the PyTorch machinery.\n",
    "\n",
    "#But bro if the out_features are 1, then in=out*in! So the layerwise vmf won't help you. Let's do Gaussian.\n",
    "layershapes = [l1shape, l2shape, l3shape, l4shape]\n",
    "\n",
    "trtimes  = np.zeros(epochs)\n",
    "\n",
    "\n",
    "#Note, for this regression task, the last 7 to 1 layer has a Gaussian VD, where we kill the prior and simply optimize with MLE.\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    vMFRegression= FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='vmf',\n",
    "                                b_kappa=torch.Tensor(1).uniform_(3,3.1),\n",
    "                                w_kappa=torch.Tensor(1).uniform_(7.5,7.6),\n",
    "                                Temper = 0,classification='Regression',NODEFORCE =False)\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(vMFRegression.parameters(), lr=0.11)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(vMFRegression, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(vMFRegression,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f99831",
   "metadata": {},
   "outputs": [],
   "source": [
    "GC_intensity = 1\n",
    "\n",
    "#Kanskje er det en god idee å også arve kappaene, men halvere dem? Hva om vi også kjører på med litt normalizering av mu-ene her?\n",
    "#Hva med adaptiv kappa per lag? Det er jo bare en kappa per bias-lag. GC fungerer rett og slett ikke.\n",
    "\n",
    "w_mu_R = []\n",
    "for i in range(len(vMFRegression.weight_mu)):\n",
    "    #print('\\n','torch.norm(net4.weight_mu[i]):',torch.norm(net4.weight_mu[i]))\n",
    "    w_mu.append(vMFRegression.weight_mu[i]/torch.norm(vMFRegression.weight_mu[i]))\n",
    "    #print('\\n','norm w_mu[i]',torch.norm(w_mu[i]))\n",
    "    \n",
    "b_mu_R = []\n",
    "for i in range(len(vMFRegression.bias_mu)):\n",
    "    #print('\\n','torch.norm(net4.bais_mu[i]):',torch.norm(net4.bias_mu[i]))\n",
    "    b_mu.append(vMFRegression.bias_mu[i]/torch.norm(vMFRegression.bias_mu[i]))\n",
    "    #print('\\n','norm b_mu[i]',torch.norm(b_mu[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    vMFRegression2= FVMF.BayesianNetwork(w_mu = w_mu_R, b_mu = b_mu_R, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='vmf',\n",
    "                                b_kappa= b_rho, #torch.Tensor(1).uniform_(1.0,3.1), \n",
    "                                w_kappa= w_rho, #torch.Tensor(1).uniform_(2.0,4.1), \n",
    "                                Temper = 1,classification = 'regression')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(vMFRegression2.parameters(), lr=0.025)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(vMFRegression2, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(vMFRegression2,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379109ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = [0,1,2,3,4,5]\n",
    "for i,G, in enumerate(g):\n",
    "    print('i:',i,'g:',G)\n",
    "print(len(g))\n",
    "i,j = True,True\n",
    "if (i==1) or (j==1):\n",
    "    print('The or statement in Python means and/or')\n",
    "else:\n",
    "    print('The or statement in Python means either or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d295471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The RSS doesn't really vary much based on the number of epochs I administer. I suspect this to be either because we are not\n",
    "#training the network correctly, or we are testing it incorrectly.\n",
    "\n",
    "#Make many forward passes, and calculate the MSE on all of the forward passes!\n",
    "\n",
    "#Also calculate the percentiles for them!\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "cifar_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "cifar_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cifar_train.__getitem__(1)[0][1])\n",
    "#print(cifar_test.__getitem__(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dbc2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "What if the problem is that since the mu's in the Gaussian is (out,in), and the vMF is out*in,\n",
    "this could mean that we have \"strethed\" a single vMF pdf over all the parameters, while in the Gaussian we have made one for each output?\n",
    "\n",
    "I don't know, also intuitively this should be the case, since the whole point of the vMF is the norm 1, which obviously\n",
    "will require that it for each forward-pass is only one massive pdf for all inputs and outputs.\n",
    "\n",
    "Ok, this can get rough, I will try my best to reshape the w_mu's and b_mu's the best I can, perhaps this will work just fine without too much tuning...\n",
    "Hopefully.\n",
    "\n",
    "This doe not seem to ave worked. I don't konw exactly what the problem is. Per haps it is a good idea to consult the new loss function\n",
    "suggested in the paper that made the code we based our vMF on?\n",
    "\n",
    "It is very strange that the loss is not at all affected by completely ridiculous learning rates...\n",
    "\n",
    "Remember that vMF makes the norm of the weights and biases 1, not the forward pass of the x's. Hence the advantage is that the gradient\n",
    "will not explode, since the backward pass of it will also be approx. 1. In batchnorm, maybe the gradient can explode? Since the weights \n",
    "can be whatever?\n",
    "\n",
    "\n",
    "IMPORTANT: The gaussian neuralnet will also collapse to 257 if I apply more than 3 layers. This must be somehow related to the similar\n",
    "behavior in the vMF when the size of each layer exceeds 3. Per haps there is an error in the loss afterall?\n",
    "However, in the Gaussian case, increasing the learning rate by a factor of 10 solved the issue. This makes me suspect it is the mathematical\n",
    "properties of the loss function, rather than incorrect implementation.\n",
    "\"\"\"\n",
    "\n",
    "#When l4 is (3,5):\n",
    "r\"\"\"\n",
    "File ~/projects/BNN/AliaksandrFolder/FVMF.py:289, in vMF.sample(self, N, rsf)\n",
    "    287 e1mu = torch.zeros(d, 1).to(DEVICE)\n",
    "    288 e1mu[0, 0] = 1.0\n",
    "--> 289 e1mu = e1mu - self.mu if len(self.mu.shape) == 2 else e1mu - self.mu.unsqueeze(1) #e1mu.shape = (1,self.x_dim). mu_unnorm.shape = (mu_unnorm)\n",
    "    290 e1mu = e1mu / norm(e1mu, dim=0).to(DEVICE)\n",
    "    291 samples = samples - 2 * (samples @ e1mu) @ e1mu.t()\n",
    "\n",
    "RuntimeError: The size of tensor a (15) must match the size of tensor b (9) at non-singleton dimension 0\n",
    "\"\"\"\n",
    "\n",
    "#When l4 is (5,5):\n",
    "r\"\"\"\n",
    "File ~/projects/BNN/AliaksandrFolder/FVMF.py:289, in vMF.sample(self, N, rsf)\n",
    "    287 e1mu = torch.zeros(d, 1).to(DEVICE)\n",
    "    288 e1mu[0, 0] = 1.0\n",
    "--> 289 e1mu = e1mu - self.mu if len(self.mu.shape) == 2 else e1mu - self.mu.unsqueeze(1) #e1mu.shape = (1,self.x_dim). mu_unnorm.shape = (mu_unnorm)\n",
    "    290 e1mu = e1mu / norm(e1mu, dim=0).to(DEVICE)\n",
    "    291 samples = samples - 2 * (samples @ e1mu) @ e1mu.t()\n",
    "\n",
    "RuntimeError: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 0\n",
    "\"\"\"\n",
    "#These errors above were caused by my initialization being wrong. I copy paster mu_3 for layer4, and forgot to change to mu_4. So now \n",
    "#I always get the error below.\n",
    "\n",
    "\n",
    "#in all cases now: \n",
    "\n",
    "r\"\"\"\n",
    "It seems that the whole thing does not progress at all. We just get the warning and then no further output.\n",
    "\n",
    "self.l4(x, sample)\n",
    "\n",
    "--> self.bias.sample()\n",
    "\n",
    "It always get's stuck there!!\n",
    "\n",
    "Specifically, it get's stuck in the while loop:\n",
    "\n",
    "while len(v0) < N:\n",
    "            eps = beta.sample([1, rsf * (N - len(v0))]).squeeze().to(DEVICE)\n",
    "            uns = uniform.sample([1, rsf * (N - len(v0))]).squeeze().to(DEVICE)\n",
    "            w0 = (1 - (1 + bb) * eps) / (1 - (1 - bb) * eps)\n",
    "            t0 = (2 * aa * bb) / (1 - (1 - bb) * eps)\n",
    "            det = (d - 1) * t0.log() - t0 + dd - uns.log()\n",
    "            v0 = torch.cat([v0, torch.tensor(w0[det >= 0]).to(DEVICE)])\n",
    "            if len(v0) > N:\n",
    "                v0 = v0[:N]\n",
    "                break\n",
    "\"\"\"\n",
    "\n",
    "r\"\"\"\n",
    "From further investigations it is clear that the error lies in w0[det >= 0] consistently being an empty Tensor.\n",
    "\n",
    "Even further, bb is 0 here which it usually is not. That must definitely indicate something is wrong.\n",
    "\n",
    "Adjusting the initialization of kappa to be 9 or less on both weights and biases makes the code run, \n",
    "but posterior collapse is back. Increasing kappa seems to increase the compute aswell... however, getting the kappa inits\n",
    "closer to 10 seems to also help avoid the posterior collapse. And the lower bound increased also helps, looks like 3 is optimal.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "r\"\"\"\n",
    "It seems that Torch likes that each of the bias_mu's and weight_mu's from each layer are separately registered with name as an nn.parameter.\n",
    "\"\"\"\n",
    "\n",
    "r\"\"\"\n",
    "Currently, for 4 layers with 3 hiddenwidth, it seems around 10 epochs with .14 learning rate is optimal for testperformance.\n",
    "I suspect this is because we do not have the modelcapacity to go beyond the overfiting case just yet, and must settle for the classical\n",
    "best-fit.\n",
    "\"\"\"\n",
    "\n",
    "r\"\"\"\n",
    "The problem for both the random initialization and the variable length Gaussian has the same root. It is that the mu's and rho's \n",
    "are not being registered as parameters per layer to begin with!\n",
    "\n",
    "And since this registering works just fine when we are directly assigning self.layer's to be each layer the parameters are registered correctly,\n",
    "it must be the case that this part in the BayesianNetwork's initializer is where the problem originates.\n",
    "\"\"\"\n",
    "\n",
    "r\"\"\"\n",
    "Part of the reason why the layerwise vMF might not be that performant, \n",
    "is that it forces one kappa on every weight in the entire layer.\n",
    "\"\"\"\n",
    "\n",
    "r\"\"\"\n",
    "The way the mu of the weights are used in the vMF is that any mu is accepted, and then the mu's are normalized. \n",
    "I think this is the reason for why the net becomes intractably slow as we increase the layers. \n",
    "If it is possible to set the weight_mu parameters to be the normalized versions for every epoch I think that would be great.\n",
    "\n",
    "Or perhaps build into the loss a term that penalizes the weights from deviating from norm=1...\n",
    "\n",
    "I don't know exactly where to put this\n",
    "\"\"\"\n",
    "\n",
    "r\"\"\"\n",
    "The training function I got from Aliaksandr uses Variable() which is depcrecated since PyTorch 0.4, and creates tensors directly \n",
    "via torch.FloatTensor which is also bad practice. In addition the data is presented to the train function as numpy arrays.\n",
    "\n",
    "All these three issues require resolution.\n",
    "\n",
    "I starred a S.Overflow post that showed what I think will be the solution to the ghost-Mu. Remember that you could motivate this \n",
    "mathematically by proving dF(x/norm(x);parameters)/dx = dF(x;parameters)/dnorm(x)\n",
    "\n",
    "Should I try to see what happens if I use float64 instead?\n",
    "\"\"\"\n",
    "\n",
    "r\"\"\"\n",
    "Should I really have a vMF on the biases? They don't contribute to the gradient much in the backward-pass, \n",
    "making them norm 1 doesn't really make too much sense..\n",
    "\"\"\"\n",
    "\n",
    "r\"\"\"\n",
    "[Kappa is called concentration parameter because the higher the kappa the lower the variance, \n",
    "i.e. the higher the concentration around mu. This also explains why the vMF struggles for higher kappas, \n",
    "because it is known that the vMF is numerically heavy and unstable for high concentrations, aka low variance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2634079",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Plan:\n",
    "\n",
    "First solve the uncertainty estimation.\n",
    "\n",
    "Solve comparing deeper nets, with alternating use of batchnorm.\n",
    "Solve a couple more image-classification data sets\n",
    "Solve the nn.Parameter(the Mu's) normalization business.\n",
    "Solve having separate kappas for each dimension of the vMF. (I probably won't get to this..)\n",
    "\n",
    "Compare with FNN's?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(42069)\n",
    "torch.manual_seed(42069)\n",
    "\n",
    "PL = [torch.Tensor([1-3, 1-3]),torch.Tensor([5-3, 1-3]),torch.Tensor([1-3, 5-3]),torch.Tensor([5-3, 5-3]),torch.Tensor([3-3, 3-3])]\n",
    "cov = torch.eye(2)\n",
    "n = 150\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=500)\n",
    "\n",
    "DF = torch.zeros((len(PL),n,1,3)) #multiple of 4\n",
    "for i, MU in enumerate(PL): #enumerate starts from and including 0.\n",
    "    distrib = torch.distributions.MultivariateNormal(loc=MU, covariance_matrix=cov)\n",
    "    DATA_ = distrib.sample((n,1))\n",
    "\n",
    "    DATA  = torch.zeros([n, 1, 3])\n",
    "    DATA[:,:,:2] = DATA_\n",
    "    DATA[:,:,2]  = i\n",
    "    DF[i,:,:,:] = DATA\n",
    "    \n",
    "    x = DATA[:,0,0]\n",
    "    y = DATA[:,0,1]\n",
    "    plt.plot(x,y,'.',markersize=1.25)\n",
    "plt.show()\n",
    "\n",
    "C = int(3*n/4)\n",
    "\n",
    "#DATA_train = torch.zeros((len(PL)*C,3))\n",
    "#DATA_test = torch.zeros((len(PL)*(n-C),3))\n",
    "\n",
    "DATA = DF.reshape(len(PL)*n,3)\n",
    "#print('DATA:',DATA,'len(DATA):',len(DATA),'mean dtrain:',DATA.mean(axis=0)[2])\n",
    "\n",
    "#data_mean = DATA.mean(axis=1)[0:2]\n",
    "#data_std = DATA.std(axis=1)[0:2]\n",
    "\n",
    "#DATA[:,0:2] = (DATA[:,0:2]  - data_mean)/data_std\n",
    "#print('DATA normalized:',DATA,'len(DATA) normalized:',len(DATA),'mean dtrain normalized:',DATA.mean(axis=0)[2])\n",
    "tr_ids = np.random.choice(n*5, n*4, replace = False)\n",
    "\n",
    "\n",
    "\n",
    "dtrain = DATA[tr_ids,:]\n",
    "dtest = DATA[-tr_ids,:]\n",
    "\n",
    "#print('\\n','dtrain:',dtrain, 'len(dtrain):',len(dtrain),'mean dtrain:',dtrain.mean(axis=0)[2])\n",
    "#print('\\n','dtest:',dtest, 'len(dtest):',len(dtest),'mean dtest:',dtest.mean(axis=0)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb74d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "\n",
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "# define the parameters\n",
    "\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 50\n",
    "TEST_BATCH_SIZE = 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "l1shape=(2, 5)\n",
    "l2shape=(5, 5)\n",
    "l3shape=(5, 5)\n",
    "l4shape=(5, 5)\n",
    "l5shape=(5, 5)\n",
    "l6shape=(5, 5)\n",
    "layershapes = [l1shape, l2shape, l3shape, l4shape]\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "data_shape = (0,2,2,3)\n",
    "\n",
    "epochs = 30\n",
    "trtimes  = np.zeros(epochs)\n",
    "#w_mu = [w_mu1, w_mu2, w_mu3, w_mu4]\n",
    "#b_mu = [b_mu1, b_mu2, b_mu3, b_mu4]\n",
    "\n",
    "#w_mu_nodewise = [w_mu1_nodewise,w_mu2_nodewise,w_mu3_nodewise,w_mu4_nodewise]\n",
    "#b_mu_nodewise = [b_mu1_nodewise,b_mu2_nodewise,b_mu3_nodewise,b_mu4_nodewise]\n",
    "# make inference on 10 networks\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net3 = FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='Gaussian',\n",
    "                                #b_kappa=torch.Tensor(1).uniform_(4,4.1),\n",
    "                                #w_kappa=torch.Tensor(1).uniform_(6,6.1),\n",
    "                                Temper = 1,classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net3.parameters(), lr=0.04)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net3, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100, shape = data_shape)\n",
    "        print('max:',net3.weight_mu[1].max())\n",
    "        print('norm:',torch.norm(net3.weight_mu[1]))\n",
    "\n",
    "    res = test_ensemble.test_ensemble(net3,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6139cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net4 = FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='vmf',\n",
    "                                b_kappa=torch.Tensor(1).uniform_(4.0,4.1),\n",
    "                                w_kappa=torch.Tensor(1).uniform_(6.0,6.1),\n",
    "                                Temper = 1,classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net4.parameters(), lr=0.14)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net4, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100, shape = data_shape)\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    #res = test_ensemble.test_ensemble(net4,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09481e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GC_intensity = 1\n",
    "\n",
    "#Kanskje er det en god idee å også arve kappaene, men halvere dem? Hva om vi også kjører på med litt normalizering av mu-ene her?\n",
    "#Hva med adaptiv kappa per lag? Det er jo bare en kappa per bias-lag.\n",
    "\n",
    "w_mu = []\n",
    "for i in range(len(net4.weight_mu)):\n",
    "    #print('\\n','torch.norm(net4.weight_mu[i]):',torch.norm(net4.weight_mu[i]))\n",
    "    w_mu.append(net4.weight_mu[i]/torch.norm(net4.weight_mu[i]))\n",
    "    #print('\\n','norm w_mu[i]',torch.norm(w_mu[i]))\n",
    "    \n",
    "b_mu = []\n",
    "for i in range(len(net4.bias_mu)):\n",
    "    #print('\\n','torch.norm(net4.bais_mu[i]):',torch.norm(net4.bias_mu[i]))\n",
    "    b_mu.append(net4.bias_mu[i]/torch.norm(net4.bias_mu[i]))\n",
    "    #print('\\n','norm b_mu[i]',torch.norm(b_mu[i]))\n",
    "    \n",
    "b_rho= []\n",
    "for i in range(len(net4.bias_rho)):\n",
    "    b_rho.append(net4.bias_rho[i]*GC_intensity)\n",
    "\n",
    "w_rho= []\n",
    "for i in range(len(net4.weight_rho)):\n",
    "    w_rho.append(net4.weight_rho[i]*GC_intensity)\n",
    "\n",
    "#print('b_rho:',b_rho, '\\n','w_rho:',b_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net4_GC = FVMF.BayesianNetwork(w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='vmf',\n",
    "                                b_kappa= b_rho, #torch.Tensor(1).uniform_(1.0,3.1), \n",
    "                                w_kappa= w_rho, #torch.Tensor(1).uniform_(2.0,4.1), \n",
    "                                Temper = 1,classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net4_GC.parameters(), lr=0.07)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net4_GC, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100, shape = data_shape)\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    #res = test_ensemble.test_ensemble(net4_GC,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "#print('This is the loss with the Gradient Capture method/Normalized Initialization Inheritance')\n",
    "\n",
    "#Ok, så kanskje drit i gradient-capture-method. Men! Husk hva du har vist her nå med kun normalisering av vektene, Du får trent nettet\n",
    "#Videre, og det fungerer faktisk. I tilfellet hvor du ikke gjør det ender du faktisk ikke bare opp med dårlig ytelse, men\n",
    "#hele nettet brekker!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593138c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GC_intensity = 1\n",
    "\n",
    "#Kanskje er det en god idee å også arve kappaene, men halvere dem? Hva om vi også kjører på med litt normalizering av mu-ene her?\n",
    "#Hva med adaptiv kappa per lag? Det er jo bare en kappa per bias-lag. GC fungerer rett og slett ikke.\n",
    "\n",
    "w_mu = []\n",
    "for i in range(len(net4_GC.weight_mu)):\n",
    "    #print('\\n','torch.norm(net4.weight_mu[i]):',torch.norm(net4.weight_mu[i]))\n",
    "    w_mu.append(net4_GC.weight_mu[i]/torch.norm(net4_GC.weight_mu[i]))\n",
    "    #print('\\n','norm w_mu[i]',torch.norm(w_mu[i]))\n",
    "    \n",
    "b_mu = []\n",
    "for i in range(len(net4_GC.bias_mu)):\n",
    "    #print('\\n','torch.norm(net4.bais_mu[i]):',torch.norm(net4.bias_mu[i]))\n",
    "    b_mu.append(net4_GC.bias_mu[i]/torch.norm(net4_GC.bias_mu[i]))\n",
    "    #print('\\n','norm b_mu[i]',torch.norm(b_mu[i]))\n",
    "    \n",
    "b_rho= []\n",
    "for i in range(len(net4_GC.bias_rho)):\n",
    "    b_rho.append(net4_GC.bias_rho[i]*GC_intensity)\n",
    "\n",
    "w_rho= []\n",
    "for i in range(len(net4_GC.weight_rho)):\n",
    "    w_rho.append(net4_GC.weight_rho[i]*GC_intensity)\n",
    "\n",
    "#print('b_rho:',b_rho, '\\n','w_rho:',b_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ac231",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net4_GC2 = FVMF.BayesianNetwork(w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='vmf',\n",
    "                                b_kappa= b_rho, #torch.Tensor(1).uniform_(1.0,3.1), \n",
    "                                w_kappa= w_rho, #torch.Tensor(1).uniform_(2.0,4.1), \n",
    "                                Temper = 1,classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net4_GC2.parameters(), lr=0.035)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net4_GC2, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100, shape = data_shape)\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    res = test_ensemble.test_ensemble(net4_GC2,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "print('This is the loss with the Gradient Capture method/Normalized Initialization Inheritance, applied twice')\n",
    "\n",
    "#Ok, så kanskje drit i gradient-capture-method. Men! Husk hva du har vist her nå med kun normalisering av vektene, Du får trent nettet\n",
    "#Videre, og det fungerer faktisk. I tilfellet hvor du ikke gjør det ender du faktisk ikke bare opp med dårlig ytelse, men\n",
    "#hele nettet brekker!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9932a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 1028 #Ca. Full HD resolution. If you struggle with memory adjust this parameter down. 64 for example already gives a decent view.\n",
    "enums = 10\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "Temps = torch.zeros((RES**2,3))\n",
    "i, j = 0, 0\n",
    "\n",
    "x = torch.linspace(-5,5,RES)\n",
    "\n",
    "Temps[:,:2] = torch.cartesian_prod(x, x)\n",
    "probs = torch.zeros((1,5)).to(DEVICE)\n",
    "\n",
    "for j in range(enums):\n",
    "    probs = probs + torch.exp(net4_GC2(Temps[:,0:2])) #This is done enums times, each one will be different due to stochastic.\n",
    "probs = probs/enums\n",
    "\n",
    "Temps[:,2] = -torch.sum(probs * torch.log(probs),dim = 1)\n",
    "print(torch.max(Temps[:,2]),torch.min(Temps[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('vMF Uncertainty Plot 42epocsLR0.1 3*10epocs, NII in between.')\n",
    "#xy = [0,0.2,0.4,0.6,0.8,1]\n",
    "#z = xy\n",
    "plot = plt.scatter(Temps[:,0].detach().numpy(), Temps[:,1].detach().numpy(),vmin = 0, vmax = 1.4, s=221000/(RES**2), c=Temps[:,2].detach().numpy(), cmap='RdYlBu', marker='s')\n",
    "plt.gcf().set_size_inches((14, 12))\n",
    "plt.colorbar(plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43a9faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RES = 1028\n",
    "enums = 10\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "Temps2 = torch.zeros((RES**2,3))\n",
    "i, j = 0, 0\n",
    "\n",
    "x = torch.linspace(-5,5,RES)\n",
    "\n",
    "Temps2[:,:2] = torch.cartesian_prod(x, x)\n",
    "probs = torch.zeros((1,5)).to(DEVICE)\n",
    "\n",
    "for j in range(enums):\n",
    "    probs = probs + torch.exp(net3(Temps2[:,0:2])) #This is done enums times, each one will be different due to stochastic.\n",
    "probs = probs/enums\n",
    "\n",
    "Temps2[:,2] = -torch.sum(probs * torch.log(probs),dim = 1)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1bfba7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('Gaussian Uncertainty Plot')\n",
    "#xy = [0,0.2,0.4,0.6,0.8,1]\n",
    "#z = xy\n",
    "plot2 = plt.scatter(Temps2[:,0].detach().numpy(), Temps2[:,1].detach().numpy(),vmin = 0, vmax = 1.4, s=221000/(RES**2), c=Temps2[:,2].detach().numpy(), cmap='RdYlBu', marker='s')\n",
    "plt.gcf().set_size_inches((14, 12))\n",
    "plt.colorbar(plot2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 1028\n",
    "enums = 10\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "Temps2 = torch.zeros((RES**2,3))\n",
    "i, j = 0, 0\n",
    "\n",
    "x = torch.linspace(-5,5,RES)\n",
    "\n",
    "Temps2[:,:2] = torch.cartesian_prod(x, x)\n",
    "probs = torch.zeros((1,5)).to(DEVICE)\n",
    "\n",
    "for j in range(enums):\n",
    "    probs = probs + torch.exp(net3(Temps2[:,0:2])) #This is done enums times, each one will be different due to stochastic.\n",
    "probs = probs/enums\n",
    "\n",
    "Temps2[:,2] = -torch.sum(probs * torch.log(probs),dim = 1)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4db301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(42069)\n",
    "torch.manual_seed(42069)\n",
    "\n",
    "PL = [torch.Tensor([1-3, 1-3]),torch.Tensor([5-3, 1-3]),torch.Tensor([1-3, 5-3]),torch.Tensor([5-3, 5-3]),torch.Tensor([3-3, 3-3])]\n",
    "cov = torch.eye(2)\n",
    "n = 50\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=500)\n",
    "\n",
    "DF = torch.zeros((len(PL),n,1,3)) #multiple of 4\n",
    "for i, MU in enumerate(PL): #enumerate starts from and including 0.\n",
    "    distrib = torch.distributions.MultivariateNormal(loc=MU, covariance_matrix=cov)\n",
    "    DATA_ = distrib.sample((n,1))\n",
    "\n",
    "    DATA  = torch.zeros([n, 1, 3])\n",
    "    DATA[:,:,:2] = DATA_\n",
    "    DATA[:,:,2]  = i\n",
    "    DF[i,:,:,:] = DATA\n",
    "    \n",
    "    x = DATA[:,0,0]\n",
    "    y = DATA[:,0,1]\n",
    "    plt.plot(x,y,'.',markersize=1.25)\n",
    "plt.show()\n",
    "\n",
    "C = int(3*n/4)\n",
    "\n",
    "#DATA_train = torch.zeros((len(PL)*C,3))\n",
    "#DATA_test = torch.zeros((len(PL)*(n-C),3))\n",
    "\n",
    "DATA = DF.reshape(len(PL)*n,3)\n",
    "#print('DATA:',DATA,'len(DATA):',len(DATA),'mean dtrain:',DATA.mean(axis=0)[2])\n",
    "\n",
    "#data_mean = DATA.mean(axis=1)[0:2]\n",
    "#data_std = DATA.std(axis=1)[0:2]\n",
    "\n",
    "#DATA[:,0:2] = (DATA[:,0:2]  - data_mean)/data_std\n",
    "#print('DATA normalized:',DATA,'len(DATA) normalized:',len(DATA),'mean dtrain normalized:',DATA.mean(axis=0)[2])\n",
    "tr_ids = np.random.choice(n*5, n*4, replace = False)\n",
    "\n",
    "\n",
    "\n",
    "dtrain = DATA[tr_ids,:]\n",
    "dtest = DATA[-tr_ids,:]\n",
    "\n",
    "#print('\\n','dtrain:',dtrain, 'len(dtrain):',len(dtrain),'mean dtrain:',dtrain.mean(axis=0)[2])\n",
    "#print('\\n','dtest:',dtest, 'len(dtest):',len(dtest),'mean dtest:',dtest.mean(axis=0)[2])\n",
    "\n",
    "import math\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "\n",
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "# define the parameters\n",
    "\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 50\n",
    "TEST_BATCH_SIZE = 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "l1shape=(2, 5)\n",
    "l2shape=(5, 5)\n",
    "l3shape=(5, 5)\n",
    "l4shape=(5, 5)\n",
    "l5shape=(5, 5)\n",
    "l6shape=(5, 5)\n",
    "layershapes = [l1shape, l2shape, l3shape, l4shape]\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "data_shape = (0,2,2,3)\n",
    "\n",
    "epochs = 30\n",
    "trtimes  = np.zeros(epochs)\n",
    "#w_mu = [w_mu1, w_mu2, w_mu3, w_mu4]\n",
    "#b_mu = [b_mu1, b_mu2, b_mu3, b_mu4]\n",
    "\n",
    "#w_mu_nodewise = [w_mu1_nodewise,w_mu2_nodewise,w_mu3_nodewise,w_mu4_nodewise]\n",
    "#b_mu_nodewise = [b_mu1_nodewise,b_mu2_nodewise,b_mu3_nodewise,b_mu4_nodewise]\n",
    "# make inference on 10 networks\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net3 = FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='Gaussian',\n",
    "                                #b_kappa=torch.Tensor(1).uniform_(4,4.1),\n",
    "                                #w_kappa=torch.Tensor(1).uniform_(6,6.1),\n",
    "                                Temper = 1,classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net3.parameters(), lr=0.04)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net3, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100, shape = data_shape)\n",
    "        print('max:',net3.weight_mu[1].max())\n",
    "        print('norm:',torch.norm(net3.weight_mu[1]))\n",
    "\n",
    "    res = test_ensemble.test_ensemble(net3,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "\n",
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net4 = FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='vmf',\n",
    "                                b_kappa=torch.Tensor(1).uniform_(4.0,4.1),\n",
    "                                w_kappa=torch.Tensor(1).uniform_(6.0,6.1),\n",
    "                                Temper = 1,classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net4.parameters(), lr=0.14)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net4, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100, shape = data_shape)\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    #res = test_ensemble.test_ensemble(net4,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "\n",
    "GC_intensity = 1\n",
    "\n",
    "#Kanskje er det en god idee å også arve kappaene, men halvere dem? Hva om vi også kjører på med litt normalizering av mu-ene her?\n",
    "#Hva med adaptiv kappa per lag? Det er jo bare en kappa per bias-lag.\n",
    "\n",
    "w_mu = []\n",
    "for i in range(len(net4.weight_mu)):\n",
    "    #print('\\n','torch.norm(net4.weight_mu[i]):',torch.norm(net4.weight_mu[i]))\n",
    "    w_mu.append(net4.weight_mu[i]/torch.norm(net4.weight_mu[i]))\n",
    "    #print('\\n','norm w_mu[i]',torch.norm(w_mu[i]))\n",
    "    \n",
    "b_mu = []\n",
    "for i in range(len(net4.bias_mu)):\n",
    "    #print('\\n','torch.norm(net4.bais_mu[i]):',torch.norm(net4.bias_mu[i]))\n",
    "    b_mu.append(net4.bias_mu[i]/torch.norm(net4.bias_mu[i]))\n",
    "    #print('\\n','norm b_mu[i]',torch.norm(b_mu[i]))\n",
    "    \n",
    "b_rho= []\n",
    "for i in range(len(net4.bias_rho)):\n",
    "    b_rho.append(net4.bias_rho[i]*GC_intensity)\n",
    "\n",
    "w_rho= []\n",
    "for i in range(len(net4.weight_rho)):\n",
    "    w_rho.append(net4.weight_rho[i]*GC_intensity)\n",
    "\n",
    "#print('b_rho:',b_rho, '\\n','w_rho:',b_mu)\n",
    "\n",
    "epochs = 10\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net4_GC = FVMF.BayesianNetwork(w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='vmf',\n",
    "                                b_kappa= b_rho, #torch.Tensor(1).uniform_(1.0,3.1), \n",
    "                                w_kappa= w_rho, #torch.Tensor(1).uniform_(2.0,4.1), \n",
    "                                Temper = 1,classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net4_GC.parameters(), lr=0.07)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net4_GC, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100, shape = data_shape)\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    #res = test_ensemble.test_ensemble(net4_GC,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "#print('This is the loss with the Gradient Capture method/Normalized Initialization Inheritance')\n",
    "\n",
    "#Ok, så kanskje drit i gradient-capture-method. Men! Husk hva du har vist her nå med kun normalisering av vektene, Du får trent nettet\n",
    "#Videre, og det fungerer faktisk. I tilfellet hvor du ikke gjør det ender du faktisk ikke bare opp med dårlig ytelse, men\n",
    "#hele nettet brekker!!!!!!!!!!!\n",
    "\n",
    "GC_intensity = 1\n",
    "\n",
    "#Kanskje er det en god idee å også arve kappaene, men halvere dem? Hva om vi også kjører på med litt normalizering av mu-ene her?\n",
    "#Hva med adaptiv kappa per lag? Det er jo bare en kappa per bias-lag. GC fungerer rett og slett ikke.\n",
    "\n",
    "w_mu = []\n",
    "for i in range(len(net4_GC.weight_mu)):\n",
    "    #print('\\n','torch.norm(net4.weight_mu[i]):',torch.norm(net4.weight_mu[i]))\n",
    "    w_mu.append(net4_GC.weight_mu[i]/torch.norm(net4_GC.weight_mu[i]))\n",
    "    #print('\\n','norm w_mu[i]',torch.norm(w_mu[i]))\n",
    "    \n",
    "b_mu = []\n",
    "for i in range(len(net4_GC.bias_mu)):\n",
    "    #print('\\n','torch.norm(net4.bais_mu[i]):',torch.norm(net4.bias_mu[i]))\n",
    "    b_mu.append(net4_GC.bias_mu[i]/torch.norm(net4_GC.bias_mu[i]))\n",
    "    #print('\\n','norm b_mu[i]',torch.norm(b_mu[i]))\n",
    "    \n",
    "b_rho= []\n",
    "for i in range(len(net4_GC.bias_rho)):\n",
    "    b_rho.append(net4_GC.bias_rho[i]*GC_intensity)\n",
    "\n",
    "w_rho= []\n",
    "for i in range(len(net4_GC.weight_rho)):\n",
    "    w_rho.append(net4_GC.weight_rho[i]*GC_intensity)\n",
    "\n",
    "#print('b_rho:',b_rho, '\\n','w_rho:',b_mu)\n",
    "\n",
    "epochs = 10\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net4_GC2 = FVMF.BayesianNetwork(w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='vmf',\n",
    "                                b_kappa= b_rho, #torch.Tensor(1).uniform_(1.0,3.1), \n",
    "                                w_kappa= w_rho, #torch.Tensor(1).uniform_(2.0,4.1), \n",
    "                                Temper = 1,classification = 'classification')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net4_GC2.parameters(), lr=0.035)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net4_GC2, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100, shape = data_shape)\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    res = test_ensemble.test_ensemble(net4_GC2,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "print('This is the loss with the Gradient Capture method/Normalized Initialization Inheritance, applied twice')\n",
    "\n",
    "#Ok, så kanskje drit i gradient-capture-method. Men! Husk hva du har vist her nå med kun normalisering av vektene, Du får trent nettet\n",
    "#Videre, og det fungerer faktisk. I tilfellet hvor du ikke gjør det ender du faktisk ikke bare opp med dårlig ytelse, men\n",
    "#hele nettet brekker!!!!!!!!!!!\n",
    "\n",
    "RES = 1028 #Ca. Full HD resolution. If you struggle with memory adjust this parameter down. 64 for example already gives a decent view.\n",
    "enums = 10\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "Temps = torch.zeros((RES**2,3))\n",
    "i, j = 0, 0\n",
    "\n",
    "x = torch.linspace(-5,5,RES)\n",
    "\n",
    "Temps[:,:2] = torch.cartesian_prod(x, x)\n",
    "probs = torch.zeros((1,5)).to(DEVICE)\n",
    "\n",
    "for j in range(enums):\n",
    "    probs = probs + torch.exp(net4_GC2(Temps[:,0:2])) #This is done enums times, each one will be different due to stochastic.\n",
    "probs = probs/enums\n",
    "\n",
    "Temps[:,2] = -torch.sum(probs * torch.log(probs),dim = 1)\n",
    "print(torch.max(Temps[:,2]),torch.min(Temps[:,2]))\n",
    "\n",
    "plt.title('vMF Uncertainty Plot 42epocsLR0.1 3*10epocs, NII in between.')\n",
    "#xy = [0,0.2,0.4,0.6,0.8,1]\n",
    "#z = xy\n",
    "plot = plt.scatter(Temps[:,0].detach().numpy(), Temps[:,1].detach().numpy(),vmin = 0, vmax = 1.4, s=221000/(RES**2), c=Temps[:,2].detach().numpy(), cmap='RdYlBu', marker='s')\n",
    "plt.gcf().set_size_inches((14, 12))\n",
    "plt.colorbar(plot)\n",
    "plt.show()\n",
    "\n",
    "RES = 1028\n",
    "enums = 10\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "Temps2 = torch.zeros((RES**2,3))\n",
    "i, j = 0, 0\n",
    "\n",
    "x = torch.linspace(-5,5,RES)\n",
    "\n",
    "Temps2[:,:2] = torch.cartesian_prod(x, x)\n",
    "probs = torch.zeros((1,5)).to(DEVICE)\n",
    "\n",
    "for j in range(enums):\n",
    "    probs = probs + torch.exp(net3(Temps2[:,0:2])) #This is done enums times, each one will be different due to stochastic.\n",
    "probs = probs/enums\n",
    "\n",
    "Temps2[:,2] = -torch.sum(probs * torch.log(probs),dim = 1)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21acd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
