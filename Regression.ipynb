{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5099aac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0375e+00,  3.9931e-01,  1.4072e-01,  ..., -3.8084e-02,\n",
      "         -5.6848e-04, -1.1847e+00],\n",
      "        [ 2.0375e+00,  3.9931e-01,  2.0051e-01,  ..., -7.3575e-03,\n",
      "         -1.3260e-01, -8.2473e-01],\n",
      "        [ 2.0375e+00,  3.9931e-01, -1.5547e-01,  ..., -4.6846e-01,\n",
      "         -8.3970e-01, -1.0649e+00],\n",
      "        ...,\n",
      "        [ 1.8122e+01,  9.3640e+00,  5.2303e-01,  ...,  3.3039e-01,\n",
      "         -2.7718e-01,  1.5732e-01],\n",
      "        [ 1.8122e+01,  9.3640e+00,  5.4044e-01,  ...,  3.5240e-01,\n",
      "         -2.1551e-01,  3.5602e-01],\n",
      "        [ 1.8122e+01,  9.3640e+00,  5.6962e-01,  ...,  3.8670e-01,\n",
      "         -4.0853e-02,  3.9589e-01]])\n",
      "GPUs are used!\n",
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "FVMF RELOADED\n",
      "0\n",
      "Random Init Utilized\n",
      "1\n",
      "loss: tensor(1.2491, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1212, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "loss: tensor(1.2371, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1101, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "loss: tensor(1.2343, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1055, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "loss: tensor(1.2112, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1015, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "loss: tensor(1.1772, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0983, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "loss: tensor(1.1435, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0480, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "loss: tensor(1.1265, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0301, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "loss: tensor(1.0770, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0381, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "loss: tensor(1.0574, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0080, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "loss: tensor(1.0768, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0132, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "\n",
      " This is the TSS 1400.4136962890625 This is the SS_res: 62.81597137451172 \n",
      " This is the R^2 on testset: tensor(0.9551, device='cuda:1')\n",
      "Gaussian 10 epoch\n",
      "0\n",
      "Random Init Utilized\n",
      "1\n",
      "loss: tensor(1.2491, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1212, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "loss: tensor(1.2371, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1101, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "loss: tensor(1.2343, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1055, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "loss: tensor(1.2112, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1015, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "loss: tensor(1.1772, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0983, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "loss: tensor(1.1435, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0480, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "loss: tensor(1.1265, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0301, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "loss: tensor(1.0770, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0381, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "loss: tensor(1.0574, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0080, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "loss: tensor(1.0768, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0132, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "loss: tensor(1.0343, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0046, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "loss: tensor(1.0200, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0040, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "loss: tensor(0.9967, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0031, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "loss: tensor(0.9978, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0053, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "loss: tensor(0.9950, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0225, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "loss: tensor(0.9920, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0064, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "loss: tensor(0.9972, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0434, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "loss: tensor(0.9748, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0122, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "loss: tensor(0.9566, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0068, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "loss: tensor(0.9395, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0079, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "loss: tensor(0.9359, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0244, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "loss: tensor(0.9343, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0222, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "loss: tensor(0.8750, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0024, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "loss: tensor(0.9129, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0145, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "loss: tensor(0.8554, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0205, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "26\n",
      "loss: tensor(0.8739, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0038, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "27\n",
      "loss: tensor(0.8300, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0038, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "28\n",
      "loss: tensor(0.8594, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0149, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "29\n",
      "loss: tensor(0.8256, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0045, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "30\n",
      "loss: tensor(0.8746, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0413, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "\n",
      " This is the TSS 1400.4136962890625 This is the SS_res: 31.145498275756836 \n",
      " This is the R^2 on testset: tensor(0.9778, device='cuda:1')\n",
      "Gaussian 30 epoch\n",
      "0\n",
      "Random Init Utilized\n",
      "1\n",
      "loss: tensor(1.2491, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1212, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "loss: tensor(1.2371, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1101, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "loss: tensor(1.2343, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1055, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "loss: tensor(1.2112, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1015, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "loss: tensor(1.1772, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0983, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "loss: tensor(1.1435, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0480, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "loss: tensor(1.1265, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0301, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "loss: tensor(1.0770, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0381, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "loss: tensor(1.0574, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0080, device='cuda:1', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "loss: tensor(1.0768, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0132, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "loss: tensor(1.0343, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0046, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "loss: tensor(1.0200, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0040, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "loss: tensor(0.9967, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0031, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "loss: tensor(0.9978, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0053, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "loss: tensor(0.9950, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0225, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "loss: tensor(0.9920, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0064, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "loss: tensor(0.9972, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0434, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "loss: tensor(0.9748, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0122, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "loss: tensor(0.9566, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0068, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "loss: tensor(0.9395, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0079, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "loss: tensor(0.9359, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0244, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "loss: tensor(0.9343, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0222, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "loss: tensor(0.8750, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0024, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "loss: tensor(0.9129, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0145, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "loss: tensor(0.8554, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0205, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "26\n",
      "loss: tensor(0.8739, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0038, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "27\n",
      "loss: tensor(0.8300, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0038, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "28\n",
      "loss: tensor(0.8594, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0149, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "29\n",
      "loss: tensor(0.8256, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0045, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "30\n",
      "loss: tensor(0.8746, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0413, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "31\n",
      "loss: tensor(0.8464, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0341, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "32\n",
      "loss: tensor(0.8251, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0051, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "33\n",
      "loss: tensor(0.8118, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0216, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "34\n",
      "loss: tensor(0.7892, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0144, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "35\n",
      "loss: tensor(0.7810, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0100, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "36\n",
      "loss: tensor(0.7700, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0086, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "37\n",
      "loss: tensor(0.8000, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0455, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "38\n",
      "loss: tensor(0.7601, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0172, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "39\n",
      "loss: tensor(0.7207, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0198, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "40\n",
      "loss: tensor(0.7028, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0047, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "41\n",
      "loss: tensor(0.7174, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0119, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "42\n",
      "loss: tensor(0.6945, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0042, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "43\n",
      "loss: tensor(0.6993, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0362, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "44\n",
      "loss: tensor(0.6647, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0033, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "45\n",
      "loss: tensor(0.6803, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0137, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "46\n",
      "loss: tensor(0.6618, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0326, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "47\n",
      "loss: tensor(0.6734, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0197, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "48\n",
      "loss: tensor(0.6657, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0035, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "49\n",
      "loss: tensor(0.6346, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0128, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "50\n",
      "loss: tensor(0.6050, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0073, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "\n",
      " This is the TSS 1400.4136962890625 This is the SS_res: 108.7964096069336 \n",
      " This is the R^2 on testset: tensor(0.9223, device='cuda:1')\n",
      "Gaussian 50 epoch\n",
      "0\n",
      "Random Init Utilized\n",
      "1\n",
      "loss: tensor(0.1516, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1516, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.2483, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.6155, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "2\n",
      "loss: tensor(0.0354, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0354, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.7255, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.8358, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "3\n",
      "loss: tensor(0.0332, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0332, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.9328, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(10.0912, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "4\n",
      "loss: tensor(0.0460, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0460, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.2777, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(11.7586, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "5\n",
      "loss: tensor(0.0586, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0586, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.8862, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.0257, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "6\n",
      "loss: tensor(0.0973, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0973, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.9442, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.1906, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "7\n",
      "loss: tensor(0.0288, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0288, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.5393, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.2525, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "8\n",
      "loss: tensor(0.0061, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0061, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.6048, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.4807, device='cuda:1', grad_fn=<NormBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "loss: tensor(0.0447, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0447, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.5952, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.5565, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "10\n",
      "loss: tensor(0.0078, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0078, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.7024, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.5590, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "11\n",
      "loss: tensor(0.1408, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1408, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.6174, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.6328, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "12\n",
      "loss: tensor(0.0231, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0231, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.7358, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.6483, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "13\n",
      "loss: tensor(0.0109, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0109, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.6785, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.1548, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "14\n",
      "loss: tensor(0.1956, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1956, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.6787, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.3167, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "15\n",
      "loss: tensor(0.0063, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0063, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.7195, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.4421, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "16\n",
      "loss: tensor(0.0255, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0255, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.7772, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.2227, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "17\n",
      "loss: tensor(0.0095, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0095, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.9350, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.3985, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "18\n",
      "loss: tensor(0.0067, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0067, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.8964, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.4696, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "19\n",
      "loss: tensor(0.0271, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0271, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.8580, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.1450, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "20\n",
      "loss: tensor(0.0048, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0048, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.7992, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.7635, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "21\n",
      "loss: tensor(0.0175, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0175, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.8825, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.9629, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "22\n",
      "loss: tensor(0.0175, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0175, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.2220, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.2258, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "23\n",
      "loss: tensor(0.0233, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0233, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.2740, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.0121, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "24\n",
      "loss: tensor(0.0652, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0652, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.2763, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.0901, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "25\n",
      "loss: tensor(0.0034, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0034, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.4786, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.2141, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "26\n",
      "loss: tensor(0.1083, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1083, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.4353, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.0371, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "27\n",
      "loss: tensor(0.0600, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0600, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.3183, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.3425, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "28\n",
      "loss: tensor(0.0043, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0043, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.4108, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.5816, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "29\n",
      "loss: tensor(0.1962, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1962, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.2769, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.8259, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "30\n",
      "loss: tensor(0.0109, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0109, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.3670, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.7191, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "\n",
      " This is the TSS 1400.4136962890625 This is the SS_res: 40.832279205322266 \n",
      " This is the R^2 on testset: tensor(0.9708, device='cuda:1')\n",
      "vMF 30 epoch\n",
      "0\n",
      "Random Init Utilized\n",
      "1\n",
      "loss: tensor(0.1516, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1516, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.2483, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.6155, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "2\n",
      "loss: tensor(0.0354, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0354, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.7255, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.8358, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "3\n",
      "loss: tensor(0.0332, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0332, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.9328, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(10.0912, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "4\n",
      "loss: tensor(0.0460, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0460, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.2777, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(11.7586, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "5\n",
      "loss: tensor(0.0586, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0586, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.8862, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.0257, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "6\n",
      "loss: tensor(0.0973, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0973, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.9442, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.1906, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "7\n",
      "loss: tensor(0.0288, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0288, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.5393, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.2525, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "8\n",
      "loss: tensor(0.0061, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0061, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.6048, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.4807, device='cuda:1', grad_fn=<NormBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "loss: tensor(0.0447, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0447, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.5952, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.5565, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "10\n",
      "loss: tensor(0.0078, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0078, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.7024, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.5590, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "11\n",
      "loss: tensor(0.1408, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1408, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.6174, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.6328, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "12\n",
      "loss: tensor(0.0231, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0231, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.7358, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.6483, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "13\n",
      "loss: tensor(0.0109, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0109, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.6785, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.1548, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "14\n",
      "loss: tensor(0.1956, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1956, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.6787, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.3167, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "15\n",
      "loss: tensor(0.0063, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0063, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.7195, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.4421, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "16\n",
      "loss: tensor(0.0255, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0255, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.7772, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.2227, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "17\n",
      "loss: tensor(0.0095, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0095, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.9350, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.3985, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "18\n",
      "loss: tensor(0.0067, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0067, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.8964, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.4696, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "19\n",
      "loss: tensor(0.0271, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0271, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.8580, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.1450, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "20\n",
      "loss: tensor(0.0048, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0048, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.7992, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.7635, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "21\n",
      "loss: tensor(0.0175, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0175, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.8825, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.9629, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "22\n",
      "loss: tensor(0.0175, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0175, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.2220, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.2258, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "23\n",
      "loss: tensor(0.0233, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0233, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.2740, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.0121, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "24\n",
      "loss: tensor(0.0652, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0652, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.2763, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.0901, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "25\n",
      "loss: tensor(0.0034, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0034, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.4786, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.2141, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "26\n",
      "loss: tensor(0.1083, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1083, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.4353, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.0371, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "27\n",
      "loss: tensor(0.0600, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0600, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.3183, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.3425, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "28\n",
      "loss: tensor(0.0043, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0043, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.4108, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.5816, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "29\n",
      "loss: tensor(0.1962, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1962, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.2769, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.8259, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "30\n",
      "loss: tensor(0.0109, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0109, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.3670, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.7191, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "31\n",
      "loss: tensor(0.0075, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0075, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.4510, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.4314, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "32\n",
      "loss: tensor(0.0075, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0075, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.7242, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.7429, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "33\n",
      "loss: tensor(0.0058, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0058, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.8551, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(16.0285, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "34\n",
      "loss: tensor(0.0292, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0292, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.8629, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(16.1328, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "35\n",
      "loss: tensor(0.0276, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0276, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.8553, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(16.0174, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "36\n",
      "loss: tensor(0.0310, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0310, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.8972, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.9557, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "37\n",
      "loss: tensor(0.0381, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0381, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.7463, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(16.3077, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "38\n",
      "loss: tensor(0.0030, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0030, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.8173, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(16.4006, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "39\n",
      "loss: tensor(0.0026, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0026, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.0844, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(16.7300, device='cuda:1', grad_fn=<NormBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "loss: tensor(0.0112, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0112, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.8043, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(16.6114, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "41\n",
      "loss: tensor(0.0265, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0265, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.7676, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(16.9864, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "42\n",
      "loss: tensor(0.0567, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0567, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.2232, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(17.7289, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "43\n",
      "loss: tensor(0.0322, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0322, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.2073, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(18.2362, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "44\n",
      "loss: tensor(0.0029, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0029, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.2855, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(17.9290, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "45\n",
      "loss: tensor(0.0042, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0042, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.6433, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(17.8372, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "46\n",
      "loss: tensor(0.0046, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0046, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.6769, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(18.1994, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "47\n",
      "loss: tensor(0.0091, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0091, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.8740, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(18.4599, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "48\n",
      "loss: tensor(0.0111, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0111, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.8929, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(18.3169, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "49\n",
      "loss: tensor(0.0468, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0468, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.8957, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(18.4105, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "50\n",
      "loss: tensor(0.0903, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0903, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.8445, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(18.6088, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "\n",
      " This is the TSS 1400.4136962890625 This is the SS_res: 44.79217529296875 \n",
      " This is the R^2 on testset: tensor(0.9680, device='cuda:1')\n",
      "vMF 10 epoch\n",
      "0\n",
      "1\n",
      "loss: tensor(5.1633, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0136, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "loss: tensor(5.0709, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0026, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "loss: tensor(5.2645, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0113, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "loss: tensor(5.4865, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0101, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "loss: tensor(5.3609, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0041, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "loss: tensor(5.1154, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0396, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "loss: tensor(5.3598, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0276, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "loss: tensor(5.4531, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0379, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "loss: tensor(5.4250, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0066, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "loss: tensor(5.2118, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0071, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "loss: tensor(5.3038, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0050, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "loss: tensor(5.2989, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0107, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "loss: tensor(5.3657, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0116, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "loss: tensor(5.1381, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0067, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "loss: tensor(5.4210, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0082, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "loss: tensor(5.2245, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0023, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "loss: tensor(5.5981, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0024, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "loss: tensor(4.9659, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0297, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "loss: tensor(5.2687, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0846, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "loss: tensor(5.3159, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0045, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "\n",
      " This is the TSS 1400.4136962890625 This is the SS_res: 53.441688537597656 \n",
      " This is the R^2 on testset: tensor(0.9618, device='cuda:1')\n",
      "vMF 10+20 epoch NII\n",
      "0\n",
      "1\n",
      "loss: tensor(5.1794, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0127, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "loss: tensor(5.1261, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0160, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "loss: tensor(5.3526, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0384, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "loss: tensor(5.5711, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0086, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "loss: tensor(5.4576, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0053, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "loss: tensor(5.2056, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0225, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "loss: tensor(5.4767, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0031, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "loss: tensor(5.6554, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0611, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "loss: tensor(5.6143, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0040, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "loss: tensor(5.4477, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0212, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "loss: tensor(5.5581, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0027, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "loss: tensor(5.5891, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0150, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "loss: tensor(5.6750, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0148, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "loss: tensor(5.4586, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0045, device='cuda:1', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "loss: tensor(5.7819, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0097, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "loss: tensor(5.6054, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0076, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "loss: tensor(5.9940, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0033, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "loss: tensor(5.3382, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0053, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "loss: tensor(5.6498, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0173, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "loss: tensor(5.7898, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0112, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "\n",
      " This is the TSS 1400.4136962890625 This is the SS_res: 28.397449493408203 \n",
      " This is the R^2 on testset: tensor(0.9797, device='cuda:1')\n",
      "vMF 10+20+20 epoch NII\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "DATA = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00448/carbon_nanotubes.csv',sep = ';', decimal=\",\")\n",
    "#This is the carbon nanotubes data.\n",
    "DATA = DATA.astype(float)\n",
    "#print(DATA)\n",
    "#print(DATA)\n",
    "DATA = DATA.to_numpy()\n",
    "DATA = torch.from_numpy(DATA)\n",
    "#print(DATA)\n",
    "np.random.seed(42069)\n",
    "\n",
    "DATA = DATA.type(torch.float32)\n",
    "\n",
    "data_mean = DATA.mean(axis=1)[0:8]\n",
    "data_std = DATA.std(axis=1)[0:8]\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "\n",
    "#DATA[:,7] = 5*DATA[:,1] + 3*DATA[:,6] + 2*DATA[:,3] + 1*DATA[:,2] \n",
    "DATA[:,0:8] = (DATA[:,0:8]  - data_mean)/data_std\n",
    "#print('DATA normalized:',DATA,'len(DATA) normalized:',len(DATA),'mean dtrain normalized:',DATA.mean(axis=0)[2])\n",
    "tr_ids = np.random.choice(10721, 6000, replace = False)\n",
    "\n",
    "dTRAIN_CARBON = DATA[tr_ids,:]\n",
    "dTEST_CARBON = DATA[-tr_ids,:]\n",
    "\n",
    "print(DATA)\n",
    "\n",
    "import math\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "COND_OPT = False\n",
    "CLASSES = 1\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 30\n",
    "epochs =10\n",
    "TEST_BATCH_SIZE = 6000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "data_shape = (0,7,7,8)\n",
    "l1shape=(7, 7)\n",
    "l2shape=(7, 7)\n",
    "l3shape=(7, 7)\n",
    "l4shape=(7, 1)\n",
    "layershapes = [l1shape, l2shape, l3shape, l4shape]\n",
    "\n",
    "trtimes  = []\n",
    "\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net5= FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='Gaussian',\n",
    "                                #b_kappa=torch.Tensor(1).uniform_(3,3.1),\n",
    "                                #w_kappa=torch.Tensor(1).uniform_(5,5.1),\n",
    "                                Temper = 0.1,\n",
    "                                classification='Regression')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net5.parameters(), lr=0.0007)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train = FVMF.train(net5, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)    \n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(net5,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "    print('Gaussian 10 epoch')\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "\n",
    "epochs = 30\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net5= FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='Gaussian',\n",
    "                                #b_kappa=torch.Tensor(1).uniform_(3,3.1),\n",
    "                                #w_kappa=torch.Tensor(1).uniform_(5,5.1),\n",
    "                                Temper = 0.1,\n",
    "                                classification='Regression')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net5.parameters(), lr=0.0007)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train = FVMF.train(net5, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)    \n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "    \n",
    "    loss, outputs, output = test_ensemble.test_ensemble(net5,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "    print('Gaussian 30 epoch')\n",
    "\n",
    "epochs = 50\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net5= FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='Gaussian',\n",
    "                                #b_kappa=torch.Tensor(1).uniform_(3,3.1),\n",
    "                                #w_kappa=torch.Tensor(1).uniform_(5,5.1),\n",
    "                                Temper = 0.1,\n",
    "                                classification='Regression')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net5.parameters(), lr=0.0007)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train = FVMF.train(net5, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)    \n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "    \n",
    "    loss, outputs, output = test_ensemble.test_ensemble(net5,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "    print('Gaussian 50 epoch')\n",
    "\n",
    "trtimes  = []\n",
    "epochs = 30\n",
    "\n",
    "#Note, for this regression task, the last 7 to 1 layer has a Gaussian VD, where we kill the prior and simply optimize with MLE.\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    vMFRegression30= FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='vmf',\n",
    "                                b_kappa=torch.Tensor(1).uniform_(3,3.1),\n",
    "                                w_kappa=torch.Tensor(1).uniform_(7.5,7.6),\n",
    "                                Temper = 0,classification='Regression',NODEFORCE =False)\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(vMFRegression30.parameters(), lr=0.11)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train = FVMF.train(vMFRegression30, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)    \n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        print('max:',vMFRegression30.weight_mu[1].max())\n",
    "        print('norm:',torch.norm(vMFRegression30.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(vMFRegression30,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "    print('vMF 30 epoch')\n",
    "\n",
    "trtimes  = []\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "#This test crashes. Likely due to the Ghost Mu's.\n",
    "##Note, for this regression task, the last 7 to 1 layer has a Gaussian VD, where we kill the prior and simply optimize with MLE.\n",
    "#for i in range(0, 1):\n",
    "#    print(i)\n",
    "#    torch.manual_seed(i)\n",
    "#    vMFRegression50= FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "#                                #w_mu = None, b_mu = None,\n",
    "#                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "#                                layershapes = layershapes,\n",
    "#                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "#                                VD='vmf',\n",
    "#                                b_kappa=torch.Tensor(1).uniform_(3,3.1),\n",
    "#                                w_kappa=torch.Tensor(1).uniform_(7.5,7.6),\n",
    "#                                Temper = 0,classification='Regression',NODEFORCE =False)\n",
    "#    \n",
    "#    #for j,p in enumerate(net2.l1.parameters()):\n",
    "#    #    p.requires_grad_(False)\n",
    "#    #    \n",
    "#    #for j,p in enumerate(net2.l2.parameters()):\n",
    "#    #    p.requires_grad_(False)\n",
    "#    \n",
    "#    optimizer = optim.Adam(vMFRegression.parameters(), lr=0.11)\n",
    "#    \n",
    "#    \n",
    "#    for epoch in range(epochs):\n",
    "#        train = FVMF.train(vMFRegression50, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "#                                    shape = data_shape,CLASSES = 1)    \n",
    "#        trtimes.append(train[1].detach().cpu().numpy())\n",
    "#        print('max:',vMFRegression30.weight_mu[1].max())\n",
    "#        print('norm:',torch.norm(vMFRegression30.weight_mu[1]))\n",
    "#\n",
    "#    loss, outputs, output = test_ensemble.test_ensemble(vMFRegression50,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "#                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "#    print('vMF 50 epoch')\n",
    "#\n",
    "#trtimes  = []\n",
    "\n",
    "\n",
    "#Note, for this regression task, the last 7 to 1 layer has a Gaussian VD, where we kill the prior and simply optimize with MLE.\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    vMFRegression= FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='vmf',\n",
    "                                b_kappa=torch.Tensor(1).uniform_(3,3.1),\n",
    "                                w_kappa=torch.Tensor(1).uniform_(7.5,7.6),\n",
    "                                Temper = 0,classification='Regression',NODEFORCE =False)\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(vMFRegression.parameters(), lr=0.11)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train = FVMF.train(vMFRegression, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)    \n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        print('max:',vMFRegression.weight_mu[1].max())\n",
    "        print('norm:',torch.norm(vMFRegression.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(vMFRegression,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "    print('vMF 10 epoch')\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "\n",
    "GC_intensity = 1\n",
    "\n",
    "#Kanskje er det en god idee å også arve kappaene, men halvere dem? Hva om vi også kjører på med litt normalizering av mu-ene her?\n",
    "#Hva med adaptiv kappa per lag? Det er jo bare en kappa per bias-lag. GC fungerer rett og slett ikke.\n",
    "\n",
    "w_mu_R = []\n",
    "for i in range(len(vMFRegression.weight_mu)):\n",
    "    #print('\\n','torch.norm(net4.weight_mu[i]):',torch.norm(net4.weight_mu[i]))\n",
    "    w_mu_R.append(vMFRegression.weight_mu[i]/torch.norm(vMFRegression.weight_mu[i]))\n",
    "    #print('\\n','norm w_mu[i]',torch.norm(w_mu[i]))\n",
    "    \n",
    "b_mu_R = []\n",
    "for i in range(len(vMFRegression.bias_mu)):\n",
    "    #print('\\n','torch.norm(net4.bais_mu[i]):',torch.norm(net4.bias_mu[i]))\n",
    "    b_mu_R.append(vMFRegression.bias_mu[i]/torch.norm(vMFRegression.bias_mu[i]))\n",
    "    #print('\\n','norm b_mu[i]',torch.norm(b_mu[i]))\n",
    "    \n",
    "b_rho_R= []\n",
    "for i in range(len(vMFRegression.bias_rho)):\n",
    "    b_rho_R.append(vMFRegression.bias_rho[i])\n",
    "\n",
    "w_rho_R= []\n",
    "for i in range(len(vMFRegression.weight_rho)):\n",
    "    w_rho_R.append(vMFRegression.weight_rho[i])\n",
    "#For some reason, it seems that the weights are not being properly normalized here. no actually they are, but the norm reported is the norm after 1 optimization step.\n",
    "\n",
    "epochs = 20\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    vMFRegression2= FVMF.BayesianNetwork(w_mu = w_mu_R, b_mu = b_mu_R, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='vmf',\n",
    "                                b_kappa= b_rho_R, #torch.Tensor(1).uniform_(1.0,3.1), \n",
    "                                w_kappa= w_rho_R, #torch.Tensor(1).uniform_(2.0,4.1), \n",
    "                                Temper = 1,classification = 'regression')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(vMFRegression2.parameters(), lr=0.01) #0.01 9727 acc\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train = FVMF.train(vMFRegression2, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)    \n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        #print('max:',vMFRegression2.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(vMFRegression2.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(vMFRegression2,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "    print('vMF 10+20 epoch NII')\n",
    "\n",
    "#Kanskje er det en god idee å også arve kappaene, men halvere dem? Hva om vi også kjører på med litt normalizering av mu-ene her?\n",
    "#Hva med adaptiv kappa per lag? Det er jo bare en kappa per bias-lag. GC fungerer rett og slett ikke.\n",
    "\n",
    "w_mu_R = []\n",
    "for i in range(len(vMFRegression2.weight_mu)):\n",
    "    #print('\\n','torch.norm(net4.weight_mu[i]):',torch.norm(net4.weight_mu[i]))\n",
    "    w_mu_R.append(vMFRegression2.weight_mu[i]/torch.norm(vMFRegression2.weight_mu[i]))\n",
    "    #print('\\n','norm w_mu[i]',torch.norm(w_mu[i]))\n",
    "    \n",
    "b_mu_R = []\n",
    "for i in range(len(vMFRegression2.bias_mu)):\n",
    "    #print('\\n','torch.norm(net4.bais_mu[i]):',torch.norm(net4.bias_mu[i]))\n",
    "    b_mu_R.append(vMFRegression2.bias_mu[i]/torch.norm(vMFRegression2.bias_mu[i]))\n",
    "    #print('\\n','norm b_mu[i]',torch.norm(b_mu[i]))\n",
    "    \n",
    "b_rho_R= []\n",
    "for i in range(len(vMFRegression2.bias_rho)):\n",
    "    b_rho_R.append(vMFRegression2.bias_rho[i])\n",
    "\n",
    "w_rho_R= []\n",
    "for i in range(len(vMFRegression2.weight_rho)):\n",
    "    w_rho_R.append(vMFRegression2.weight_rho[i])\n",
    "#For some reason, it seems that the weights are not being properly normalized here. no actually they are, but the norm reported is the norm after 1 optimization step.\n",
    "\n",
    "epochs = 20\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    vMFRegression3= FVMF.BayesianNetwork(w_mu = w_mu_R, b_mu = b_mu_R, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='vmf',\n",
    "                                b_kappa= b_rho_R, #torch.Tensor(1).uniform_(1.0,3.1), \n",
    "                                w_kappa= w_rho_R, #torch.Tensor(1).uniform_(2.0,4.1), \n",
    "                                Temper = 1,classification = 'regression')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(vMFRegression2.parameters(), lr=0.004) #0.01 9727 acc\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train = FVMF.train(vMFRegression3, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)    \n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        #print('max:',vMFRegression2.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(vMFRegression2.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(vMFRegression3,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "    print('vMF 10+20+20 epoch NII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e9e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
