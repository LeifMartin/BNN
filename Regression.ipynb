{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc0d09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Chiral indice n  Chiral indice m  Initial atomic coordinate u  \\\n",
      "0                    2                1                     0.679005   \n",
      "1                    2                1                     0.717298   \n",
      "2                    2                1                     0.489336   \n",
      "3                    2                1                     0.413957   \n",
      "4                    2                1                     0.334292   \n",
      "...                ...              ...                          ...   \n",
      "10716               12                6                     0.834201   \n",
      "10717               12                6                     0.698374   \n",
      "10718               12                6                     0.923823   \n",
      "10719               12                6                     0.934978   \n",
      "10720               12                6                     0.953664   \n",
      "\n",
      "       Initial atomic coordinate v  Initial atomic coordinate w  \\\n",
      "0                         0.701318                     0.017033   \n",
      "1                         0.642129                     0.231319   \n",
      "2                         0.303751                     0.088462   \n",
      "3                         0.632996                     0.040843   \n",
      "4                         0.543401                     0.159890   \n",
      "...                            ...                          ...   \n",
      "10716                     0.399891                     0.891270   \n",
      "10717                     0.244710                     0.962699   \n",
      "10718                     0.568913                     0.819842   \n",
      "10719                     0.602319                     0.938889   \n",
      "10720                     0.698374                     0.962699   \n",
      "\n",
      "       Calculated atomic coordinates u'  Calculated atomic coordinates v'  \\\n",
      "0                              0.721039                          0.730232   \n",
      "1                              0.738414                          0.656750   \n",
      "2                              0.477676                          0.263221   \n",
      "3                              0.408823                          0.657897   \n",
      "4                              0.303349                          0.558807   \n",
      "...                                 ...                               ...   \n",
      "10716                          0.841858                          0.405882   \n",
      "10717                          0.706555                          0.248416   \n",
      "10718                          0.929403                          0.576284   \n",
      "10719                          0.941844                          0.610608   \n",
      "10720                          0.961243                          0.707812   \n",
      "\n",
      "       Calculated atomic coordinates w'  \n",
      "0                              0.017014  \n",
      "1                              0.232369  \n",
      "2                              0.088712  \n",
      "3                              0.039796  \n",
      "4                              0.157373  \n",
      "...                                 ...  \n",
      "10716                          0.891356  \n",
      "10717                          0.962833  \n",
      "10718                          0.819879  \n",
      "10719                          0.938755  \n",
      "10720                          0.962605  \n",
      "\n",
      "[10721 rows x 8 columns]\n",
      "tensor([[ 1.7186e+00,  2.9862e-01,  2.3448e-02,  1.8096e-02, -9.5999e-01,\n",
      "         -1.1688e+00],\n",
      "        [ 1.7186e+00,  2.9862e-01,  7.5363e-02, -6.1921e-02, -6.5842e-01,\n",
      "         -8.4162e-01],\n",
      "        [ 1.7186e+00,  2.9862e-01, -2.3370e-01, -5.1937e-01, -8.5947e-01,\n",
      "         -1.0599e+00],\n",
      "        ...,\n",
      "        [ 1.5313e+01,  7.9107e+00,  3.5536e-01, -1.6090e-01,  1.6984e-01,\n",
      "          5.0962e-02],\n",
      "        [ 1.5313e+01,  7.9107e+00,  3.7048e-01, -1.1574e-01,  3.3738e-01,\n",
      "          2.3157e-01],\n",
      "        [ 1.5313e+01,  7.9107e+00,  3.9582e-01,  1.4116e-02,  3.7089e-01,\n",
      "          2.6780e-01]])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "DATA = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00448/carbon_nanotubes.csv',sep = ';', decimal=\",\")\n",
    "print(DATA)\n",
    "#This is the carbon nanotubes data.\n",
    "DATA = DATA.drop(\"Calculated atomic coordinates u'\", axis=1) #This dataset has three targets for multivariate regression. \n",
    "DATA = DATA.drop(\"Calculated atomic coordinates v'\", axis=1) #This dataset has three targets for multivariate regression. \n",
    "#But we would like to, for simplicity's sake, stick to univariate regression.\n",
    "DATA = DATA.astype(float)\n",
    "#print(DATA)\n",
    "#print(DATA)\n",
    "DATA = DATA.to_numpy()\n",
    "DATA = torch.from_numpy(DATA)\n",
    "#print(DATA)\n",
    "np.random.seed(42069)\n",
    "\n",
    "DATA = DATA.type(torch.float32)\n",
    "\n",
    "data_mean = DATA.mean(axis=1)[0:6]\n",
    "data_std = DATA.std(axis=1)[0:6]\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "\n",
    "#DATA[:,7] = 5*DATA[:,1] + 3*DATA[:,6] + 2*DATA[:,3] + 1*DATA[:,2] \n",
    "DATA[:,0:6] = (DATA[:,0:6]  - data_mean)/data_std\n",
    "#print('DATA normalized:',DATA,'len(DATA) normalized:',len(DATA),'mean dtrain normalized:',DATA.mean(axis=0)[2])\n",
    "tr_ids = np.random.choice(10721, 6000, replace = False)\n",
    "\n",
    "dTRAIN_CARBON = DATA[tr_ids,:]\n",
    "dTEST_CARBON = DATA[-tr_ids,:]\n",
    "\n",
    "print(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0e59ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "FVMF RELOADED\n",
      "0\n",
      "Random Init Utilized\n",
      "1\n",
      "loss: tensor(0.8478, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.2239, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "loss: tensor(0.7924, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1670, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "loss: tensor(0.7103, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0952, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "loss: tensor(0.7032, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0924, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "loss: tensor(0.6755, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0894, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "loss: tensor(0.6902, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0871, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "loss: tensor(0.7012, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0829, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "loss: tensor(0.6533, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0746, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "loss: tensor(0.6305, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0447, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "loss: tensor(0.6013, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0242, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "\n",
      " This is the TSS 1156.893798828125 This is the SS_res: 291.2354431152344 \n",
      " This is the R^2 on testset: tensor(0.7483, device='cuda:1')\n",
      "Gaussian 10 epoch\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "COND_OPT = False\n",
    "CLASSES = 1\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 30\n",
    "\n",
    "TEST_BATCH_SIZE = 6000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "data_shape = (0,5,5,6)\n",
    "l1shape=(5, 5)\n",
    "l2shape=(5, 5)\n",
    "l3shape=(5, 5)\n",
    "l4shape=(5, 1)\n",
    "layershapes = [l1shape, l2shape, l3shape, l4shape]\n",
    "\n",
    "trtimes  = []\n",
    "\n",
    "epochs =10\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net5= FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='Gaussian',\n",
    "                                #b_kappa=torch.Tensor(1).uniform_(3,3.1),\n",
    "                                #w_kappa=torch.Tensor(1).uniform_(5,5.1),\n",
    "                                Temper = 0.1,\n",
    "                                classification='Regression')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net5.parameters(), lr=0.0007)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train = FVMF.train(net5, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)    \n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        #print('max:',net4.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(net5,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "    print('Gaussian 10 epoch')\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb120022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "loss: tensor(0.0457, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0457, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "loss: tensor(0.0054, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0054, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "loss: tensor(0.0027, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0027, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "loss: tensor(0.0167, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0167, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "loss: tensor(0.0015, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0015, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "loss: tensor(0.0077, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0077, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "loss: tensor(0.0031, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0031, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "loss: tensor(0.0049, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0049, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "loss: tensor(0.0102, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0102, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "loss: tensor(0.0008, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0008, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "loss: tensor(0.0015, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0015, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "loss: tensor(0.0253, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0253, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "loss: tensor(0.0061, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0061, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "loss: tensor(0.0024, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0024, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "loss: tensor(0.0072, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0072, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "loss: tensor(0.0014, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0014, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "loss: tensor(0.0072, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0072, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "loss: tensor(0.0011, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0011, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "loss: tensor(0.0016, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0016, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "loss: tensor(0.0013, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0013, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "\n",
      " This is the TSS 1156.893798828125 This is the SS_res: 31.97066307067871 \n",
      " This is the R^2 on testset: tensor(0.9724, device='cuda:1')\n",
      "Gaussian 30 epoch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train = FVMF.train(net5, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                shape = data_shape,CLASSES = 1)    \n",
    "    trtimes.append(train[1].detach().cpu().numpy())\n",
    "    #print('max:',net4.weight_mu[1].max())\n",
    "    #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "loss, outputs, output = test_ensemble.test_ensemble(net5,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                  CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "print('Gaussian 30 epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f29ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "loss: tensor(0.0020, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0020, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "loss: tensor(0.0078, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0078, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "loss: tensor(0.0021, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0021, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "loss: tensor(0.0013, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0013, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "loss: tensor(0.0025, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0025, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "loss: tensor(0.0028, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0028, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "loss: tensor(0.0086, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0086, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "loss: tensor(0.0077, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0077, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "loss: tensor(0.0013, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0013, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "loss: tensor(0.0006, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0006, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "loss: tensor(0.0011, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0011, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "loss: tensor(0.0085, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0085, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "loss: tensor(0.0010, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0010, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "loss: tensor(0.0094, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0094, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "loss: tensor(0.0100, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0100, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "loss: tensor(0.0006, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0006, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "loss: tensor(0.0061, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0061, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "loss: tensor(0.0012, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0012, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "loss: tensor(0.0010, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0010, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "loss: tensor(0.0090, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0090, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "\n",
      " This is the TSS 1156.893798828125 This is the SS_res: 10.828275680541992 \n",
      " This is the R^2 on testset: tensor(0.9906, device='cuda:1')\n",
      "Gaussian 50 epoch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train = FVMF.train(net5, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                shape = data_shape,CLASSES = 1)    \n",
    "    trtimes.append(train[1].detach().cpu().numpy())\n",
    "    #print('max:',net4.weight_mu[1].max())\n",
    "    #print('norm:',torch.norm(net4.weight_mu[1]))\n",
    "\n",
    "loss, outputs, output = test_ensemble.test_ensemble(net5,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                  CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "print('Gaussian 50 epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "badcdd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian loss curve, 50 epoch regression\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = []\n",
    "for i in range(50):\n",
    "    x.append(i+1)\n",
    "print('Gaussian loss curve, 50 epoch regression')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(x,trtimes)\n",
    "plt.savefig('Plots/Gaussian_loss_50_epoch_regression.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d673827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Random Init Utilized\n",
      "1\n",
      "loss: tensor(0.0963, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0963, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.2822, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.7391, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "2\n",
      "loss: tensor(0.0868, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0868, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.2961, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.2026, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "3\n",
      "loss: tensor(0.0382, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0382, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.2975, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(7.8344, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "4\n",
      "loss: tensor(0.0113, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0113, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.3658, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(7.6327, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "5\n",
      "loss: tensor(0.0329, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0329, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.3011, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.3353, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "6\n",
      "loss: tensor(0.0719, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0719, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.5546, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.3609, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "7\n",
      "loss: tensor(0.0780, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0780, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.5769, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.6597, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "8\n",
      "loss: tensor(0.0095, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0095, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.6388, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.0397, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "9\n",
      "loss: tensor(0.0347, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0347, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.7466, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.2163, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "10\n",
      "loss: tensor(0.0078, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0078, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.8248, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.8262, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "11\n",
      "loss: tensor(0.0119, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0119, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.8374, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.9343, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "12\n",
      "loss: tensor(0.0020, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0020, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.6007, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(10.4034, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "13\n",
      "loss: tensor(0.0075, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0075, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.2505, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(10.7606, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "14\n",
      "loss: tensor(0.0061, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0061, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.7293, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(11.0318, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "15\n",
      "loss: tensor(0.0410, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0410, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(6.0239, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(11.4171, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "16\n",
      "loss: tensor(0.0123, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0123, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(6.1386, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(11.7896, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "17\n",
      "loss: tensor(0.0218, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0218, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(6.4464, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(11.9305, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "18\n",
      "loss: tensor(0.0045, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0045, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(6.5918, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.1822, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "19\n",
      "loss: tensor(0.0063, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0063, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(6.6021, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.2103, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "20\n",
      "loss: tensor(0.0055, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0055, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.1425, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.5527, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "21\n",
      "loss: tensor(0.0092, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0092, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.1385, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.7493, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "22\n",
      "loss: tensor(0.0198, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0198, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.5957, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.9852, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "23\n",
      "loss: tensor(0.0042, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0042, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.7046, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.0820, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "24\n",
      "loss: tensor(0.0155, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0155, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.6447, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.0889, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "25\n",
      "loss: tensor(0.0020, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0020, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.2414, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.0441, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "26\n",
      "loss: tensor(0.0044, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0044, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.5625, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.2576, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "27\n",
      "loss: tensor(0.0176, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0176, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.6209, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.3928, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "28\n",
      "loss: tensor(0.0052, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0052, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.4576, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.3492, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "29\n",
      "loss: tensor(0.0147, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0147, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.3724, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.3337, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "30\n",
      "loss: tensor(0.0114, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0114, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.3453, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.3836, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "\n",
      " This is the TSS 1156.893798828125 This is the SS_res: 47.23618698120117 \n",
      " This is the R^2 on testset: tensor(0.9592, device='cuda:1')\n",
      "vMF 30 epoch\n"
     ]
    }
   ],
   "source": [
    "trtimes  = []\n",
    "epochs = 30\n",
    "\n",
    "#Note, for this regression task, the last 7 to 1 layer has a Gaussian VD, where we kill the prior and simply optimize with MLE.\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    vMFRegression30= FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='vmf',\n",
    "                                b_kappa=torch.Tensor(1).uniform_(3,3.1),\n",
    "                                w_kappa=torch.Tensor(1).uniform_(7.5,7.6),\n",
    "                                Temper = 0,classification='Regression',NODEFORCE =False)\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(vMFRegression30.parameters(), lr=0.08)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train = FVMF.train(vMFRegression30, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)    \n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        print('max:',vMFRegression30.weight_mu[1].max())\n",
    "        print('norm:',torch.norm(vMFRegression30.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(vMFRegression30,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "    print('vMF 30 epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16496c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "loss: tensor(0.0120, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0120, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.3972, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.5032, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "2\n",
      "loss: tensor(0.0140, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0140, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.5367, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.6858, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "3\n",
      "loss: tensor(0.0241, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0241, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.3280, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.6593, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "4\n",
      "loss: tensor(0.0083, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0083, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.6334, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.8689, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "5\n",
      "loss: tensor(0.0132, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0132, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.0162, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.9853, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "6\n",
      "loss: tensor(0.0395, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0395, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.7819, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.0176, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "7\n",
      "loss: tensor(0.0069, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0069, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.8346, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.1058, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "8\n",
      "loss: tensor(0.0074, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0074, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.5265, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.4774, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "9\n",
      "loss: tensor(0.0035, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0035, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.1624, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.5794, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "10\n",
      "loss: tensor(0.0039, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0039, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.2073, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.6308, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "11\n",
      "loss: tensor(0.0039, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0039, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.6846, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.9834, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "12\n",
      "loss: tensor(0.0045, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0045, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.6140, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.1011, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "13\n",
      "loss: tensor(0.0260, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0260, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.5790, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.0767, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "14\n",
      "loss: tensor(0.0086, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0086, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.5988, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.1942, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "15\n",
      "loss: tensor(0.0095, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0095, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.6999, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.2980, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "16\n",
      "loss: tensor(0.0069, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0069, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.7218, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.3986, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "17\n",
      "loss: tensor(0.0144, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0144, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.8802, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.5449, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "18\n",
      "loss: tensor(0.0075, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0075, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.6446, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.5456, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "19\n",
      "loss: tensor(0.0038, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0038, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.4714, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.5972, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "20\n",
      "loss: tensor(0.0111, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0111, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.4605, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.5995, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "\n",
      " This is the TSS 1156.893798828125 This is the SS_res: 30.646963119506836 \n",
      " This is the R^2 on testset: tensor(0.9735, device='cuda:1')\n",
      "vMF 50 epoch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train = FVMF.train(vMFRegression30, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                shape = data_shape,CLASSES = 1)    \n",
    "    trtimes.append(train[1].detach().cpu().numpy())\n",
    "    print('max:',vMFRegression30.weight_mu[1].max())\n",
    "    print('norm:',torch.norm(vMFRegression30.weight_mu[1]))\n",
    "loss, outputs, output = test_ensemble.test_ensemble(vMFRegression30,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                  CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "print('vMF 50 epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08223e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vMF loss curve, 50 epoch regression\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = []\n",
    "for i in range(50):\n",
    "    x.append(i+1)\n",
    "print('vMF loss curve, 50 epoch regression')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(x,trtimes)\n",
    "plt.savefig('Plots/vMF_loss_50_epoch_regression.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34203507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "FVMF RELOADED\n",
      "0\n",
      "Random Init Utilized\n",
      "1\n",
      "loss: tensor(0.0891, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0891, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.5014, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.3171, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "2\n",
      "loss: tensor(0.0845, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0845, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.4328, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.0394, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "3\n",
      "loss: tensor(0.0580, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0580, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.8436, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(7.9262, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "4\n",
      "loss: tensor(0.0884, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0884, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.0151, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.0298, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "5\n",
      "loss: tensor(0.0477, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0477, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.4203, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.8337, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "6\n",
      "loss: tensor(0.0757, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0757, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.7873, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.1630, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "7\n",
      "loss: tensor(0.0167, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0167, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.9457, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.6868, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "8\n",
      "loss: tensor(0.0214, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0214, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.8940, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.9241, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "9\n",
      "loss: tensor(0.0323, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0323, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.1540, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(10.5583, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "10\n",
      "loss: tensor(0.0132, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0132, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.6773, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(11.2296, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "11\n",
      "loss: tensor(0.0138, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0138, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(4.8699, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(11.5413, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "12\n",
      "loss: tensor(0.0061, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0061, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.4577, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.0645, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "13\n",
      "loss: tensor(0.0461, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0461, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.5409, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.4599, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "14\n",
      "loss: tensor(0.0099, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0099, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.4355, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.5213, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "15\n",
      "loss: tensor(0.1607, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1607, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(5.7276, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(12.7676, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "16\n",
      "loss: tensor(0.0109, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0109, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(6.5467, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(13.2273, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "17\n",
      "loss: tensor(0.0072, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0072, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.3542, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.1306, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "18\n",
      "loss: tensor(0.0084, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0084, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.7796, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(14.8534, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "19\n",
      "loss: tensor(0.0323, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0323, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.4004, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.5681, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "20\n",
      "loss: tensor(0.0066, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0066, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(7.5911, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.6378, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "21\n",
      "loss: tensor(0.0179, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0179, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(8.1072, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(15.7282, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "22\n",
      "loss: tensor(0.0962, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0962, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(9.8324, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(17.0901, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "23\n",
      "loss: tensor(0.0129, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0129, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(9.7622, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(17.3871, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "24\n",
      "loss: tensor(0.0443, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0443, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(10.3229, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(17.6331, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "25\n",
      "loss: tensor(0.0065, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0065, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(11.8982, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(18.4516, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "26\n",
      "loss: tensor(0.0131, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0131, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(11.2180, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(18.4185, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "27\n",
      "loss: tensor(0.0303, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0303, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(12.4491, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(18.8978, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "28\n",
      "loss: tensor(0.0215, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0215, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(14.8194, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(19.7180, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "29\n",
      "loss: tensor(0.0215, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0215, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(15.8762, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(20.2767, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "30\n",
      "loss: tensor(0.0103, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0103, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(14.1985, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(19.5552, device='cuda:1', grad_fn=<NormBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This is the TSS 1156.893798828125 This is the SS_res: 106.65696716308594 \n",
      " This is the R^2 on testset: tensor(0.9078, device='cuda:1')\n",
      "vMF 25 epoch\n",
      "0\n",
      "1\n",
      "loss: tensor(2.5112, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0295, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "loss: tensor(2.5137, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0115, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "loss: tensor(2.5708, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0372, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "loss: tensor(2.6710, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0195, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "loss: tensor(2.5709, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0175, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "loss: tensor(2.5820, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0183, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "loss: tensor(2.6205, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0195, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "loss: tensor(2.5269, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0066, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "loss: tensor(2.5075, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0239, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "loss: tensor(2.6009, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0333, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "loss: tensor(2.6109, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0130, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "loss: tensor(2.6462, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0434, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "loss: tensor(2.4369, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0055, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "loss: tensor(2.5812, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0249, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "loss: tensor(2.6747, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0373, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "loss: tensor(2.5736, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0191, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "loss: tensor(2.7320, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0020, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "loss: tensor(2.5133, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0328, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "loss: tensor(2.5450, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0105, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "loss: tensor(2.6450, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0183, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "\n",
      " This is the TSS 1156.893798828125 This is the SS_res: 47.44758605957031 \n",
      " This is the R^2 on testset: tensor(0.9590, device='cuda:1')\n",
      "vMF 25+25 epoch NII\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "COND_OPT = False\n",
    "CLASSES = 1\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 30\n",
    "\n",
    "TEST_BATCH_SIZE = 6000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "data_shape = (0,5,5,6)\n",
    "l1shape=(5, 5)\n",
    "l2shape=(5, 5)\n",
    "l3shape=(5, 5)\n",
    "l4shape=(5, 1)\n",
    "layershapes = [l1shape, l2shape, l3shape, l4shape]\n",
    "\n",
    "epochs = 30\n",
    "trtimes = []\n",
    "\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    vMFRegression= FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='vmf',\n",
    "                                b_kappa=torch.Tensor(1).uniform_(3,3.1),\n",
    "                                w_kappa=torch.Tensor(1).uniform_(7.5,7.6),\n",
    "                                Temper = 0,classification='Regression',NODEFORCE =False)\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(vMFRegression.parameters(), lr=0.11)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train = FVMF.train(vMFRegression, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)    \n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        print('max:',vMFRegression.weight_mu[1].max())\n",
    "        print('norm:',torch.norm(vMFRegression.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(vMFRegression,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "    print('vMF 25 epoch')\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "\n",
    "GC_intensity = 1\n",
    "\n",
    "#Kanskje er det en god idee å også arve kappaene, men halvere dem? Hva om vi også kjører på med litt normalizering av mu-ene her?\n",
    "#Hva med adaptiv kappa per lag? Det er jo bare en kappa per bias-lag. GC fungerer rett og slett ikke.\n",
    "\n",
    "w_mu_R = []\n",
    "for i in range(len(vMFRegression.weight_mu)):\n",
    "    #print('\\n','torch.norm(net4.weight_mu[i]):',torch.norm(net4.weight_mu[i]))\n",
    "    w_mu_R.append(vMFRegression.weight_mu[i]/torch.norm(vMFRegression.weight_mu[i]))\n",
    "    #print('\\n','norm w_mu[i]',torch.norm(w_mu[i]))\n",
    "    \n",
    "b_mu_R = []\n",
    "for i in range(len(vMFRegression.bias_mu)):\n",
    "    #print('\\n','torch.norm(net4.bais_mu[i]):',torch.norm(net4.bias_mu[i]))\n",
    "    b_mu_R.append(vMFRegression.bias_mu[i]/torch.norm(vMFRegression.bias_mu[i]))\n",
    "    #print('\\n','norm b_mu[i]',torch.norm(b_mu[i]))\n",
    "    \n",
    "b_rho_R= []\n",
    "for i in range(len(vMFRegression.bias_rho)):\n",
    "    b_rho_R.append(vMFRegression.bias_rho[i])\n",
    "\n",
    "w_rho_R= []\n",
    "for i in range(len(vMFRegression.weight_rho)):\n",
    "    w_rho_R.append(vMFRegression.weight_rho[i])\n",
    "#For some reason, it seems that the weights are not being properly normalized here. no actually they are, but the norm reported is the norm after 1 optimization step.\n",
    "\n",
    "epochs = 20\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    vMFRegression2= FVMF.BayesianNetwork(w_mu = w_mu_R, b_mu = b_mu_R, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='vmf',\n",
    "                                b_kappa= b_rho_R, #torch.Tensor(1).uniform_(1.0,3.1), \n",
    "                                w_kappa= w_rho_R, #torch.Tensor(1).uniform_(2.0,4.1), \n",
    "                                Temper = 1,classification = 'regression')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(vMFRegression2.parameters(), lr=0.01) #0.01 9727 acc\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train = FVMF.train(vMFRegression2, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)    \n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        #print('max:',vMFRegression2.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(vMFRegression2.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(vMFRegression2,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "    print('vMF 30+20 epoch NII')\n",
    "\n",
    "#Kanskje er det en god idee å også arve kappaene, men halvere dem? Hva om vi også kjører på med litt normalizering av mu-ene her?\n",
    "#Hva med adaptiv kappa per lag? Det er jo bare en kappa per bias-lag. GC fungerer rett og slett ikke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "446e9e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "FVMF RELOADED\n",
      "0\n",
      "Random Init Utilized\n",
      "1\n",
      "loss: tensor(0.0963, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0963, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.2822, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.7391, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "2\n",
      "loss: tensor(0.0868, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0868, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.2961, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.2026, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "3\n",
      "loss: tensor(0.0382, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0382, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.2975, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(7.8344, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "4\n",
      "loss: tensor(0.0113, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0113, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.3658, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(7.6327, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "5\n",
      "loss: tensor(0.0329, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0329, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.3011, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.3353, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "6\n",
      "loss: tensor(0.0719, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0719, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.5546, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.3609, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "7\n",
      "loss: tensor(0.0780, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0780, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.5769, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(8.6597, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "8\n",
      "loss: tensor(0.0095, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0095, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.6388, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.0397, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "9\n",
      "loss: tensor(0.0347, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0347, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(2.7466, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.2163, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "10\n",
      "loss: tensor(0.0078, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0078, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "max: tensor(3.8248, device='cuda:1', grad_fn=<MaxBackward1>)\n",
      "norm: tensor(9.8262, device='cuda:1', grad_fn=<NormBackward1>)\n",
      "\n",
      " This is the TSS 1156.893798828125 This is the SS_res: 39.911827087402344 \n",
      " This is the R^2 on testset: tensor(0.9655, device='cuda:1')\n",
      "vMF 10 epoch\n",
      "0\n",
      "1\n",
      "loss: tensor(2.6189, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1067, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "loss: tensor(2.5714, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0568, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "loss: tensor(2.5647, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0362, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "loss: tensor(2.7376, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0631, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "loss: tensor(2.5885, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0153, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "loss: tensor(2.7486, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1697, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "loss: tensor(2.7362, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0913, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "loss: tensor(2.5846, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0341, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "loss: tensor(2.5506, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0357, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "loss: tensor(2.6677, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0903, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "loss: tensor(2.6390, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0211, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "loss: tensor(2.6970, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0900, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "loss: tensor(2.4989, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0753, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "loss: tensor(2.5583, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0243, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "loss: tensor(2.6526, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0471, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "loss: tensor(2.5780, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0313, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "loss: tensor(2.7092, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0155, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "loss: tensor(2.5568, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.1085, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "loss: tensor(2.5661, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0552, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "loss: tensor(2.7189, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "negative_log_likelihood: tensor(0.0797, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "\n",
      " This is the TSS 1156.893798828125 This is the SS_res: 236.0122833251953 \n",
      " This is the R^2 on testset: tensor(0.7960, device='cuda:1')\n",
      "vMF 10+20 epoch NII\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 189\u001b[0m\n\u001b[1;32m    185\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(vMFRegression2\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.08\u001b[39m) \u001b[38;5;66;03m#0.01 9727 acc\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 189\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[43mFVMF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvMFRegression3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdTRAIN_CARBON\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAMPLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43mCLASSES\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m    191\u001b[0m     trtimes\u001b[38;5;241m.\u001b[39mappend(train[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m#print('max:',vMFRegression2.weight_mu[1].max())\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m#print('norm:',torch.norm(vMFRegression2.weight_mu[1]))\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/BNN/FVMF.py:911\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, dtrain, SAMPLES, optimizer, epoch, i, shape, BATCH_SIZE, CLASSES)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m#print(target)\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m#print('\\n','target:',target,'\\n')\u001b[39;00m\n\u001b[1;32m    910\u001b[0m net\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 911\u001b[0m loss, log_prior, log_variational_posterior, negative_log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_elbo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNUM_BATCHES\u001b[49m\u001b[43m,\u001b[49m\u001b[43mSAMPLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43mCLASSES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m#start = time.time()\u001b[39;00m\n\u001b[1;32m    913\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/projects/BNN/FVMF.py:842\u001b[0m, in \u001b[0;36mBayesianNetwork.sample_elbo\u001b[0;34m(self, input, target, NUM_BATCHES, samples, CLASSES)\u001b[0m\n\u001b[1;32m    840\u001b[0m log_variational_posteriors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(samples)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(samples):\n\u001b[0;32m--> 842\u001b[0m     outputs[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m     log_priors[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_prior()\n\u001b[1;32m    844\u001b[0m     log_variational_posteriors[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_variational_posterior()\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/BNN/FVMF.py:818\u001b[0m, in \u001b[0;36mBayesianNetwork.forward\u001b[0;34m(self, x, sample)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m<\u001b[39mend\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 818\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    820\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x,sample)\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/BNN/FVMF.py:610\u001b[0m, in \u001b[0;36mvMF_NodeWise.forward\u001b[0;34m(self, input, sample, calculate_log_probs)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features):\n\u001b[1;32m    609\u001b[0m         weight[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight[i]\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m--> 610\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features):\n",
      "File \u001b[0;32m~/projects/BNN/FVMF.py:281\u001b[0m, in \u001b[0;36mvMF.sample\u001b[0;34m(self, N, rsf)\u001b[0m\n\u001b[1;32m    279\u001b[0m w0 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m bb) \u001b[38;5;241m*\u001b[39m eps) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m bb) \u001b[38;5;241m*\u001b[39m eps)\n\u001b[1;32m    280\u001b[0m t0 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m aa \u001b[38;5;241m*\u001b[39m bb) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m bb) \u001b[38;5;241m*\u001b[39m eps)\n\u001b[0;32m--> 281\u001b[0m det \u001b[38;5;241m=\u001b[39m (d \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[43mt0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m t0 \u001b[38;5;241m+\u001b[39m dd \u001b[38;5;241m-\u001b[39m uns\u001b[38;5;241m.\u001b[39mlog()\n\u001b[1;32m    282\u001b[0m v0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v0, w0\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()[det \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)])\u001b[38;5;66;03m#torch.tensor(w0[det >= 0]).to(DEVICE)])\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m#print('w0:',w0[det >= 0])\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w_mu_R = []\n",
    "for i in range(len(vMFRegression2.weight_mu)):\n",
    "    #print('\\n','torch.norm(net4.weight_mu[i]):',torch.norm(net4.weight_mu[i]))\n",
    "    w_mu_R.append(vMFRegression2.weight_mu[i]/torch.norm(vMFRegression2.weight_mu[i]))\n",
    "    #print('\\n','norm w_mu[i]',torch.norm(w_mu[i]))\n",
    "    \n",
    "b_mu_R = []\n",
    "for i in range(len(vMFRegression2.bias_mu)):\n",
    "    #print('\\n','torch.norm(net4.bais_mu[i]):',torch.norm(net4.bias_mu[i]))\n",
    "    b_mu_R.append(vMFRegression2.bias_mu[i]/torch.norm(vMFRegression2.bias_mu[i]))\n",
    "    #print('\\n','norm b_mu[i]',torch.norm(b_mu[i]))\n",
    "    \n",
    "b_rho_R= []\n",
    "for i in range(len(vMFRegression2.bias_rho)):\n",
    "    b_rho_R.append(vMFRegression2.bias_rho[i])\n",
    "\n",
    "w_rho_R= []\n",
    "for i in range(len(vMFRegression2.weight_rho)):\n",
    "    w_rho_R.append(vMFRegression2.weight_rho[i])\n",
    "#For some reason, it seems that the weights are not being properly normalized here. no actually they are, but the norm reported is the norm after 1 optimization step.\n",
    "\n",
    "epochs = 20\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    vMFRegression3= FVMF.BayesianNetwork(w_mu = w_mu_R, b_mu = b_mu_R, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dTRAIN_CARBON, dtest=dTEST_CARBON,\n",
    "                                VD='vmf',\n",
    "                                b_kappa= b_rho_R, #torch.Tensor(1).uniform_(1.0,3.1), \n",
    "                                w_kappa= w_rho_R, #torch.Tensor(1).uniform_(2.0,4.1), \n",
    "                                Temper = 1,classification = 'regression')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(vMFRegression2.parameters(), lr=0.08) #0.01 9727 acc\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train = FVMF.train(vMFRegression3, dTRAIN_CARBON, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,\n",
    "                                    shape = data_shape,CLASSES = 1)    \n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        #print('max:',vMFRegression2.weight_mu[1].max())\n",
    "        #print('norm:',torch.norm(vMFRegression2.weight_mu[1]))\n",
    "\n",
    "    loss, outputs, output = test_ensemble.test_ensemble(vMFRegression3,dTEST_CARBON,TEST_SAMPLES,TEST_BATCH_SIZE,TEST_BATCH_SIZE,\n",
    "                                      CLASSES,DEVICE,shape = data_shape,classification = False,plot = True)\n",
    "    print('vMF 10+20+20 epoch NII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105168a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in range(50):\n",
    "    x.append(i+1)\n",
    "print('NIIvMF loss curve, 50 epoch regression')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(x,trtimes)\n",
    "plt.savefig('Plots/NIIvMF_loss_50_epoch_regression.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dabf8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
