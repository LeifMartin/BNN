{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ddbed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "Classes loaded\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "\n",
    "import VMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9837faf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#epochs = 225\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#trtimes  = np.zeros(epochs)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# make inference on 10 networks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mlayers = [(256, 400),(400, 600),(400, 600),(600, 5)]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mself_layerdict = {}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03mself_l1\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mset_printoptions(edgeitems\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor((\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m     32\u001b[0m x\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "#epochs = 225\n",
    "#trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "#for i in range(0, 1):\n",
    "#    print(i)\n",
    "#    torch.manual_seed(i)\n",
    "#    net = VMF.BayesianNetwork().to(VMF.DEVICE)\n",
    "#    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "#    for epoch in range(epochs):\n",
    "#\n",
    "#        trtimes[epoch] = VMF.train(net, optimizer, epoch, i)\n",
    "#        print(net.l1.weight_mu.mean())\n",
    "#\n",
    "#    res = VMF.test_ensemble()\n",
    "#\n",
    "#    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "r\"\"\"\n",
    "layers = [(256, 400),(400, 600),(400, 600),(600, 5)]\n",
    "self_layerdict = {}\n",
    "i = 0\n",
    "while i<len(layers):\n",
    "            name = 'self_l'+str(i)\n",
    "            self_layerdict[name] = layers[i]\n",
    "            i = i+1\n",
    "            print(name)\n",
    "for k,v in self_layerdict.items():\n",
    "    exec(\"%s = %s\" % (k, v))\n",
    "self_l1\n",
    "\"\"\"\n",
    "torch.set_printoptions(edgeitems=1)\n",
    "x=torch.tensor((4,4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a223bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8ca520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(251.5287, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.0679, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0040, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(251.3824, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.7604, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0029, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(249.7483, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.0165, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0031, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(245.5632, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(155.7485, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0097, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(233.2977, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(144.6123, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0204, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(219.1594, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(131.0194, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0242, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(210.3794, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(123.1321, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0213, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(204.0602, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.9010, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0173, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(200.6461, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.7153, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0152, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(196.7289, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(110.7560, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0111, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(193.5298, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(107.5611, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0074, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(190.0633, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(105.0851, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0049, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(185.4069, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(101.2987, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0026, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(184.6697, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(100.6750, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0011, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(178.9831, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(96.7930, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0008, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(178.0702, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(95.4042, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0027, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(176.0226, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(93.4787, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0036, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(174.2403, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(91.7499, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0049, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(172.5303, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(90.5331, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0064, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(170.5629, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(89.5907, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0078, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(168.7057, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(88.7159, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0085, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(167.3969, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(87.0187, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0092, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(165.6795, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(86.3942, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0102, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(164.3373, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(84.8164, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0110, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(161.3309, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(83.9651, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0118, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "26\n",
      "tensor(160.4807, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(83.3044, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0118, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "27\n",
      "tensor(159.5173, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(82.2616, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0127, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "28\n",
      "tensor(158.9262, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(82.2104, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0131, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "29\n",
      "tensor(156.5130, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(81.2434, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0141, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "30\n",
      "tensor(155.4329, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(79.8154, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0148, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "31\n",
      "tensor(153.2176, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(77.7497, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0145, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "32\n",
      "tensor(152.3419, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(77.7240, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0146, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "33\n",
      "tensor(152.2692, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(79.2551, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0147, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "34\n",
      "tensor(148.0408, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(75.0622, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0151, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "35\n",
      "tensor(146.9435, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(74.5964, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0157, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "36\n",
      "tensor(144.1741, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(71.3327, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0165, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "37\n",
      "tensor(141.6204, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(69.6155, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0175, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "38\n",
      "tensor(140.3093, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(68.6643, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0181, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "39\n",
      "tensor(137.5406, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(66.2880, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0178, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "40\n",
      "tensor(132.9795, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(62.2007, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0195, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Component 0 Accuracy: 859/1000\n",
      "Component 1 Accuracy: 854/1000\n",
      "Component 2 Accuracy: 858/1000\n",
      "Component 3 Accuracy: 858/1000\n",
      "Component 4 Accuracy: 856/1000\n",
      "Component 5 Accuracy: 859/1000\n",
      "Component 6 Accuracy: 855/1000\n",
      "Component 7 Accuracy: 858/1000\n",
      "Component 8 Accuracy: 857/1000\n",
      "Component 9 Accuracy: 860/1000\n",
      "Posterior Mean Accuracy: 860/1000\n",
      "Ensemble Accuracy: 860/1000\n"
     ]
    }
   ],
   "source": [
    "#matplotlib inline\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import VMF\n",
    "\n",
    "import importlib\n",
    "importlib.reload(VMF)\n",
    "\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPUs are used!\")\n",
    "else:\n",
    "    print(\"CPUs are used!\")\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "batch_size = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "epochs = 250\n",
    "pepochs = 50\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "\n",
    "TRAIN_SIZE = len(tr_ids)\n",
    "TEST_SIZE = len(te_ids)\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "NUM_TEST_BATCHES = len(te_ids)/BATCH_SIZE\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "\n",
    "#The net does not like to get larger at a given layer??\n",
    "l1shape=(256, 3)\n",
    "l2shape=(3, 3)\n",
    "l3shape=(3, 3)\n",
    "l4shape=(3, 5)\n",
    "\n",
    "\n",
    "epochs = 40\n",
    "trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net = VMF.BayesianNetwork(l1=l1shape, l2=l2shape, l3=l3shape,l4=l4shape,BN='notbatchnorm').to(DEVICE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0007)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = VMF.train(net, optimizer, epoch, i)\n",
    "        print(net.l1.weight_mu.mean())\n",
    "\n",
    "    res = VMF.test_ensemble(net)\n",
    "\n",
    "    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f26701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nw_mu5 = net.l5.weight_mu\\nw_mu5 = w_mu5.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\\n#net.l3.weight_rho\\nb_mu5 = net.l5.bias_mu.to(DEVICE) #5\\n#net.l3.bias_rho\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_mu1 = net.l1.weight_mu\n",
    "w_mu1 = w_mu1.reshape(l1shape[0]*l1shape[1]).to(DEVICE)\n",
    "#net.l1.weight_rho\n",
    "b_mu1 = net.l1.bias_mu.to(DEVICE) #400\n",
    "#net.l1.bias_rho\n",
    "\n",
    "w_mu2 = net.l2.weight_mu\n",
    "w_mu2 = w_mu2.reshape(l2shape[0]*l2shape[1]).to(DEVICE)\n",
    "#net.l2.weight_rho\n",
    "b_mu2 = net.l2.bias_mu.to(DEVICE) #600\n",
    "#net.l2.bias_rho\n",
    "\n",
    "w_mu3 = net.l3.weight_mu\n",
    "w_mu3 = w_mu3.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu3 = net.l3.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho\n",
    "\n",
    "w_mu4 = net.l4.weight_mu\n",
    "w_mu4 = w_mu4.reshape(l4shape[0]*l4shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu4 = net.l4.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho\n",
    "r\"\"\"\n",
    "w_mu5 = net.l5.weight_mu\n",
    "w_mu5 = w_mu5.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu5 = net.l5.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0470924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uio/hume/student-u44/lmsunde/projects/BNN/AliaksandrFolder/FVMF.py:282: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  v0 = torch.cat([v0, torch.tensor(w0[det >= 0]).to(DEVICE)])\n",
      "/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(91.4396, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(127.0758, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3661, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(82.8092, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(119.3555, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2683, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(81.8464, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.9631, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2944, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(79.4755, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.1733, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3035, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(78.8205, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.4841, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3244, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(79.5187, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.6972, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3558, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(81.3520, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.3278, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3574, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(78.6650, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.0534, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3244, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(81.0394, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.2754, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3935, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(78.3541, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.3269, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4459, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(79.7950, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.2827, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3777, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(78.9674, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.4545, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4088, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(79.8829, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.2516, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4064, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(83.1689, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(119.1315, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4680, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(80.6053, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.0960, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4428, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(79.7884, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.4780, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4646, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(82.0421, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(118.4510, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4731, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(79.3778, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.5226, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4750, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(78.8171, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.8387, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4885, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(79.4152, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.9739, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4649, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(80.5149, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.2335, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4845, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(79.2089, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.5831, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5012, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(78.5116, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.1925, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4925, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(79.9500, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.5834, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5188, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(80.0860, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.8216, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5115, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "26\n",
      "tensor(80.2621, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.0682, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5523, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "27\n",
      "tensor(78.6613, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.7517, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5366, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "28\n",
      "tensor(79.4158, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.5417, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5323, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "29\n",
      "tensor(79.1897, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.1944, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5424, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "30\n",
      "tensor(80.8955, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.1408, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5477, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "31\n",
      "tensor(79.5802, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.9236, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5293, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "32\n",
      "tensor(81.2677, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.7963, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5987, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "33\n",
      "tensor(79.6193, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.8036, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5325, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "34\n",
      "tensor(78.6845, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.3157, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6093, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "35\n",
      "tensor(81.1087, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.6490, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5850, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "36\n",
      "tensor(78.5636, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.5551, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5849, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "37\n",
      "tensor(79.8656, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.6646, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6125, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "38\n",
      "tensor(79.5609, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.5941, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6053, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "39\n",
      "tensor(78.4645, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.0500, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6170, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "40\n",
      "tensor(80.3014, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.6061, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6329, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "41\n",
      "tensor(78.3161, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.4804, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5915, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "42\n",
      "tensor(78.9937, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.7574, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6344, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "43\n",
      "tensor(79.1200, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.5060, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6719, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "44\n",
      "tensor(79.6534, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.0515, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6376, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "45\n",
      "tensor(79.2479, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.7819, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6493, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "46\n",
      "tensor(78.8730, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.8725, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6558, device='cuda:1', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "tensor(79.7997, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.2413, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6643, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "48\n",
      "tensor(80.2384, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.7573, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6993, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "49\n",
      "tensor(79.6894, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.7543, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7141, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "50\n",
      "tensor(78.7474, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.1056, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6891, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Component 0 Accuracy: 623/1000\n",
      "Component 1 Accuracy: 621/1000\n",
      "Component 2 Accuracy: 595/1000\n",
      "Component 3 Accuracy: 608/1000\n",
      "Component 4 Accuracy: 609/1000\n",
      "Component 5 Accuracy: 622/1000\n",
      "Component 6 Accuracy: 597/1000\n",
      "Component 7 Accuracy: 616/1000\n",
      "Component 8 Accuracy: 590/1000\n",
      "Component 9 Accuracy: 619/1000\n",
      "Posterior Mean Accuracy: 550/1000\n",
      "Ensemble Accuracy: 615/1000\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import os\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "import VMF\n",
    "\n",
    "import importlib\n",
    "importlib.reload(VMF)\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPUs are used!\")\n",
    "else:\n",
    "    print(\"CPUs are used!\")\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "batch_size = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 50\n",
    "epochs = 250\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "\n",
    "TRAIN_SIZE = len(tr_ids)\n",
    "TEST_SIZE = len(te_ids)\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "NUM_TEST_BATCHES = len(te_ids)/BATCH_SIZE\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "\n",
    "epochs = 15\n",
    "trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net2 = FVMF.BayesianNetwork(w_mu1 = w_mu1, w_mu2 = w_mu2, w_mu3 = w_mu3, w_mu4 = w_mu4, b_mu1 = b_mu1, b_mu2 = b_mu2, \n",
    "                                b_mu3 = b_mu3, b_mu4 = b_mu4, \n",
    "                                l1=l1shape, l2=l2shape, l3=l3shape, l4=l4shape, VD='vmf')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net2.parameters(), lr=0.07)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net2, optimizer, epoch, i)\n",
    "        print(net2.l1.weight_mu.mean())\n",
    "\n",
    "    res = FVMF.test_ensemble(net2)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5ab7d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(115.0081, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.0081, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6881, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(114.8410, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.8410, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6850, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(116.0712, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.0712, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6755, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(115.9967, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.9967, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6726, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(115.1423, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.1423, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6626, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(115.3034, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.3034, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6587, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(114.1336, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.1336, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6616, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(114.7626, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.7626, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6703, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(113.7474, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.7474, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6680, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(114.5765, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.5765, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6713, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(115.1688, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.1688, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6601, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(114.4106, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.4106, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6686, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(115.2381, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.2381, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6711, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(114.3669, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.3669, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6766, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(115.4019, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.4019, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6750, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(116.4970, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.4970, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6716, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(115.2226, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.2226, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6678, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(114.2054, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.2054, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6654, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(114.7489, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.7489, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6711, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(114.0314, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.0314, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6686, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(115.6970, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.6970, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6694, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(115.3538, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.3538, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6620, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(115.1695, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.1695, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6733, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(114.9144, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.9144, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6796, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(115.3162, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.3162, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "26\n",
      "tensor(114.1386, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.1386, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6907, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "27\n",
      "tensor(117.2073, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.2073, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6810, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "28\n",
      "tensor(114.6283, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.6283, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6761, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "29\n",
      "tensor(116.0506, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.0506, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6710, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "30\n",
      "tensor(116.8303, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.8303, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6674, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "31\n",
      "tensor(116.9803, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.9803, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6619, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "32\n",
      "tensor(114.5934, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.5934, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6644, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "33\n",
      "tensor(114.6620, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.6620, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6739, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "34\n",
      "tensor(114.9819, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.9819, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "35\n",
      "tensor(115.9524, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.9524, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "36\n",
      "tensor(115.2923, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.2923, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6818, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "37\n",
      "tensor(113.7722, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.7722, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6778, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "38\n",
      "tensor(115.0854, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.0854, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6769, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "39\n",
      "tensor(113.8581, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.8581, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6785, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "40\n",
      "tensor(114.3992, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.3992, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6676, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "41\n",
      "tensor(115.6165, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.6165, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6762, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "42\n",
      "tensor(114.8804, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.8804, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6859, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "43\n",
      "tensor(115.8124, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.8124, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6860, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "44\n",
      "tensor(115.7218, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.7218, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6737, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "45\n",
      "tensor(115.5815, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.5815, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6599, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "46\n",
      "tensor(115.0670, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.0670, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6594, device='cuda:1', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "tensor(114.6262, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.6262, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6558, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "48\n",
      "tensor(115.1557, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.1557, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6647, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "49\n",
      "tensor(114.8146, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.8146, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6626, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "50\n",
      "tensor(117.0598, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.0598, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6659, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Component 0 Accuracy: 620/1000\n",
      "Component 1 Accuracy: 624/1000\n",
      "Component 2 Accuracy: 613/1000\n",
      "Component 3 Accuracy: 613/1000\n",
      "Component 4 Accuracy: 623/1000\n",
      "Component 5 Accuracy: 620/1000\n",
      "Component 6 Accuracy: 619/1000\n",
      "Component 7 Accuracy: 630/1000\n",
      "Component 8 Accuracy: 602/1000\n",
      "Component 9 Accuracy: 624/1000\n",
      "Posterior Mean Accuracy: 547/1000\n",
      "Ensemble Accuracy: 624/1000\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net2.parameters(), lr=0.007)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    trtimes[epoch] = FVMF.train(net2, optimizer, epoch, i)\n",
    "    print(net2.l1.weight_mu.mean())\n",
    "res = FVMF.test_ensemble(net2)\n",
    "#np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2e9bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(115.3149, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.3149, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6657, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(114.2734, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.2734, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6645, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(114.5413, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.5413, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(114.2229, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.2229, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6666, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(115.5146, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.5146, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6683, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(114.1779, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.1779, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6696, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(115.3323, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.3323, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6700, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(115.9394, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.9394, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6711, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(116.4500, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.4500, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6710, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(115.8810, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.8810, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6717, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(114.8351, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.8351, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6713, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(115.6450, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.6450, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6714, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(115.1525, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.1525, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6704, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(115.3112, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.3112, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6709, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(113.4700, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.4700, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6714, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(113.1042, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.1042, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6719, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(116.0058, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.0058, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6725, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(115.8714, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.8714, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6712, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(114.4041, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.4041, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6708, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(114.5625, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.5625, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6717, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(114.2799, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.2799, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6719, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(115.8028, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.8028, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6710, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(114.9935, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.9935, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6701, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(116.0217, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.0217, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6699, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(115.3182, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.3182, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6697, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "26\n",
      "tensor(113.5726, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.5726, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6708, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "27\n",
      "tensor(114.8279, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.8279, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6716, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "28\n",
      "tensor(115.7418, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.7418, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6708, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "29\n",
      "tensor(113.9518, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.9518, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6711, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "30\n",
      "tensor(115.4950, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.4950, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6712, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "31\n",
      "tensor(115.8201, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.8201, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6719, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "32\n",
      "tensor(114.9437, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.9437, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6711, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "33\n",
      "tensor(115.1951, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.1951, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6714, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "34\n",
      "tensor(117.0928, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.0928, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6730, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "35\n",
      "tensor(116.4804, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.4804, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6727, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "36\n",
      "tensor(115.9157, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.9157, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6716, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "37\n",
      "tensor(114.9264, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.9264, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6707, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "38\n",
      "tensor(114.7465, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.7465, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6715, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "39\n",
      "tensor(116.6419, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.6419, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6711, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "40\n",
      "tensor(117.0319, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.0319, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6699, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "41\n",
      "tensor(115.8457, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.8457, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6707, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "42\n",
      "tensor(114.9709, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.9709, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6706, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "43\n",
      "tensor(115.4669, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(115.4669, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6708, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "44\n",
      "tensor(114.2342, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.2342, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6710, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "45\n",
      "tensor(117.4435, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.4435, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6702, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "46\n",
      "tensor(114.8396, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.8396, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6710, device='cuda:1', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "tensor(118.3731, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(118.3731, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6706, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "48\n",
      "tensor(112.4166, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(112.4166, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6713, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "49\n",
      "tensor(113.9534, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.9534, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6712, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "50\n",
      "tensor(114.9997, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.9997, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6719, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Component 0 Accuracy: 600/1000\n",
      "Component 1 Accuracy: 616/1000\n",
      "Component 2 Accuracy: 594/1000\n",
      "Component 3 Accuracy: 632/1000\n",
      "Component 4 Accuracy: 602/1000\n",
      "Component 5 Accuracy: 621/1000\n",
      "Component 6 Accuracy: 617/1000\n",
      "Component 7 Accuracy: 601/1000\n",
      "Component 8 Accuracy: 621/1000\n",
      "Component 9 Accuracy: 614/1000\n",
      "Posterior Mean Accuracy: 547/1000\n",
      "Ensemble Accuracy: 616/1000\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net2.parameters(), lr=0.0007)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    trtimes[epoch] = FVMF.train(net2, optimizer, epoch, i)\n",
    "    print(net2.l1.weight_mu.mean())\n",
    "res = FVMF.test_ensemble(net2)\n",
    "#np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "What if the problem is that since the mu's in the Gaussian is (out,in), and the vMF is out*in,\n",
    "this could mean that we have \"strethed\" a single vMF pdf over all the parameters, while in the Gaussian we have made one for each output?\n",
    "\n",
    "I don't know, also intuitively this should be the case, since the whole point of the vMF is the norm 1, which obviously\n",
    "will require that it for each forward-pass is only one massive pdf for all inputs and outputs.\n",
    "\n",
    "Ok, this can get rough, I will try my best to reshape the w_mu's and b_mu's the best I can, perhaps this will work just fine without too much tuning...\n",
    "Hopefully.\n",
    "\n",
    "This doe not seem to ave worked. I don't konw exactly what the problem is. Per haps it is a good idea to consult the new loss function\n",
    "suggested in the paper that made the code we based our vMF on?\n",
    "\n",
    "It is very strange that the loss is not at all affected by completely ridiculous learning rates...\n",
    "\n",
    "Remember that vMF makes the norm of the weights and biases 1, not the forward pass of the x's. Hence the advantage is that the gradient\n",
    "will not explode, since the backward pass of it will also be approx. 1. In batchnorm, maybe the gradient can explode? Since the weights \n",
    "can be whatever?\n",
    "\n",
    "\n",
    "IMPORTANT: The gaussian neuralnet will also collapse to 257 if I apply more than 3 layers. This must be somehow related to the similar\n",
    "behavior in the vMF when the size of each layer exceeds 3. Per haps there is an error in the loss afterall?\n",
    "However, in the Gaussian case, increasing the learning rate by a factor of 10 solved the issue. This makes me suspect it is the mathematical\n",
    "properties of the loss function, rather than incorrect implementation.\n",
    "\"\"\"\n",
    "\n",
    "#When l4 is (3,5):\n",
    "r\"\"\"\n",
    "File ~/projects/BNN/AliaksandrFolder/FVMF.py:289, in vMF.sample(self, N, rsf)\n",
    "    287 e1mu = torch.zeros(d, 1).to(DEVICE)\n",
    "    288 e1mu[0, 0] = 1.0\n",
    "--> 289 e1mu = e1mu - self.mu if len(self.mu.shape) == 2 else e1mu - self.mu.unsqueeze(1) #e1mu.shape = (1,self.x_dim). mu_unnorm.shape = (mu_unnorm)\n",
    "    290 e1mu = e1mu / norm(e1mu, dim=0).to(DEVICE)\n",
    "    291 samples = samples - 2 * (samples @ e1mu) @ e1mu.t()\n",
    "\n",
    "RuntimeError: The size of tensor a (15) must match the size of tensor b (9) at non-singleton dimension 0\n",
    "\"\"\"\n",
    "\n",
    "#When l4 is (5,5):\n",
    "r\"\"\"\n",
    "File ~/projects/BNN/AliaksandrFolder/FVMF.py:289, in vMF.sample(self, N, rsf)\n",
    "    287 e1mu = torch.zeros(d, 1).to(DEVICE)\n",
    "    288 e1mu[0, 0] = 1.0\n",
    "--> 289 e1mu = e1mu - self.mu if len(self.mu.shape) == 2 else e1mu - self.mu.unsqueeze(1) #e1mu.shape = (1,self.x_dim). mu_unnorm.shape = (mu_unnorm)\n",
    "    290 e1mu = e1mu / norm(e1mu, dim=0).to(DEVICE)\n",
    "    291 samples = samples - 2 * (samples @ e1mu) @ e1mu.t()\n",
    "\n",
    "RuntimeError: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 0\n",
    "\"\"\"\n",
    "#These errors above were caused by my initialization being wrong. I copy paster mu_3 for layer4, and forgot to change to mu_4. So now \n",
    "#I always get the error below.\n",
    "\n",
    "\n",
    "#in all cases now: \n",
    "\n",
    "r\"\"\"\n",
    "It seems that the whole thing does not progress at all. We just get the warning and then no further output.\n",
    "\n",
    "self.l4(x, sample)\n",
    "\n",
    "--> self.bias.sample()\n",
    "\n",
    "It always get's stuck there!!\n",
    "\n",
    "Specifically, it get's stuck in the while loop:\n",
    "\n",
    "while len(v0) < N:\n",
    "            eps = beta.sample([1, rsf * (N - len(v0))]).squeeze().to(DEVICE)\n",
    "            uns = uniform.sample([1, rsf * (N - len(v0))]).squeeze().to(DEVICE)\n",
    "            w0 = (1 - (1 + bb) * eps) / (1 - (1 - bb) * eps)\n",
    "            t0 = (2 * aa * bb) / (1 - (1 - bb) * eps)\n",
    "            det = (d - 1) * t0.log() - t0 + dd - uns.log()\n",
    "            v0 = torch.cat([v0, torch.tensor(w0[det >= 0]).to(DEVICE)])\n",
    "            if len(v0) > N:\n",
    "                v0 = v0[:N]\n",
    "                break\n",
    "\"\"\"\n",
    "\n",
    "r\"\"\"\n",
    "From further investigations it is clear that the error lies in w0[det >= 0] consistently being an empty Tensor.\n",
    "\n",
    "Even further, bb is 0 here which it usually is not. That must definitely indicate something is wrong.\n",
    "\n",
    "Adjusting the initialization of kappa to be 9 or less on both weights and biases makes the code run, \n",
    "but posterior collapse is back. Increasing kappa seems to increase the compute aswell... however, getting the kappa inits\n",
    "closer to 10 seems to also help avoid the posterior collapse. And the lower bound increased also helps, looks like 3 is optimal.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c14fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
