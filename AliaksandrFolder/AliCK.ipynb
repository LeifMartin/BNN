{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8ca520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(251.5287, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.0679, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0040, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(251.3824, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.7604, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0029, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(249.7483, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.0165, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0031, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(245.5632, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(155.7485, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0097, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(233.2977, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(144.6123, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0204, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(219.1594, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(131.0194, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0242, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(210.3794, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(123.1321, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0213, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(204.0602, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.9010, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0173, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(200.6461, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.7153, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0152, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(196.7289, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(110.7560, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0111, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(193.5298, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(107.5611, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0074, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(190.0633, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(105.0851, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0049, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(185.4069, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(101.2987, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0026, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(184.6697, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(100.6750, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0011, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(178.9831, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(96.7930, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0008, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(178.0702, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(95.4042, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0027, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(176.0226, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(93.4787, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0036, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(174.2403, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(91.7499, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0049, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(172.5303, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(90.5331, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0064, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(170.5629, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(89.5907, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0078, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(168.7057, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(88.7159, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0085, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(167.3969, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(87.0187, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0092, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(165.6795, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(86.3942, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0102, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(164.3373, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(84.8164, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0110, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(161.3309, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(83.9651, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0118, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "26\n",
      "tensor(160.4807, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(83.3044, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0118, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "27\n",
      "tensor(159.5173, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(82.2616, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0127, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "28\n",
      "tensor(158.9262, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(82.2104, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0131, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "29\n",
      "tensor(156.5130, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(81.2434, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0141, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "30\n",
      "tensor(155.4329, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(79.8154, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0148, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "31\n",
      "tensor(153.2176, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(77.7497, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0145, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "32\n",
      "tensor(152.3419, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(77.7240, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0146, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "33\n",
      "tensor(152.2692, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(79.2551, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0147, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "34\n",
      "tensor(148.0408, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(75.0622, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0151, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "35\n",
      "tensor(146.9435, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(74.5964, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0157, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "36\n",
      "tensor(144.1741, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(71.3327, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0165, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "37\n",
      "tensor(141.6204, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(69.6155, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0175, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "38\n",
      "tensor(140.3093, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(68.6643, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0181, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "39\n",
      "tensor(137.5406, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(66.2880, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0178, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "40\n",
      "tensor(132.9795, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(62.2007, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0195, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Component 0 Accuracy: 859/1000\n",
      "Component 1 Accuracy: 854/1000\n",
      "Component 2 Accuracy: 858/1000\n",
      "Component 3 Accuracy: 858/1000\n",
      "Component 4 Accuracy: 856/1000\n",
      "Component 5 Accuracy: 859/1000\n",
      "Component 6 Accuracy: 855/1000\n",
      "Component 7 Accuracy: 858/1000\n",
      "Component 8 Accuracy: 857/1000\n",
      "Component 9 Accuracy: 860/1000\n",
      "Posterior Mean Accuracy: 860/1000\n",
      "Ensemble Accuracy: 860/1000\n"
     ]
    }
   ],
   "source": [
    "#matplotlib inline\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import VMF\n",
    "\n",
    "import importlib\n",
    "importlib.reload(VMF)\n",
    "#import FVMF\n",
    "#importlib.reload(FVMF)\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPUs are used!\")\n",
    "else:\n",
    "    print(\"CPUs are used!\")\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "batch_size = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "epochs = 250\n",
    "pepochs = 50\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "\n",
    "TRAIN_SIZE = len(tr_ids)\n",
    "TEST_SIZE = len(te_ids)\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "NUM_TEST_BATCHES = len(te_ids)/BATCH_SIZE\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "\n",
    "#The net does not like to get larger at a given layer??\n",
    "l1shape=(256, 3)\n",
    "l2shape=(3, 3)\n",
    "l3shape=(3, 3)\n",
    "l4shape=(3, 5)\n",
    "layershapes = [l1shape, l2shape, l3shape, l4shape]\n",
    "\n",
    "epochs = 40\n",
    "trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net = VMF.BayesianNetwork(l1=l1shape, l2=l2shape, l3=l3shape,l4=l4shape,BN='notbatchnorm').to(DEVICE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0007)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = VMF.train(net, optimizer, epoch, i)\n",
    "        print(net.l1.weight_mu.mean())\n",
    "\n",
    "    res = VMF.test_ensemble(net)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f26701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0334, -0.1507, -0.0716,  0.5362,  0.5271,  0.4863,  0.4738,  0.5733,\n",
      "          0.4984,  0.6215,  0.5192,  0.4237, -0.0471, -0.1173, -0.2218, -0.2002,\n",
      "         -0.2028, -0.2415, -0.4574, -0.4300, -0.2626, -0.1437, -0.3166, -0.1310,\n",
      "         -0.2944, -0.2908, -0.1833, -0.5068, -0.4576, -0.3605, -0.3786, -0.1350,\n",
      "         -0.4004, -0.3736, -0.4271, -0.4072, -0.4072, -0.0838, -0.1337, -0.0853,\n",
      "         -0.2114, -0.3192, -0.2214, -0.3771, -0.3637, -0.2702, -0.1137, -0.0717,\n",
      "         -0.2505, -0.1138, -0.0288, -0.1031, -0.1202, -0.0507,  0.0816, -0.0663,\n",
      "          0.1725,  0.0267,  0.0514, -0.0020,  0.2894,  0.2170,  0.0966,  0.2657,\n",
      "          0.3123,  0.1729,  0.2358,  0.2644,  0.2401,  0.2459,  0.0561,  0.1727,\n",
      "          0.1526,  0.1638,  0.1717,  0.1671,  0.0864, -0.1244, -0.0300, -0.1469,\n",
      "         -0.2176,  0.0700,  0.1568, -0.1170, -0.0158, -0.0396, -0.0506, -0.0767,\n",
      "          0.0828, -0.1418,  0.0534,  0.0706,  0.0727,  0.0636, -0.1477, -0.1255,\n",
      "          0.0229, -0.0943, -0.2228, -0.2432,  0.0886,  0.0174, -0.2073, -0.1337,\n",
      "         -0.2101, -0.0352, -0.1233, -0.1057, -0.0301,  0.0439, -0.0399,  0.1903,\n",
      "         -0.0940,  0.1937,  0.2215, -0.0310,  0.0458,  0.0969,  0.0787,  0.1656,\n",
      "          0.2133,  0.2169,  0.1605,  0.2064, -0.0076,  0.0825,  0.1416,  0.0662,\n",
      "          0.1302, -0.0446,  0.0279,  0.1101,  0.0563,  0.1121,  0.1575,  0.0097,\n",
      "          0.0679,  0.0214,  0.0368,  0.0533,  0.0323,  0.0322,  0.1217, -0.0383,\n",
      "         -0.2088, -0.1467,  0.0130, -0.0910, -0.1923, -0.2026, -0.1959, -0.1804,\n",
      "         -0.1072,  0.0257, -0.0307, -0.1924,  0.0126, -0.1570, -0.0685, -0.1129,\n",
      "         -0.0999, -0.0710, -0.1550, -0.1997, -0.0746, -0.0489, -0.0751,  0.0436,\n",
      "         -0.2077,  0.0012,  0.0754, -0.0939,  0.0822,  0.1032, -0.1078,  0.1137,\n",
      "          0.0565,  0.0309,  0.1110, -0.1568,  0.0210,  0.0077,  0.0650,  0.0125,\n",
      "          0.0976,  0.0120, -0.1457, -0.1542, -0.1080,  0.0321,  0.0292, -0.0460,\n",
      "         -0.1853, -0.0719, -0.1420, -0.1106, -0.0881,  0.0182, -0.1676, -0.0908,\n",
      "         -0.1311,  0.0533, -0.1007, -0.0653, -0.1218,  0.1746, -0.0062,  0.1430,\n",
      "          0.0475, -0.0321, -0.0505, -0.0101,  0.0588,  0.0254,  0.0067, -0.0685,\n",
      "         -0.1043,  0.0121,  0.1129, -0.1270, -0.0771,  0.0588,  0.0203,  0.0402,\n",
      "         -0.0611,  0.0910, -0.1176, -0.1002,  0.0199,  0.0692,  0.1437,  0.1079,\n",
      "         -0.0120,  0.1926,  0.1063, -0.0938,  0.1323, -0.0548, -0.0868,  0.1241,\n",
      "          0.1022,  0.0717, -0.0988, -0.0827,  0.0086, -0.1787,  0.1008, -0.0490,\n",
      "          0.0052, -0.0376, -0.0223, -0.0701,  0.1778, -0.0579, -0.1033, -0.0164],\n",
      "        [-0.1397,  0.0582,  0.3850,  0.6207,  0.6988,  0.8561,  0.8175,  0.9383,\n",
      "          0.8712,  0.7408,  0.8025,  0.9545,  0.8823,  0.5828,  0.1344, -0.0888,\n",
      "         -0.1727, -0.0414, -0.2139, -0.2916, -0.0451, -0.3602, -0.2756, -0.2158,\n",
      "         -0.1926, -0.3244, -0.5002, -0.2067, -0.2357, -0.1886, -0.4166, -0.3421,\n",
      "         -0.2813, -0.3510, -0.4987, -0.3802, -0.3569, -0.4103, -0.4420, -0.4707,\n",
      "         -0.2764, -0.4781, -0.3195, -0.4274, -0.3807, -0.2994, -0.4616, -0.3147,\n",
      "         -0.1787, -0.1341,  0.0225, -0.0138,  0.0628,  0.1545,  0.1098,  0.1951,\n",
      "          0.4498,  0.4394,  0.5183,  0.5187,  0.4793,  0.5809,  0.5808,  0.5223,\n",
      "          0.5845,  0.5481,  0.4078,  0.5236,  0.5743,  0.6519,  0.6082,  0.5145,\n",
      "          0.4927,  0.3168,  0.3012,  0.3555,  0.5449,  0.4542,  0.1515,  0.2658,\n",
      "          0.2616,  0.3662,  0.3941,  0.3080,  0.1828,  0.2056,  0.3167,  0.3233,\n",
      "          0.1908,  0.2870,  0.2933,  0.3282,  0.0761,  0.2342,  0.1692,  0.2742,\n",
      "          0.1095,  0.1386,  0.0991,  0.1097,  0.1339,  0.3148,  0.0598,  0.1957,\n",
      "          0.0465,  0.0832,  0.0842,  0.3503,  0.2902,  0.3524,  0.4203,  0.2209,\n",
      "          0.1881,  0.2733,  0.2902,  0.4665,  0.1809,  0.2103,  0.1534,  0.1882,\n",
      "          0.3981,  0.3001,  0.3540,  0.3007,  0.2905,  0.1084,  0.2227,  0.2232,\n",
      "          0.1313,  0.2163,  0.1052,  0.0911, -0.0154,  0.1156,  0.0171, -0.0487,\n",
      "         -0.0524, -0.0950, -0.2059, -0.0359, -0.0040, -0.2132, -0.2925,  0.0071,\n",
      "         -0.1479, -0.2700, -0.2436, -0.0570, -0.1554, -0.3809, -0.0758, -0.0473,\n",
      "         -0.3027, -0.1377, -0.3457, -0.2471, -0.0460, -0.2128, -0.3480, -0.2489,\n",
      "         -0.2191, -0.2378, -0.1613,  0.0312, -0.1127, -0.1808, -0.0340,  0.0396,\n",
      "         -0.0052, -0.2029, -0.0902, -0.1663, -0.1167, -0.0273, -0.0985,  0.0376,\n",
      "          0.1975,  0.1530, -0.1339, -0.0777,  0.0226, -0.0777,  0.0468, -0.2734,\n",
      "         -0.1988, -0.2080, -0.2361, -0.1225, -0.3257, -0.0589, -0.2430, -0.2163,\n",
      "         -0.0545, -0.2321, -0.0162, -0.1306,  0.0244, -0.1079, -0.2980, -0.0177,\n",
      "         -0.0816, -0.0482, -0.1292, -0.1031, -0.2358, -0.0187, -0.0348, -0.2100,\n",
      "         -0.0061, -0.0781, -0.0985, -0.1647, -0.1477, -0.1092, -0.0086,  0.0867,\n",
      "          0.0940, -0.0823, -0.0419, -0.2489, -0.2834, -0.0585, -0.1109,  0.0187,\n",
      "         -0.0991, -0.1325, -0.2085, -0.0134, -0.0313,  0.0733, -0.0217, -0.0883,\n",
      "         -0.1048, -0.1064, -0.0678, -0.0317,  0.0734, -0.0416,  0.1099, -0.0562,\n",
      "         -0.1341, -0.1573,  0.0414,  0.0620, -0.1916, -0.0060,  0.0268,  0.1422,\n",
      "          0.1972, -0.1141,  0.0134, -0.0513, -0.0268, -0.0182, -0.0775,  0.0524],\n",
      "        [ 0.1655,  0.3015,  0.2069, -0.0785, -0.2970,  0.0519,  0.0794, -0.2096,\n",
      "         -0.2036, -0.2126, -0.0801, -0.1661,  0.1103,  0.2057,  0.5007,  0.4173,\n",
      "          0.4922,  0.4169,  0.6568,  0.6614,  0.3713,  0.3885,  0.4147,  0.5144,\n",
      "          0.4498,  0.3880,  0.4850,  0.5560,  0.3929,  0.5426,  0.5325,  0.4528,\n",
      "          0.4982,  0.6236,  0.4147,  0.5066,  0.4402,  0.6524,  0.3077,  0.3135,\n",
      "          0.6565,  0.3703,  0.4576,  0.4309,  0.4256,  0.3793,  0.4610,  0.3526,\n",
      "          0.1669,  0.1897,  0.1299,  0.2034,  0.0481,  0.1083, -0.0933, -0.0487,\n",
      "         -0.0568, -0.1858, -0.0423, -0.0183, -0.2645, -0.2841, -0.0601,  0.0084,\n",
      "         -0.1908, -0.1570,  0.0520,  0.1236, -0.0048, -0.0506, -0.0986,  0.0627,\n",
      "          0.0281, -0.0259,  0.0452,  0.0154,  0.0979,  0.0507, -0.0054, -0.0976,\n",
      "         -0.0593,  0.0807,  0.0405,  0.0604, -0.1053,  0.1036,  0.1817,  0.1646,\n",
      "         -0.0880,  0.0495,  0.0462,  0.2304,  0.0658,  0.0764,  0.1850,  0.0932,\n",
      "          0.1010,  0.1283,  0.0789,  0.0711,  0.2189,  0.0312,  0.1078,  0.2065,\n",
      "          0.1621,  0.0852,  0.1601,  0.0804,  0.0416,  0.1989,  0.0772, -0.1009,\n",
      "          0.0551,  0.0145,  0.0744, -0.1153, -0.0327,  0.0458,  0.0554, -0.0196,\n",
      "          0.0525,  0.0223,  0.0147, -0.0029,  0.0279, -0.1013, -0.1163,  0.0671,\n",
      "         -0.0688, -0.0709, -0.0020,  0.0564, -0.0696,  0.0755, -0.0016,  0.0656,\n",
      "         -0.0944, -0.0094,  0.0854,  0.0478,  0.0257, -0.0762, -0.1446, -0.1444,\n",
      "          0.0873,  0.1391, -0.0518, -0.0380,  0.0020, -0.1081,  0.0157, -0.1540,\n",
      "         -0.0559,  0.0620, -0.1725, -0.0398, -0.1694, -0.0502, -0.1480, -0.1358,\n",
      "         -0.0965, -0.0645, -0.0949, -0.0701, -0.1880, -0.0995, -0.0576, -0.1225,\n",
      "          0.0151, -0.1571, -0.1735, -0.0722, -0.0049,  0.0198, -0.2369, -0.2726,\n",
      "         -0.0442, -0.1262, -0.0595, -0.1217, -0.0938, -0.0786, -0.1060, -0.2279,\n",
      "         -0.0651, -0.2493, -0.0125, -0.0598, -0.1854, -0.0669, -0.1125, -0.0135,\n",
      "         -0.1476, -0.0539, -0.0032, -0.0072, -0.2318,  0.0168, -0.1159, -0.1458,\n",
      "         -0.2556, -0.2013, -0.0785, -0.2421, -0.1236, -0.1326, -0.0341,  0.0025,\n",
      "         -0.0974, -0.0736, -0.0483, -0.1556, -0.2284, -0.0418, -0.0744, -0.1761,\n",
      "         -0.1072, -0.1539, -0.0766, -0.0173, -0.0407, -0.0832, -0.0785, -0.0051,\n",
      "         -0.0872, -0.1615, -0.0922, -0.1065, -0.0270, -0.0933,  0.0378,  0.0814,\n",
      "          0.0367, -0.0029,  0.0288, -0.0348, -0.1202, -0.0738, -0.1477, -0.0525,\n",
      "         -0.0407,  0.0044, -0.0508, -0.0983, -0.1496,  0.0256, -0.1536, -0.2409,\n",
      "         -0.1546, -0.0163, -0.1882,  0.0781, -0.0318, -0.0769, -0.0669,  0.0623]],\n",
      "       device='cuda:1', requires_grad=True) \n",
      " Max: tensor(0.9545, device='cuda:1', grad_fn=<MaxBackward1>) Min: tensor(-0.5068, device='cuda:1', grad_fn=<MinBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nw_mu5 = net.l5.weight_mu\\nw_mu5 = w_mu5.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\\n#net.l3.weight_rho\\nb_mu5 = net.l5.bias_mu.to(DEVICE) #5\\n#net.l3.bias_rho\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net.l1.weight_mu, '\\n', 'Max:', torch.max(net.l1.weight_mu), 'Min:',torch.min(net.l1.weight_mu))\n",
    "\n",
    "w_mu1 = net.l1.weight_mu\n",
    "w_mu1 = w_mu1.reshape(l1shape[0]*l1shape[1]).to(DEVICE)\n",
    "#net.l1.weight_rho\n",
    "b_mu1 = net.l1.bias_mu.to(DEVICE) #400\n",
    "#net.l1.bias_rho\n",
    "\n",
    "w_mu2 = net.l2.weight_mu\n",
    "w_mu2 = w_mu2.reshape(l2shape[0]*l2shape[1]).to(DEVICE)\n",
    "#net.l2.weight_rho\n",
    "b_mu2 = net.l2.bias_mu.to(DEVICE) #600\n",
    "#net.l2.bias_rho\n",
    "\n",
    "w_mu3 = net.l3.weight_mu\n",
    "w_mu3 = w_mu3.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu3 = net.l3.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho\n",
    "\n",
    "w_mu4 = net.l4.weight_mu\n",
    "w_mu4 = w_mu4.reshape(l4shape[0]*l4shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu4 = net.l4.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho\n",
    "r\"\"\"\n",
    "w_mu5 = net.l5.weight_mu\n",
    "w_mu5 = w_mu5.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu5 = net.l5.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0470924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uio/hume/student-u44/lmsunde/projects/BNN/AliaksandrFolder/FVMF.py:282: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  v0 = torch.cat([v0, torch.tensor(w0[det >= 0]).to(DEVICE)])\n",
      "/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(125.6117, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(136.2265, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "2\n",
      "tensor(105.2095, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(117.0835, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "3\n",
      "tensor(105.1532, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.4231, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "4\n",
      "tensor(100.7349, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(112.8364, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "5\n",
      "tensor(96.5484, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(108.6333, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "6\n",
      "tensor(98.6265, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(109.8491, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "7\n",
      "tensor(102.4390, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.4467, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "8\n",
      "tensor(98.6739, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(110.5443, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "9\n",
      "tensor(98.0620, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(109.3788, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "10\n",
      "tensor(97.5887, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(108.6882, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "11\n",
      "tensor(96.8688, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(108.9121, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "12\n",
      "tensor(95.4498, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(107.2324, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "13\n",
      "tensor(95.4129, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(107.2617, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "14\n",
      "tensor(102.5723, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.5452, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "15\n",
      "tensor(95.4294, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(107.3736, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "16\n",
      "tensor(95.5621, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(107.7502, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "17\n",
      "tensor(102.6774, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(114.2180, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "18\n",
      "tensor(94.7408, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(105.8973, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "19\n",
      "tensor(101.0868, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(112.4356, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "20\n",
      "tensor(96.3002, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(108.5440, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "21\n",
      "tensor(96.4392, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(108.5456, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "22\n",
      "tensor(94.6793, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(106.4778, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "23\n",
      "tensor(96.5972, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(107.2120, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "24\n",
      "tensor(97.5826, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(109.9230, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "25\n",
      "tensor(97.0505, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(109.1904, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "26\n",
      "tensor(94.6624, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(105.7324, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "27\n",
      "tensor(95.7978, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(107.0512, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "28\n",
      "tensor(100.1298, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(111.5247, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "29\n",
      "tensor(94.5744, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(105.6674, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "30\n",
      "tensor(96.9475, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(108.3491, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "Component 0 Accuracy: 654/1000\n",
      "Component 1 Accuracy: 658/1000\n",
      "Component 2 Accuracy: 660/1000\n",
      "Component 3 Accuracy: 639/1000\n",
      "Component 4 Accuracy: 648/1000\n",
      "Component 5 Accuracy: 658/1000\n",
      "Component 6 Accuracy: 668/1000\n",
      "Component 7 Accuracy: 639/1000\n",
      "Component 8 Accuracy: 643/1000\n",
      "Component 9 Accuracy: 634/1000\n",
      "Posterior Mean Accuracy: 676/1000\n",
      "Ensemble Accuracy: 680/1000\n"
     ]
    }
   ],
   "source": [
    "#import importlib\n",
    "#import os\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "#import VMF\n",
    "\n",
    "#import importlib\n",
    "#importlib.reload(VMF)\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "batch_size = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 50\n",
    "epochs = 250\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "\n",
    "TRAIN_SIZE = len(tr_ids)\n",
    "TEST_SIZE = len(te_ids)\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "NUM_TEST_BATCHES = len(te_ids)/BATCH_SIZE\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "trtimes  = np.zeros(epochs)\n",
    "w_mu = [w_mu1, w_mu2, w_mu3, w_mu4,]\n",
    "b_mu = [b_mu1, b_mu2, b_mu3, b_mu4,]\n",
    "\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net2 = FVMF.BayesianNetwork(w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                layershapes = layershapes,\n",
    "                                VD='vmf',\n",
    "                                b_kappa=torch.Tensor(1).uniform_(3,3.1),\n",
    "                                w_kappa=torch.Tensor(1).uniform_(9,9.1))\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net2.parameters(), lr=0.14)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net2, optimizer, epoch, i)\n",
    "        #print(net2.l1.weight_mu.mean())\n",
    "\n",
    "    res = FVMF.test_ensemble(net2)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5ab7d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(161.2357, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.2357, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "2\n",
      "tensor(167.3114, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(167.3114, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "3\n",
      "tensor(160.9451, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.9451, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "4\n",
      "tensor(162.1563, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.1563, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "5\n",
      "tensor(160.2093, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.2093, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "6\n",
      "tensor(164.5023, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(164.5023, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "7\n",
      "tensor(160.3920, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.3920, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "8\n",
      "tensor(165.5881, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(165.5881, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "9\n",
      "tensor(164.3335, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(164.3335, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "10\n",
      "tensor(160.3408, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.3408, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "11\n",
      "tensor(167.0934, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(167.0934, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "12\n",
      "tensor(165.0824, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(165.0824, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "13\n",
      "tensor(166.0153, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(166.0153, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "14\n",
      "tensor(170.9296, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(170.9296, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "15\n",
      "tensor(165.8385, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(165.8385, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "16\n",
      "tensor(162.4119, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.4119, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "17\n",
      "tensor(160.5038, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.5038, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "18\n",
      "tensor(160.4532, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.4532, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "19\n",
      "tensor(160.9440, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.9440, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "20\n",
      "tensor(169.0983, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(169.0983, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "21\n",
      "tensor(169.2303, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(169.2303, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "22\n",
      "tensor(161.2692, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.2692, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "23\n",
      "tensor(162.6292, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.6292, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "24\n",
      "tensor(162.4867, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.4867, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "25\n",
      "tensor(163.9055, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(163.9055, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "26\n",
      "tensor(164.0138, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(164.0138, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "27\n",
      "tensor(159.7119, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.7119, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "28\n",
      "tensor(162.7078, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.7078, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "29\n",
      "tensor(160.2000, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.2000, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "30\n",
      "tensor(161.4628, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.4628, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "Component 0 Accuracy: 180/1000\n",
      "Component 1 Accuracy: 198/1000\n",
      "Component 2 Accuracy: 186/1000\n",
      "Component 3 Accuracy: 206/1000\n",
      "Component 4 Accuracy: 218/1000\n",
      "Component 5 Accuracy: 214/1000\n",
      "Component 6 Accuracy: 212/1000\n",
      "Component 7 Accuracy: 183/1000\n",
      "Component 8 Accuracy: 180/1000\n",
      "Component 9 Accuracy: 203/1000\n",
      "Posterior Mean Accuracy: 416/1000\n",
      "Ensemble Accuracy: 197/1000\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net2.parameters(), lr=0.007)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    trtimes[epoch] = FVMF.train(net2, optimizer, epoch, i)\n",
    "    #print(net2.l1.weight_mu.mean())\n",
    "res = FVMF.test_ensemble(net2)\n",
    "#np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f2e9bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(165.5318, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(165.5318, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "2\n",
      "tensor(164.8916, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(164.8916, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "3\n",
      "tensor(161.9316, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.9316, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "4\n",
      "tensor(159.3327, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.3327, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "5\n",
      "tensor(161.1887, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.1887, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "6\n",
      "tensor(162.5252, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.5252, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "7\n",
      "tensor(163.1620, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(163.1620, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "8\n",
      "tensor(162.1133, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.1133, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "9\n",
      "tensor(167.8611, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(167.8611, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "10\n",
      "tensor(168.8333, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(168.8333, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "11\n",
      "tensor(168.1095, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(168.1095, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "12\n",
      "tensor(172.8456, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(172.8456, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "13\n",
      "tensor(160.9438, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.9438, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "14\n",
      "tensor(160.9438, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.9438, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "15\n",
      "tensor(162.3474, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.3474, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "16\n",
      "tensor(161.5258, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.5258, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "17\n",
      "tensor(167.3436, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(167.3436, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "18\n",
      "tensor(160.9100, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.9100, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "19\n",
      "tensor(165.5495, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(165.5495, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "20\n",
      "tensor(159.0421, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.0421, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "21\n",
      "tensor(159.5678, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.5678, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "22\n",
      "tensor(160.9438, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.9438, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "23\n",
      "tensor(163.1353, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(163.1353, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "24\n",
      "tensor(164.9316, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(164.9316, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "25\n",
      "tensor(161.4211, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.4211, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "26\n",
      "tensor(162.8436, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.8436, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "27\n",
      "tensor(162.4522, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.4522, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "28\n",
      "tensor(160.8215, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.8215, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "29\n",
      "tensor(161.7786, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.7786, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "30\n",
      "tensor(167.7329, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(167.7329, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "Component 0 Accuracy: 210/1000\n",
      "Component 1 Accuracy: 203/1000\n",
      "Component 2 Accuracy: 192/1000\n",
      "Component 3 Accuracy: 209/1000\n",
      "Component 4 Accuracy: 168/1000\n",
      "Component 5 Accuracy: 213/1000\n",
      "Component 6 Accuracy: 153/1000\n",
      "Component 7 Accuracy: 205/1000\n",
      "Component 8 Accuracy: 194/1000\n",
      "Component 9 Accuracy: 193/1000\n",
      "Posterior Mean Accuracy: 416/1000\n",
      "Ensemble Accuracy: 181/1000\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net2.parameters(), lr=0.0007)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    trtimes[epoch] = FVMF.train(net2, optimizer, epoch, i)\n",
    "    #print(net2.l1.weight_mu.mean())\n",
    "res = FVMF.test_ensemble(net2)\n",
    "#np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "What if the problem is that since the mu's in the Gaussian is (out,in), and the vMF is out*in,\n",
    "this could mean that we have \"strethed\" a single vMF pdf over all the parameters, while in the Gaussian we have made one for each output?\n",
    "\n",
    "I don't know, also intuitively this should be the case, since the whole point of the vMF is the norm 1, which obviously\n",
    "will require that it for each forward-pass is only one massive pdf for all inputs and outputs.\n",
    "\n",
    "Ok, this can get rough, I will try my best to reshape the w_mu's and b_mu's the best I can, perhaps this will work just fine without too much tuning...\n",
    "Hopefully.\n",
    "\n",
    "This doe not seem to ave worked. I don't konw exactly what the problem is. Per haps it is a good idea to consult the new loss function\n",
    "suggested in the paper that made the code we based our vMF on?\n",
    "\n",
    "It is very strange that the loss is not at all affected by completely ridiculous learning rates...\n",
    "\n",
    "Remember that vMF makes the norm of the weights and biases 1, not the forward pass of the x's. Hence the advantage is that the gradient\n",
    "will not explode, since the backward pass of it will also be approx. 1. In batchnorm, maybe the gradient can explode? Since the weights \n",
    "can be whatever?\n",
    "\n",
    "\n",
    "IMPORTANT: The gaussian neuralnet will also collapse to 257 if I apply more than 3 layers. This must be somehow related to the similar\n",
    "behavior in the vMF when the size of each layer exceeds 3. Per haps there is an error in the loss afterall?\n",
    "However, in the Gaussian case, increasing the learning rate by a factor of 10 solved the issue. This makes me suspect it is the mathematical\n",
    "properties of the loss function, rather than incorrect implementation.\n",
    "\"\"\"\n",
    "\n",
    "#When l4 is (3,5):\n",
    "r\"\"\"\n",
    "File ~/projects/BNN/AliaksandrFolder/FVMF.py:289, in vMF.sample(self, N, rsf)\n",
    "    287 e1mu = torch.zeros(d, 1).to(DEVICE)\n",
    "    288 e1mu[0, 0] = 1.0\n",
    "--> 289 e1mu = e1mu - self.mu if len(self.mu.shape) == 2 else e1mu - self.mu.unsqueeze(1) #e1mu.shape = (1,self.x_dim). mu_unnorm.shape = (mu_unnorm)\n",
    "    290 e1mu = e1mu / norm(e1mu, dim=0).to(DEVICE)\n",
    "    291 samples = samples - 2 * (samples @ e1mu) @ e1mu.t()\n",
    "\n",
    "RuntimeError: The size of tensor a (15) must match the size of tensor b (9) at non-singleton dimension 0\n",
    "\"\"\"\n",
    "\n",
    "#When l4 is (5,5):\n",
    "r\"\"\"\n",
    "File ~/projects/BNN/AliaksandrFolder/FVMF.py:289, in vMF.sample(self, N, rsf)\n",
    "    287 e1mu = torch.zeros(d, 1).to(DEVICE)\n",
    "    288 e1mu[0, 0] = 1.0\n",
    "--> 289 e1mu = e1mu - self.mu if len(self.mu.shape) == 2 else e1mu - self.mu.unsqueeze(1) #e1mu.shape = (1,self.x_dim). mu_unnorm.shape = (mu_unnorm)\n",
    "    290 e1mu = e1mu / norm(e1mu, dim=0).to(DEVICE)\n",
    "    291 samples = samples - 2 * (samples @ e1mu) @ e1mu.t()\n",
    "\n",
    "RuntimeError: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 0\n",
    "\"\"\"\n",
    "#These errors above were caused by my initialization being wrong. I copy paster mu_3 for layer4, and forgot to change to mu_4. So now \n",
    "#I always get the error below.\n",
    "\n",
    "\n",
    "#in all cases now: \n",
    "\n",
    "r\"\"\"\n",
    "It seems that the whole thing does not progress at all. We just get the warning and then no further output.\n",
    "\n",
    "self.l4(x, sample)\n",
    "\n",
    "--> self.bias.sample()\n",
    "\n",
    "It always get's stuck there!!\n",
    "\n",
    "Specifically, it get's stuck in the while loop:\n",
    "\n",
    "while len(v0) < N:\n",
    "            eps = beta.sample([1, rsf * (N - len(v0))]).squeeze().to(DEVICE)\n",
    "            uns = uniform.sample([1, rsf * (N - len(v0))]).squeeze().to(DEVICE)\n",
    "            w0 = (1 - (1 + bb) * eps) / (1 - (1 - bb) * eps)\n",
    "            t0 = (2 * aa * bb) / (1 - (1 - bb) * eps)\n",
    "            det = (d - 1) * t0.log() - t0 + dd - uns.log()\n",
    "            v0 = torch.cat([v0, torch.tensor(w0[det >= 0]).to(DEVICE)])\n",
    "            if len(v0) > N:\n",
    "                v0 = v0[:N]\n",
    "                break\n",
    "\"\"\"\n",
    "\n",
    "r\"\"\"\n",
    "From further investigations it is clear that the error lies in w0[det >= 0] consistently being an empty Tensor.\n",
    "\n",
    "Even further, bb is 0 here which it usually is not. That must definitely indicate something is wrong.\n",
    "\n",
    "Adjusting the initialization of kappa to be 9 or less on both weights and biases makes the code run, \n",
    "but posterior collapse is back. Increasing kappa seems to increase the compute aswell... however, getting the kappa inits\n",
    "closer to 10 seems to also help avoid the posterior collapse. And the lower bound increased also helps, looks like 3 is optimal.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c14fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
