{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ddbed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "Classes loaded\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "\n",
    "import VMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9837faf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#epochs = 225\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#trtimes  = np.zeros(epochs)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# make inference on 10 networks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mlayers = [(256, 400),(400, 600),(400, 600),(600, 5)]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mself_layerdict = {}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03mself_l1\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mset_printoptions(edgeitems\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor((\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m     32\u001b[0m x\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "#epochs = 225\n",
    "#trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "#for i in range(0, 1):\n",
    "#    print(i)\n",
    "#    torch.manual_seed(i)\n",
    "#    net = VMF.BayesianNetwork().to(VMF.DEVICE)\n",
    "#    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "#    for epoch in range(epochs):\n",
    "#\n",
    "#        trtimes[epoch] = VMF.train(net, optimizer, epoch, i)\n",
    "#        print(net.l1.weight_mu.mean())\n",
    "#\n",
    "#    res = VMF.test_ensemble()\n",
    "#\n",
    "#    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "r\"\"\"\n",
    "layers = [(256, 400),(400, 600),(400, 600),(600, 5)]\n",
    "self_layerdict = {}\n",
    "i = 0\n",
    "while i<len(layers):\n",
    "            name = 'self_l'+str(i)\n",
    "            self_layerdict[name] = layers[i]\n",
    "            i = i+1\n",
    "            print(name)\n",
    "for k,v in self_layerdict.items():\n",
    "    exec(\"%s = %s\" % (k, v))\n",
    "self_l1\n",
    "\"\"\"\n",
    "torch.set_printoptions(edgeitems=1)\n",
    "x=torch.tensor((4,4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a223bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca8ca520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(251.5287, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.0679, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0040, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(251.3824, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.7604, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0029, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(249.7483, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.0165, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0031, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(245.5632, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(155.7485, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0097, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(233.2977, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(144.6123, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0204, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(219.1594, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(131.0194, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0242, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(210.3794, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(123.1321, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0213, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(204.0602, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(116.9010, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0173, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(200.6461, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(113.7153, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0152, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(196.7289, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(110.7560, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0111, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(193.5298, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(107.5611, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0074, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(190.0633, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(105.0851, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0049, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(185.4069, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(101.2987, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0026, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(184.6697, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(100.6750, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0011, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(178.9831, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(96.7930, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0008, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(178.0702, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(95.4042, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0027, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(176.0226, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(93.4787, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0036, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(174.2403, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(91.7499, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0049, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(172.5303, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(90.5331, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0064, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(170.5629, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(89.5907, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0078, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(168.7057, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(88.7159, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0085, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(167.3969, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(87.0187, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0092, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(165.6795, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(86.3942, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0102, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(164.3373, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(84.8164, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0110, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(161.3309, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(83.9651, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0118, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "26\n",
      "tensor(160.4807, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(83.3044, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0118, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "27\n",
      "tensor(159.5173, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(82.2616, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0127, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "28\n",
      "tensor(158.9262, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(82.2104, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0131, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "29\n",
      "tensor(156.5130, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(81.2434, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0141, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "30\n",
      "tensor(155.4329, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(79.8154, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0148, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "31\n",
      "tensor(153.2176, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(77.7497, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0145, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "32\n",
      "tensor(152.3419, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(77.7240, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0146, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "33\n",
      "tensor(152.2692, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(79.2551, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0147, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "34\n",
      "tensor(148.0408, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(75.0622, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0151, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "35\n",
      "tensor(146.9435, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(74.5964, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0157, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "36\n",
      "tensor(144.1741, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(71.3327, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0165, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "37\n",
      "tensor(141.6204, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(69.6155, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0175, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "38\n",
      "tensor(140.3093, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(68.6643, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0181, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "39\n",
      "tensor(137.5406, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(66.2880, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0178, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "40\n",
      "tensor(132.9795, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(62.2007, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0195, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Component 0 Accuracy: 859/1000\n",
      "Component 1 Accuracy: 854/1000\n",
      "Component 2 Accuracy: 858/1000\n",
      "Component 3 Accuracy: 858/1000\n",
      "Component 4 Accuracy: 856/1000\n",
      "Component 5 Accuracy: 859/1000\n",
      "Component 6 Accuracy: 855/1000\n",
      "Component 7 Accuracy: 858/1000\n",
      "Component 8 Accuracy: 857/1000\n",
      "Component 9 Accuracy: 860/1000\n",
      "Posterior Mean Accuracy: 860/1000\n",
      "Ensemble Accuracy: 860/1000\n"
     ]
    }
   ],
   "source": [
    "#matplotlib inline\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import VMF\n",
    "\n",
    "import importlib\n",
    "importlib.reload(VMF)\n",
    "\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPUs are used!\")\n",
    "else:\n",
    "    print(\"CPUs are used!\")\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "batch_size = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "epochs = 250\n",
    "pepochs = 50\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "\n",
    "TRAIN_SIZE = len(tr_ids)\n",
    "TEST_SIZE = len(te_ids)\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "NUM_TEST_BATCHES = len(te_ids)/BATCH_SIZE\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "\n",
    "#The net does not like to get larger at a given layer??\n",
    "l1shape=(256, 3)\n",
    "l2shape=(3, 3)\n",
    "l3shape=(3, 3)\n",
    "l4shape=(3, 5)\n",
    "\n",
    "\n",
    "epochs = 40\n",
    "trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net = VMF.BayesianNetwork(l1=l1shape, l2=l2shape, l3=l3shape,l4=l4shape,BN='notbatchnorm').to(DEVICE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0007)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = VMF.train(net, optimizer, epoch, i)\n",
    "        print(net.l1.weight_mu.mean())\n",
    "\n",
    "    res = VMF.test_ensemble(net)\n",
    "\n",
    "    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7f26701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nw_mu5 = net.l5.weight_mu\\nw_mu5 = w_mu5.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\\n#net.l3.weight_rho\\nb_mu5 = net.l5.bias_mu.to(DEVICE) #5\\n#net.l3.bias_rho\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_mu1 = net.l1.weight_mu\n",
    "w_mu1 = w_mu1.reshape(l1shape[0]*l1shape[1]).to(DEVICE)\n",
    "#net.l1.weight_rho\n",
    "b_mu1 = net.l1.bias_mu.to(DEVICE) #400\n",
    "#net.l1.bias_rho\n",
    "\n",
    "w_mu2 = net.l2.weight_mu\n",
    "w_mu2 = w_mu2.reshape(l2shape[0]*l2shape[1]).to(DEVICE)\n",
    "#net.l2.weight_rho\n",
    "b_mu2 = net.l2.bias_mu.to(DEVICE) #600\n",
    "#net.l2.bias_rho\n",
    "\n",
    "w_mu3 = net.l3.weight_mu\n",
    "w_mu3 = w_mu3.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu3 = net.l3.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho\n",
    "\n",
    "w_mu4 = net.l4.weight_mu\n",
    "w_mu4 = w_mu4.reshape(l4shape[0]*l4shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu4 = net.l4.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho\n",
    "r\"\"\"\n",
    "w_mu5 = net.l5.weight_mu\n",
    "w_mu5 = w_mu5.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu5 = net.l5.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0470924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 98\u001b[0m\n\u001b[1;32m     88\u001b[0m net2 \u001b[38;5;241m=\u001b[39m FVMF\u001b[38;5;241m.\u001b[39mBayesianNetwork(\u001b[38;5;66;03m#w_mu1 = w_mu1, w_mu2 = w_mu2, w_mu3 = w_mu3, w_mu4 = w_mu4, b_mu1 = b_mu1, b_mu2 = b_mu2, \u001b[39;00m\n\u001b[1;32m     89\u001b[0m                             \u001b[38;5;66;03m#b_mu3 = b_mu3, b_mu4 = b_mu4, \u001b[39;00m\n\u001b[1;32m     90\u001b[0m                             l1\u001b[38;5;241m=\u001b[39ml1shape, l2\u001b[38;5;241m=\u001b[39ml2shape, l3\u001b[38;5;241m=\u001b[39ml3shape, l4\u001b[38;5;241m=\u001b[39ml4shape, VD\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvmf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#for j,p in enumerate(net2.l1.parameters()):    \u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m#    p.requires_grad_(False)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#    \u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#for j,p in enumerate(net2.l2.parameters()):\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#    p.requires_grad_(False)\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0007\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    103\u001b[0m     trtimes[epoch] \u001b[38;5;241m=\u001b[39m FVMF\u001b[38;5;241m.\u001b[39mtrain(net2, optimizer, epoch, i)\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/optim/adam.py:137\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(weight_decay))\n\u001b[1;32m    133\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    134\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    135\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    136\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/optim/optimizer.py:61\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     59\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     63\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import os\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "import VMF\n",
    "\n",
    "import importlib\n",
    "importlib.reload(VMF)\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPUs are used!\")\n",
    "else:\n",
    "    print(\"CPUs are used!\")\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "batch_size = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 50\n",
    "epochs = 250\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "\n",
    "TRAIN_SIZE = len(tr_ids)\n",
    "TEST_SIZE = len(te_ids)\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "NUM_TEST_BATCHES = len(te_ids)/BATCH_SIZE\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "\n",
    "epochs = 250\n",
    "trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net2 = FVMF.BayesianNetwork(#w_mu1 = w_mu1, w_mu2 = w_mu2, w_mu3 = w_mu3, w_mu4 = w_mu4, b_mu1 = b_mu1, b_mu2 = b_mu2, \n",
    "                                #b_mu3 = b_mu3, b_mu4 = b_mu4, \n",
    "                                l1=l1shape, l2=l2shape, l3=l3shape, l4=l4shape, VD='vmf')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net2.parameters(), lr=0.0007)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net2, optimizer, epoch, i)\n",
    "        print(net2.l1.weight_mu.mean())\n",
    "\n",
    "    res = FVMF.test_ensemble(net2)\n",
    "\n",
    "    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ab7d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(130.7404, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(130.7404, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(130.6554, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(130.6554, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(141.8726, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(141.8726, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(133.0124, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(133.0124, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(134.3275, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(134.3275, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(141.2159, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(141.2159, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(132.1792, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(132.1792, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(140.4431, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(140.4431, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(136.8403, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(136.8403, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(132.8992, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(132.8992, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(138.7412, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(138.7412, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(134.5779, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(134.5779, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(130.6562, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(130.6562, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(146.8090, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(146.8090, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(138.2236, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(138.2236, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(130.3611, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(130.3611, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(129.8302, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(129.8302, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(139.3024, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(139.3024, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(131.3318, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(131.3318, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(140.5484, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(140.5484, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(133.5248, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(133.5248, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(136.0603, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(136.0603, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(131.1511, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(131.1511, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(163.7543, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(163.7543, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(132.2271, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(132.2271, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "26\n",
      "tensor(135.5673, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(135.5673, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "27\n",
      "tensor(142.9764, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(142.9764, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "28\n",
      "tensor(130.0980, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(130.0980, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "29\n",
      "tensor(134.1396, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(134.1396, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "30\n",
      "tensor(138.7288, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(138.7288, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "31\n",
      "tensor(140.3419, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(140.3419, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "32\n",
      "tensor(153.0301, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(153.0301, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "33\n",
      "tensor(133.4494, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(133.4494, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "34\n",
      "tensor(136.4703, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(136.4703, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "35\n",
      "tensor(137.4430, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(137.4430, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "36\n",
      "tensor(147.0086, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(147.0086, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "37\n",
      "tensor(136.8139, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(136.8139, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "38\n",
      "tensor(134.7560, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(134.7560, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "39\n",
      "tensor(134.2073, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(134.2073, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "40\n",
      "tensor(128.7764, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(128.7764, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "41\n",
      "tensor(137.1156, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(137.1156, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "42\n",
      "tensor(131.1805, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(131.1805, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "43\n",
      "tensor(132.1191, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(132.1191, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "44\n",
      "tensor(165.1645, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(165.1645, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "45\n",
      "tensor(135.6601, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(135.6601, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "46\n",
      "tensor(134.9917, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(134.9917, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "tensor(136.4611, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(136.4611, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "48\n",
      "tensor(138.4522, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(138.4522, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "49\n",
      "tensor(156.6162, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(156.6162, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "50\n",
      "tensor(134.1363, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(134.1363, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0650, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Component 0 Accuracy: 375/1000\n",
      "Component 1 Accuracy: 413/1000\n",
      "Component 2 Accuracy: 402/1000\n",
      "Component 3 Accuracy: 385/1000\n",
      "Component 4 Accuracy: 418/1000\n",
      "Component 5 Accuracy: 370/1000\n",
      "Component 6 Accuracy: 391/1000\n",
      "Component 7 Accuracy: 367/1000\n",
      "Component 8 Accuracy: 379/1000\n",
      "Component 9 Accuracy: 390/1000\n",
      "Posterior Mean Accuracy: 328/1000\n",
      "Ensemble Accuracy: 407/1000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "What if the problem is that since the mu's in the Gaussian is (out,in), and the vMF is out*in,\n",
    "this could mean that we have \"strethed\" a single vMF pdf over all the parameters, while in the Gaussian we have made one for each output?\n",
    "\n",
    "I don't know, also intuitively this should be the case, since the whole point of the vMF is the norm 1, which obviously\n",
    "will require that it for each forward-pass is only one massive pdf for all inputs and outputs.\n",
    "\n",
    "Ok, this can get rough, I will try my best to reshape the w_mu's and b_mu's the best I can, perhaps this will work just fine without too much tuning...\n",
    "Hopefully.\n",
    "\n",
    "This doe not seem to ave worked. I don't konw exactly what the problem is. Per haps it is a good idea to consult the new loss function\n",
    "suggested in the paper that made the code we based our vMF on?\n",
    "\n",
    "It is very strange that the loss is not at all affected by completely ridiculous learning rates...\n",
    "\n",
    "Remember that vMF makes the norm of the weights and biases 1, not the forward pass of the x's. Hence the advantage is that the gradient\n",
    "will not explode, since the backward pass of it will also be approx. 1. In batchnorm, maybe the gradient can explode? Since the weights \n",
    "can be whatever?\n",
    "\n",
    "\n",
    "IMPORTANT: The gaussian neuralnet will also collapse to 257 if I apply more than 3 layers. This must be somehow related to the similar\n",
    "behavior in the vMF when the size of each layer exceeds 3. Per haps there is an error in the loss afterall?\n",
    "However, in the Gaussian case, increasing the learning rate by a factor of 10 solved the issue. This makes me suspect it is the mathematical\n",
    "properties of the loss function, rather than incorrect implementation.\n",
    "\"\"\"\n",
    "\n",
    "#When l4 is (3,5):\n",
    "r\"\"\"\n",
    "File ~/projects/BNN/AliaksandrFolder/FVMF.py:289, in vMF.sample(self, N, rsf)\n",
    "    287 e1mu = torch.zeros(d, 1).to(DEVICE)\n",
    "    288 e1mu[0, 0] = 1.0\n",
    "--> 289 e1mu = e1mu - self.mu if len(self.mu.shape) == 2 else e1mu - self.mu.unsqueeze(1) #e1mu.shape = (1,self.x_dim). mu_unnorm.shape = (mu_unnorm)\n",
    "    290 e1mu = e1mu / norm(e1mu, dim=0).to(DEVICE)\n",
    "    291 samples = samples - 2 * (samples @ e1mu) @ e1mu.t()\n",
    "\n",
    "RuntimeError: The size of tensor a (15) must match the size of tensor b (9) at non-singleton dimension 0\n",
    "\"\"\"\n",
    "\n",
    "#When l4 is (5,5):\n",
    "r\"\"\"\n",
    "File ~/projects/BNN/AliaksandrFolder/FVMF.py:289, in vMF.sample(self, N, rsf)\n",
    "    287 e1mu = torch.zeros(d, 1).to(DEVICE)\n",
    "    288 e1mu[0, 0] = 1.0\n",
    "--> 289 e1mu = e1mu - self.mu if len(self.mu.shape) == 2 else e1mu - self.mu.unsqueeze(1) #e1mu.shape = (1,self.x_dim). mu_unnorm.shape = (mu_unnorm)\n",
    "    290 e1mu = e1mu / norm(e1mu, dim=0).to(DEVICE)\n",
    "    291 samples = samples - 2 * (samples @ e1mu) @ e1mu.t()\n",
    "\n",
    "RuntimeError: The size of tensor a (25) must match the size of tensor b (15) at non-singleton dimension 0\n",
    "\"\"\"\n",
    "#These errors above were caused by my initialization being wrong. I copy paster mu_3 for layer4, and forgot to change to mu_4. So now \n",
    "#I always get the error below.\n",
    "\n",
    "\n",
    "#in all cases now: \n",
    "\n",
    "r\"\"\"\n",
    "It seems that the whole thing does not progress at all. We just get the warning and then no further output.\n",
    "\n",
    "self.l4(x, sample)\n",
    "\n",
    "--> self.bias.sample()\n",
    "\n",
    "It always get's stuck there!!\n",
    "\n",
    "Specifically, it get's stuck in the while loop:\n",
    "\n",
    "while len(v0) < N:\n",
    "            eps = beta.sample([1, rsf * (N - len(v0))]).squeeze().to(DEVICE)\n",
    "            uns = uniform.sample([1, rsf * (N - len(v0))]).squeeze().to(DEVICE)\n",
    "            w0 = (1 - (1 + bb) * eps) / (1 - (1 - bb) * eps)\n",
    "            t0 = (2 * aa * bb) / (1 - (1 - bb) * eps)\n",
    "            det = (d - 1) * t0.log() - t0 + dd - uns.log()\n",
    "            v0 = torch.cat([v0, torch.tensor(w0[det >= 0]).to(DEVICE)])\n",
    "            if len(v0) > N:\n",
    "                v0 = v0[:N]\n",
    "                break\n",
    "\"\"\"\n",
    "\n",
    "r\"\"\"\n",
    "From further investigations it is clear that the error lies in w0[det >= 0] consistently being an empty Tensor.\n",
    "\n",
    "Even further, bb is 0 here which it usually is not. That must definitely indicate something is wrong.\n",
    "\n",
    "Adjusting the initialization of kappa to be much smaller on both weights and biases makes the code run, but posterior collapse is back.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
