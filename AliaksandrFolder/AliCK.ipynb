{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ddbed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "Classes loaded\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "\n",
    "import VMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9837faf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epochs = 225\n",
    "#trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "#for i in range(0, 1):\n",
    "#    print(i)\n",
    "#    torch.manual_seed(i)\n",
    "#    net = VMF.BayesianNetwork().to(VMF.DEVICE)\n",
    "#    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "#    for epoch in range(epochs):\n",
    "#\n",
    "#        trtimes[epoch] = VMF.train(net, optimizer, epoch, i)\n",
    "#        print(net.l1.weight_mu.mean())\n",
    "#\n",
    "#    res = VMF.test_ensemble()\n",
    "#\n",
    "#    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "r\"\"\"\n",
    "layers = [(256, 400),(400, 600),(400, 600),(600, 5)]\n",
    "self_layerdict = {}\n",
    "i = 0\n",
    "while i<len(layers):\n",
    "            name = 'self_l'+str(i)\n",
    "            self_layerdict[name] = layers[i]\n",
    "            i = i+1\n",
    "            print(name)\n",
    "for k,v in self_layerdict.items():\n",
    "    exec(\"%s = %s\" % (k, v))\n",
    "self_l1\n",
    "\"\"\"\n",
    "torch.set_printoptions(edgeitems=1)\n",
    "x=torch.tensor((4,4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a223bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8ca520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(253.3895, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.7077, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0016, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(252.4367, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.4075, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0024, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(251.8409, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.3281, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0029, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(250.9728, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.5633, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0034, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(251.1023, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(158.8183, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0036, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(250.0793, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(158.5981, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0039, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(248.6809, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(156.6419, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0042, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(247.2079, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(156.2138, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0043, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(246.4794, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(155.4216, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0044, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(244.5963, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(153.0261, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0044, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(241.4540, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(151.2758, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0044, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(240.2089, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(150.2119, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0046, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(238.3647, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(147.1735, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0046, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(236.2291, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(145.7992, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0047, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(234.0365, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(143.2255, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0047, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(230.3713, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(139.6014, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0049, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(231.0434, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(139.7317, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0051, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(225.9380, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(135.6414, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0053, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(225.7795, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(134.7536, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0057, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(224.0189, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(133.4315, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0061, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(222.4492, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(132.0904, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0067, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(221.4272, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(130.9827, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0075, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(216.4536, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(126.3281, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0083, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(215.7192, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(125.4438, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0093, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(214.1428, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(123.5022, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0104, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Component 0 Accuracy: 555/1000\n",
      "Component 1 Accuracy: 548/1000\n",
      "Component 2 Accuracy: 551/1000\n",
      "Component 3 Accuracy: 545/1000\n",
      "Component 4 Accuracy: 563/1000\n",
      "Component 5 Accuracy: 560/1000\n",
      "Component 6 Accuracy: 575/1000\n",
      "Component 7 Accuracy: 558/1000\n",
      "Component 8 Accuracy: 534/1000\n",
      "Component 9 Accuracy: 542/1000\n",
      "Posterior Mean Accuracy: 546/1000\n",
      "Ensemble Accuracy: 548/1000\n"
     ]
    }
   ],
   "source": [
    "#matplotlib inline\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import VMF\n",
    "\n",
    "import importlib\n",
    "importlib.reload(VMF)\n",
    "\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPUs are used!\")\n",
    "else:\n",
    "    print(\"CPUs are used!\")\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "batch_size = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "epochs = 250\n",
    "pepochs = 50\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "\n",
    "TRAIN_SIZE = len(tr_ids)\n",
    "TEST_SIZE = len(te_ids)\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "NUM_TEST_BATCHES = len(te_ids)/BATCH_SIZE\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "l1shape=(256, 3)\n",
    "l2shape=(3, 3)\n",
    "l3shape=(3, 5)\n",
    "\n",
    "\n",
    "epochs = 25\n",
    "trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net = VMF.BayesianNetwork(l1=l1shape, l2=l2shape, l3=l3shape,BN='batchnorm').to(DEVICE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = VMF.train(net, optimizer, epoch, i)\n",
    "        print(net.l1.weight_mu.mean())\n",
    "\n",
    "    res = VMF.test_ensemble(net)\n",
    "\n",
    "    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f26701",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mu1 = net.l1.weight_mu\n",
    "w_mu1 = w_mu1.reshape(l1shape[0]*l1shape[1]).to(DEVICE)\n",
    "#net.l1.weight_rho\n",
    "b_mu1 = net.l1.bias_mu.to(DEVICE) #400\n",
    "#net.l1.bias_rho\n",
    "\n",
    "w_mu2 = net.l2.weight_mu\n",
    "w_mu2 = w_mu2.reshape(l2shape[0]*l2shape[1]).to(DEVICE)\n",
    "#net.l2.weight_rho\n",
    "b_mu2 = net.l2.bias_mu.to(DEVICE) #600\n",
    "#net.l2.bias_rho\n",
    "\n",
    "w_mu3 = net.l3.weight_mu\n",
    "w_mu3 = w_mu3.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu3 = net.l3.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0470924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uio/hume/student-u44/lmsunde/projects/BNN/AliaksandrFolder/FVMF.py:277: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  v0 = torch.cat([v0, torch.tensor(w0[det >= 0]).to(DEVICE)])\n",
      "/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(133.3588, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(173.3521, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0099, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(123.6516, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(164.4161, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0095, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(122.7540, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.9834, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0084, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(119.4528, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.3643, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0065, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(124.7915, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(164.9965, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0045, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(122.4629, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.5589, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0032, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(122.7129, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.9332, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0020, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(122.9132, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(163.2178, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0010, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(126.5917, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(166.5007, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0004, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(121.0241, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.2764, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0009, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(122.2022, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.8694, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0023, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(116.4124, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(156.3069, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0037, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(120.6540, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.7540, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0049, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(116.2177, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(156.6597, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0061, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(115.6030, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(155.9288, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0083, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(117.9594, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(158.4841, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0106, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(116.1521, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(155.9029, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0120, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(110.0855, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(150.1719, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0133, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(119.5959, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.6607, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0148, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(118.8400, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.2472, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0159, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(124.2890, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(164.2369, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0176, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(119.0552, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.2692, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0191, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(116.6771, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(156.5450, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0204, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(119.9365, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.7224, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0216, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(118.6056, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(158.7891, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0235, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "26\n",
      "tensor(120.3516, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.2557, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0250, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "27\n",
      "tensor(117.0225, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(156.8654, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0267, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "28\n",
      "tensor(120.9107, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.2229, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0287, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "29\n",
      "tensor(119.1184, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.1645, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0302, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "30\n",
      "tensor(115.1326, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(154.9622, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0318, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "31\n",
      "tensor(119.5045, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.6447, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0329, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "32\n",
      "tensor(115.1740, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(155.3520, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0342, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "33\n",
      "tensor(112.0972, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(152.1724, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0356, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "34\n",
      "tensor(108.8400, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(148.6583, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0372, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "35\n",
      "tensor(120.3631, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.6240, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0388, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "36\n",
      "tensor(114.8035, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(155.3061, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0407, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "37\n",
      "tensor(115.2522, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(155.7201, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0422, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "38\n",
      "tensor(117.2462, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(157.3341, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0437, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "39\n",
      "tensor(109.5395, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(149.3499, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0453, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "40\n",
      "tensor(113.9279, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(154.0656, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0466, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "41\n",
      "tensor(115.7062, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(155.8874, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0477, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "42\n",
      "tensor(116.5774, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(156.5271, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0486, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "43\n",
      "tensor(111.9737, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(151.9613, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0491, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "44\n",
      "tensor(110.4924, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(150.6852, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0501, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "45\n",
      "tensor(120.9501, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.0625, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0510, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "46\n",
      "tensor(104.2112, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(144.3683, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0516, device='cuda:1', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "tensor(105.2796, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(145.2800, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0527, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "48\n",
      "tensor(107.5399, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(148.1114, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0534, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "49\n",
      "tensor(106.1646, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(146.3329, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0542, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "50\n",
      "tensor(110.7962, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(150.6084, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0550, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Component 0 Accuracy: 347/1000\n",
      "Component 1 Accuracy: 346/1000\n",
      "Component 2 Accuracy: 322/1000\n",
      "Component 3 Accuracy: 349/1000\n",
      "Component 4 Accuracy: 391/1000\n",
      "Component 5 Accuracy: 327/1000\n",
      "Component 6 Accuracy: 333/1000\n",
      "Component 7 Accuracy: 376/1000\n",
      "Component 8 Accuracy: 312/1000\n",
      "Component 9 Accuracy: 289/1000\n",
      "Posterior Mean Accuracy: 350/1000\n",
      "Ensemble Accuracy: 406/1000\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import os\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPUs are used!\")\n",
    "else:\n",
    "    print(\"CPUs are used!\")\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "batch_size = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "epochs = 250\n",
    "pepochs = 50\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "\n",
    "TRAIN_SIZE = len(tr_ids)\n",
    "TEST_SIZE = len(te_ids)\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "NUM_TEST_BATCHES = len(te_ids)/BATCH_SIZE\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net2 = FVMF.BayesianNetwork(w_mu1 = w_mu1, w_mu2 = w_mu2, w_mu3 = w_mu3, b_mu1 = b_mu1, b_mu2 = b_mu2, b_mu3 = b_mu3, l1=l1shape, l2=l2shape, l3=l3shape, VD='vmf')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net2.parameters(), lr=0.00005)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net2, optimizer, epoch, i)\n",
    "        print(net2.l1.weight_mu.mean())\n",
    "\n",
    "    res = FVMF.test_ensemble(net2)\n",
    "\n",
    "    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(20,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "What if the problem is that since the mu's in the Gaussian is (out,in), and the vMF is out*in,\n",
    "this could mean that we have \"strethed\" a single vMF pdf over all the parameters, while in the Gaussian we have made one for each output?\n",
    "\n",
    "I don't know, also intuitively this should be the case, since the whole point of the vMF is the norm 1, which obviously\n",
    "will require that it for each forward-pass is only one massive pdf for all inputs and outputs.\n",
    "\n",
    "Ok, this can get rough, I will try my best to reshape the w_mu's and b_mu's the best I can, perhaps this will work just fine without too much tuning...\n",
    "Hopefully.\n",
    "\n",
    "This doe not seem to ave worked. I don't konw exactly what the problem is. Per haps it is a good idea to consult the new loss function\n",
    "suggested in the paper that made the code we based our vMF on?\n",
    "\n",
    "It is very strange that the loss is not at all affected by completely ridiculous learning rates...\n",
    "\n",
    "Remember that vMF makes the norm of the weights and biases 1, not the forward pass of the x's. Hence the advantage is that the gradient\n",
    "will not explode, since the backward pass of it will also be approx. 1. In batchnorm, maybe the gradient can explode? Since the weights \n",
    "can be whatever?\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
