{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ddbed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "Classes loaded\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "\n",
    "import VMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9837faf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epochs = 225\n",
    "#trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "#for i in range(0, 1):\n",
    "#    print(i)\n",
    "#    torch.manual_seed(i)\n",
    "#    net = VMF.BayesianNetwork().to(VMF.DEVICE)\n",
    "#    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "#    for epoch in range(epochs):\n",
    "#\n",
    "#        trtimes[epoch] = VMF.train(net, optimizer, epoch, i)\n",
    "#        print(net.l1.weight_mu.mean())\n",
    "#\n",
    "#    res = VMF.test_ensemble()\n",
    "#\n",
    "#    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "r\"\"\"\n",
    "layers = [(256, 400),(400, 600),(400, 600),(600, 5)]\n",
    "self_layerdict = {}\n",
    "i = 0\n",
    "while i<len(layers):\n",
    "            name = 'self_l'+str(i)\n",
    "            self_layerdict[name] = layers[i]\n",
    "            i = i+1\n",
    "            print(name)\n",
    "for k,v in self_layerdict.items():\n",
    "    exec(\"%s = %s\" % (k, v))\n",
    "self_l1\n",
    "\"\"\"\n",
    "torch.set_printoptions(edgeitems=1)\n",
    "x=torch.tensor((4,4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a223bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8ca520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(253.3895, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.7077, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0016, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(252.4367, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.4075, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0024, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(251.8409, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.3281, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0029, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(250.9728, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.5633, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0034, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(251.1023, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(158.8183, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0036, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(250.0793, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(158.5981, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0039, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(248.6809, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(156.6419, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0042, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(247.2079, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(156.2138, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0043, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(246.4794, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(155.4216, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0044, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(244.5963, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(153.0261, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0044, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(241.4540, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(151.2758, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0044, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(240.2089, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(150.2119, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0046, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(238.3647, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(147.1735, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0046, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(236.2291, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(145.7992, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0047, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(234.0365, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(143.2255, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0047, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(230.3713, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(139.6014, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0049, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(231.0434, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(139.7317, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0051, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(225.9380, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(135.6414, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0053, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(225.7795, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(134.7536, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0057, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(224.0189, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(133.4315, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0061, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(222.4492, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(132.0904, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0067, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(221.4272, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(130.9827, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0075, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(216.4536, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(126.3281, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0083, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(215.7192, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(125.4438, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0093, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(214.1428, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(123.5022, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.0104, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Component 0 Accuracy: 555/1000\n",
      "Component 1 Accuracy: 548/1000\n",
      "Component 2 Accuracy: 551/1000\n",
      "Component 3 Accuracy: 545/1000\n",
      "Component 4 Accuracy: 563/1000\n",
      "Component 5 Accuracy: 560/1000\n",
      "Component 6 Accuracy: 575/1000\n",
      "Component 7 Accuracy: 558/1000\n",
      "Component 8 Accuracy: 534/1000\n",
      "Component 9 Accuracy: 542/1000\n",
      "Posterior Mean Accuracy: 546/1000\n",
      "Ensemble Accuracy: 548/1000\n"
     ]
    }
   ],
   "source": [
    "#matplotlib inline\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import VMF\n",
    "\n",
    "import importlib\n",
    "importlib.reload(VMF)\n",
    "\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPUs are used!\")\n",
    "else:\n",
    "    print(\"CPUs are used!\")\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "batch_size = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "epochs = 250\n",
    "pepochs = 50\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "\n",
    "TRAIN_SIZE = len(tr_ids)\n",
    "TEST_SIZE = len(te_ids)\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "NUM_TEST_BATCHES = len(te_ids)/BATCH_SIZE\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "l1shape=(256, 3)\n",
    "l2shape=(3, 3)\n",
    "l3shape=(3, 5)\n",
    "\n",
    "\n",
    "epochs = 25\n",
    "trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net = VMF.BayesianNetwork(l1=l1shape, l2=l2shape, l3=l3shape,BN='batchnorm').to(DEVICE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = VMF.train(net, optimizer, epoch, i)\n",
    "        print(net.l1.weight_mu.mean())\n",
    "\n",
    "    res = VMF.test_ensemble(net)\n",
    "\n",
    "    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f26701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nw_mu4 = net.l4.weight_mu\\nw_mu4 = w_mu4.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\\n#net.l3.weight_rho\\nb_mu4 = net.l4.bias_mu.to(DEVICE) #5\\n#net.l3.bias_rho\\n\\nw_mu5 = net.l5.weight_mu\\nw_mu5 = w_mu5.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\\n#net.l3.weight_rho\\nb_mu5 = net.l5.bias_mu.to(DEVICE) #5\\n#net.l3.bias_rho\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_mu1 = net.l1.weight_mu\n",
    "w_mu1 = w_mu1.reshape(l1shape[0]*l1shape[1]).to(DEVICE)\n",
    "#net.l1.weight_rho\n",
    "b_mu1 = net.l1.bias_mu.to(DEVICE) #400\n",
    "#net.l1.bias_rho\n",
    "\n",
    "w_mu2 = net.l2.weight_mu\n",
    "w_mu2 = w_mu2.reshape(l2shape[0]*l2shape[1]).to(DEVICE)\n",
    "#net.l2.weight_rho\n",
    "b_mu2 = net.l2.bias_mu.to(DEVICE) #600\n",
    "#net.l2.bias_rho\n",
    "\n",
    "w_mu3 = net.l3.weight_mu\n",
    "w_mu3 = w_mu3.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu3 = net.l3.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho\n",
    "r\"\"\"\n",
    "w_mu4 = net.l4.weight_mu\n",
    "w_mu4 = w_mu4.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu4 = net.l4.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho\n",
    "\n",
    "w_mu5 = net.l5.weight_mu\n",
    "w_mu5 = w_mu5.reshape(l3shape[0]*l3shape[1]).to(DEVICE)\n",
    "#net.l3.weight_rho\n",
    "b_mu5 = net.l5.bias_mu.to(DEVICE) #5\n",
    "#net.l3.bias_rho\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0470924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "Classes loaded\n",
      "GPUs are used!\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uio/hume/student-u44/lmsunde/projects/BNN/AliaksandrFolder/FVMF.py:277: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  v0 = torch.cat([v0, torch.tensor(w0[det >= 0]).to(DEVICE)])\n",
      "/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(122.3385, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(162.3318, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0079, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "2\n",
      "tensor(118.0130, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(158.7774, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0088, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "3\n",
      "tensor(119.0853, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.3147, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0103, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "4\n",
      "tensor(112.9642, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(152.8757, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0122, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "5\n",
      "tensor(120.0318, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.2368, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0140, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "6\n",
      "tensor(117.4663, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(157.5623, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0157, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "7\n",
      "tensor(119.0059, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.2262, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0170, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "8\n",
      "tensor(119.0551, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.3597, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0180, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "9\n",
      "tensor(121.9343, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(161.8433, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0187, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "10\n",
      "tensor(117.9864, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(158.2387, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0200, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "11\n",
      "tensor(123.3589, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(164.0261, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0218, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "12\n",
      "tensor(113.6904, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(153.5849, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0230, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "13\n",
      "tensor(120.6513, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.7514, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0244, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "14\n",
      "tensor(114.3040, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(154.7461, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0257, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "15\n",
      "tensor(113.2070, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(153.5328, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0275, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "16\n",
      "tensor(113.6635, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(154.1881, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0296, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "17\n",
      "tensor(111.8841, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(151.6349, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0310, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "18\n",
      "tensor(106.9042, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(146.9905, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0321, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "19\n",
      "tensor(118.4880, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(158.5528, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0334, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "20\n",
      "tensor(116.8015, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(157.2086, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0347, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "21\n",
      "tensor(118.4906, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(158.4384, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0363, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "22\n",
      "tensor(116.9126, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(157.1267, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0379, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "23\n",
      "tensor(109.9904, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(149.8583, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0391, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "24\n",
      "tensor(114.8436, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(154.6295, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0402, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "25\n",
      "tensor(116.3808, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(156.5643, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0416, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "26\n",
      "tensor(117.1085, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(157.0126, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0431, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "27\n",
      "tensor(110.6750, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(150.5180, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0444, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "28\n",
      "tensor(120.0678, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(160.3801, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0461, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "29\n",
      "tensor(115.9734, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(156.0195, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0473, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "30\n",
      "tensor(110.9296, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(150.7592, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0486, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "31\n",
      "tensor(118.4594, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(158.5996, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0492, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "32\n",
      "tensor(110.1424, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(150.3203, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0501, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "33\n",
      "tensor(105.4900, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(145.5652, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0510, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "34\n",
      "tensor(104.0404, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(143.8587, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0522, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "35\n",
      "tensor(119.0801, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.3410, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0534, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "36\n",
      "tensor(111.8326, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(152.3352, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0547, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "37\n",
      "tensor(108.5900, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(149.0579, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0556, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "38\n",
      "tensor(111.7894, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(151.8774, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0567, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "39\n",
      "tensor(104.2852, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(144.0956, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0577, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "40\n",
      "tensor(109.3404, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(149.4782, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0585, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "41\n",
      "tensor(111.9539, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(152.1352, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0593, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "42\n",
      "tensor(115.0554, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(155.0051, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0597, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "43\n",
      "tensor(106.1295, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(146.1172, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0601, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "44\n",
      "tensor(105.2089, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(145.4017, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0605, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "45\n",
      "tensor(119.7039, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(159.8163, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0609, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "46\n",
      "tensor(99.8544, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(140.0115, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0613, device='cuda:1', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "tensor(100.6689, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(140.6693, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0618, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "48\n",
      "tensor(101.4197, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(141.9912, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0622, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "49\n",
      "tensor(103.1441, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(143.3124, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0625, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "50\n",
      "tensor(105.5462, device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor(145.3585, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0629, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "Component 0 Accuracy: 362/1000\n",
      "Component 1 Accuracy: 382/1000\n",
      "Component 2 Accuracy: 346/1000\n",
      "Component 3 Accuracy: 361/1000\n",
      "Component 4 Accuracy: 390/1000\n",
      "Component 5 Accuracy: 332/1000\n",
      "Component 6 Accuracy: 352/1000\n",
      "Component 7 Accuracy: 375/1000\n",
      "Component 8 Accuracy: 342/1000\n",
      "Component 9 Accuracy: 313/1000\n",
      "Posterior Mean Accuracy: 341/1000\n",
      "Ensemble Accuracy: 406/1000\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import os\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "\n",
    "import VMF\n",
    "\n",
    "import importlib\n",
    "importlib.reload(VMF)\n",
    "\n",
    "prefix = \"_phoneme_bg_\"\n",
    "# define the summary writer\n",
    "writer = SummaryWriter()\n",
    "sns.set()\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "\n",
    "# select the device\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print(\"GPUs are used!\")\n",
    "else:\n",
    "    print(\"CPUs are used!\")\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "batch_size = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 5\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "epochs = 250\n",
    "pepochs = 50\n",
    "\n",
    "#prepare the data\n",
    "data = pd.read_csv('http://www.uio.no/studier/emner/matnat/math/STK2100/data/phoneme.data')\n",
    "data = data.drop(columns=[\"row.names\"])\n",
    "data = pd.concat([data,data.g.astype(\"category\").cat.codes.astype(int)],sort=False, axis=1) #get_dummies(data['g'], prefix='phoneme')],sort=False, axis=1)\n",
    "data = data.drop(columns=[\"g\",\"speaker\"])\n",
    "data = data.values\n",
    "\n",
    "\n",
    "np.random.seed(40590)\n",
    "\n",
    "tr_ids = np.random.choice(4509, 3500, replace = False)\n",
    "te_ids = np.setdiff1d(np.arange(4509),tr_ids)[0:1000]\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "\n",
    "data_mean = dtrain.mean(axis=0)[0:256]\n",
    "data_std = dtrain.std(axis=0)[0:256]\n",
    "\n",
    "data[:,0:256] = (data[:,0:256]  - data_mean)/data_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtrain = data[tr_ids,:]\n",
    "dtest = data[te_ids,:]\n",
    "\n",
    "\n",
    "TRAIN_SIZE = len(tr_ids)\n",
    "TEST_SIZE = len(te_ids)\n",
    "NUM_BATCHES = TRAIN_SIZE/BATCH_SIZE\n",
    "NUM_TEST_BATCHES = len(te_ids)/BATCH_SIZE\n",
    "\n",
    "# set prior parameters\n",
    "PI = 1\n",
    "SIGMA_1 = torch.cuda.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.cuda.FloatTensor([math.exp(-6)])\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "trtimes  = np.zeros(epochs)\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net2 = FVMF.BayesianNetwork(w_mu1 = w_mu1, w_mu2 = w_mu2, w_mu3 = w_mu3, b_mu1 = b_mu1, b_mu2 = b_mu2, b_mu3 = b_mu3, l1=l1shape, l2=l2shape, l3=l3shape, VD='vmf')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #    p.requires_grad_(False)\n",
    "    #    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #    p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net2.parameters(), lr=0.0002)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        trtimes[epoch] = FVMF.train(net2, optimizer, epoch, i)\n",
    "        print(net2.l1.weight_mu.mean())\n",
    "\n",
    "    res = FVMF.test_ensemble(net2)\n",
    "\n",
    "    np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(20,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "What if the problem is that since the mu's in the Gaussian is (out,in), and the vMF is out*in,\n",
    "this could mean that we have \"strethed\" a single vMF pdf over all the parameters, while in the Gaussian we have made one for each output?\n",
    "\n",
    "I don't know, also intuitively this should be the case, since the whole point of the vMF is the norm 1, which obviously\n",
    "will require that it for each forward-pass is only one massive pdf for all inputs and outputs.\n",
    "\n",
    "Ok, this can get rough, I will try my best to reshape the w_mu's and b_mu's the best I can, perhaps this will work just fine without too much tuning...\n",
    "Hopefully.\n",
    "\n",
    "This doe not seem to ave worked. I don't konw exactly what the problem is. Per haps it is a good idea to consult the new loss function\n",
    "suggested in the paper that made the code we based our vMF on?\n",
    "\n",
    "It is very strange that the loss is not at all affected by completely ridiculous learning rates...\n",
    "\n",
    "Remember that vMF makes the norm of the weights and biases 1, not the forward pass of the x's. Hence the advantage is that the gradient\n",
    "will not explode, since the backward pass of it will also be approx. 1. In batchnorm, maybe the gradient can explode? Since the weights \n",
    "can be whatever?\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
