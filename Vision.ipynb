{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a05b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are used!\n",
      "FVMF RELOADED\n",
      "GPUs are used!\n",
      "FVMF RELOADED\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import time\n",
    "import mpmath\n",
    "import os\n",
    "import importlib\n",
    "import importlib\n",
    "import FVMF\n",
    "importlib.reload(FVMF)\n",
    "import test_ensemble\n",
    "importlib.reload(test_ensemble)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define the parameters\n",
    "BATCH_SIZE = 100\n",
    "TEST_BATCH_SIZE = 100\n",
    "COND_OPT = False\n",
    "CLASSES = 10\n",
    "# TRAIN_EPOCHS = 250\n",
    "SAMPLES = 1\n",
    "TEST_SAMPLES = 10\n",
    "TEMPER = 0.001\n",
    "TEMPER_PRIOR = 0.001\n",
    "pepochs = 50\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "LOADER_KWARGS = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "cuda = torch.cuda.set_device(1)\n",
    "\n",
    "\n",
    "#d_CIFAR_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "#d_CIFAR_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "#print(transforms.ToTensor(cifar_train))\n",
    "#print(dtrain[0].shape)\n",
    "\n",
    "#Ok so this is actually not trivial... Maybe we need to place a convulutional layer first in order to properly parse this into\n",
    "#our vMF so that we don't lose the positionality of our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "872a4e6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     train_data, train_target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(DEVICE), target\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#print(train_data.shape)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     22\u001b[0m     test_data, test_target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(DEVICE), target\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1027\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/multiprocessing/context.py:281\u001b[0m, in \u001b[0;36mForkProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_fork\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/uio/modules/rhel8/easybuild/software/Miniconda3/lmsunde/envs/BNN2/lib/python3.10/multiprocessing/popen_fork.py:66\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m parent_r, child_w \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpipe()\n\u001b[1;32m     65\u001b[0m child_r, parent_w \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpipe()\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        './mnist', train=True, download=True,\n",
    "        transform=transforms.ToTensor()),\n",
    "    batch_size=30000, shuffle=True, **LOADER_KWARGS)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        './mnist', train=False, download=True,\n",
    "        transform=transforms.ToTensor()),\n",
    "    batch_size=10000, shuffle=False, **LOADER_KWARGS)\n",
    "\n",
    "TRAIN_SIZE = len(train_loader.dataset)\n",
    "TEST_SIZE = len(test_loader.dataset)\n",
    "NUM_BATCHES = len(train_loader)\n",
    "NUM_TEST_BATCHES = len(test_loader)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    train_data, train_target = data.to(DEVICE), target.to(DEVICE)\n",
    "#print(train_data.shape)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    test_data, test_target = data.to(DEVICE), target.to(DEVICE)\n",
    "#test_data = torch.cat\n",
    "\n",
    "#dtrain Må pakke den inn i en dimensjon... kanskje?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1fdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape,test_data.shape)\n",
    "print(train_target.shape,test_target.shape)\n",
    "\n",
    "dtrain = torch.flatten(train_data, start_dim=1, end_dim= -1)\n",
    "#print(dtrain.shape)\n",
    "\n",
    "dtest = torch.flatten(test_data, start_dim=1, end_dim= -1)\n",
    "#print(dtest.shape)\n",
    "\n",
    "#print(train_target.shape)\n",
    "#dtrain = torch.cat((dtrain,train_target), dim = 0)\n",
    "\n",
    "train_target1 = torch.reshape(train_target,(30000, 1))\n",
    "#print(train_target1.shape)\n",
    "\n",
    "dtrain = torch.cat((dtrain,train_target1), dim = 1).numpy(force=True)\n",
    "print(dtrain.shape)\n",
    "\n",
    "test_target1 = torch.reshape(test_target,(10000, 1))\n",
    "#print(test_target1.shape)\n",
    "\n",
    "dtest = torch.cat((dtest,test_target1), dim = 1).numpy(force=True)\n",
    "print(dtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3909fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1shape=(784,10)\n",
    "l2shape=(10, 10)\n",
    "l3shape=(10, 10)\n",
    "#l4shape=(10, 10)\n",
    "#layershapes = [l1shape, l2shape, l3shape, l4shape]\n",
    "layershapes = [l1shape, l2shape, l3shape]\n",
    "data_shape = (0,784,784,785)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d511988",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "trtimes  = []\n",
    "# make inference on 10 networks\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net = FVMF.BayesianNetwork(layershapes=layershapes,BN='notbatchnorm',VD='Gaussian',\n",
    "                               dtrain=dtrain,dtest=dtest,BATCH_SIZE = 100,classification = 'classification',VISION = 'dense').to(DEVICE)\n",
    "    #net = VMF.BayesianNetwork(l1=l1shape, l2=l2shape, l3=l3shape,l4=l4shape,BN='notbatchnorm').to(DEVICE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.007)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train = FVMF.train(net, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,shape = data_shape,CLASSES = CLASSES)\n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        #print(net.l1.weight_mu.mean())\n",
    "\n",
    "    res = test_ensemble.test_ensemble(net,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "\n",
    "\n",
    "x = []\n",
    "for i in range(epochs):\n",
    "    x.append(i+1)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Gaussian loss, 10 epoch MNIST classification with half the trainingset')\n",
    "plt.plot(x,trtimes)\n",
    "plt.savefig('Plots/Gaussian_loss_10_epoch_MNIST_VISION.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586fc754",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train = FVMF.train(net, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,shape = data_shape,CLASSES = CLASSES)\n",
    "    trtimes.append(train[1].detach().cpu().numpy())\n",
    "    #print(net.l1.weight_mu.mean())\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    x.append(10+epochs)\n",
    "res = test_ensemble.test_ensemble(net,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Gaussian loss, 30 epoch MNIST classification with half the trainingset')\n",
    "plt.plot(x,trtimes)\n",
    "plt.savefig('Plots/Gaussian_loss_30_epoch_MNIST_VISION.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa383f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train = FVMF.train(net, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,shape = data_shape,CLASSES = CLASSES)\n",
    "    trtimes.append(train[1].detach().cpu().numpy())\n",
    "    #print(net.l1.weight_mu.mean())\n",
    "\n",
    "res = test_ensemble.test_ensemble(net,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    x.append(30+1)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Gaussian loss, 50 epoch MNIST classification with half the trainingset')\n",
    "plt.plot(x,trtimes)\n",
    "plt.savefig('Plots/Gaussian_loss_50_epoch_MNIST_VISION.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(FVMF)\n",
    "epochs = 10\n",
    "trtimes = []\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    torch.manual_seed(i)\n",
    "    net2 = FVMF.BayesianNetwork(#w_mu = w_mu, b_mu = b_mu, \n",
    "                                #w_mu = None, b_mu = None,\n",
    "                                #w_mu = w_mu_nodewise, b_mu = b_mu,\n",
    "                                layershapes = layershapes,\n",
    "                                dtrain=dtrain, dtest=dtest,\n",
    "                                VD='vmf',\n",
    "                                b_kappa=torch.Tensor(1).uniform_(2,2.1),\n",
    "                                w_kappa=torch.Tensor(1).uniform_(2.5,2.6),\n",
    "                                Temper = 1, normalize = 'No',classification = 'classification',VISION = 'dense')\n",
    "    \n",
    "    #for j,p in enumerate(net2.l1.parameters()):    \n",
    "    #p.requires_grad_(False)\n",
    "    \n",
    "    #for j,p in enumerate(net2.l2.parameters()):\n",
    "    #p.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.Adam(net2.parameters(), lr=0.14)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train = FVMF.train(net2, dtrain, SAMPLES, optimizer, epoch, i,BATCH_SIZE = 100,shape = data_shape,CLASSES = CLASSES)\n",
    "        trtimes.append(train[1].detach().cpu().numpy())\n",
    "        \n",
    "        print('max:',net2.weight_mu[1].max())\n",
    "        print('norm:',torch.norm(net2.weight_mu[1]))\n",
    "\n",
    "    res = test_ensemble.test_ensemble(net2,dtest,TEST_SAMPLES,TEST_BATCH_SIZE,BATCH_SIZE,CLASSES,DEVICE,shape = data_shape)\n",
    "\n",
    "    #np.savetxt(\"soundGmaccuracies_\" + str(i) + \".csv\", res, delimiter=\",\")\n",
    "plt.title('vMF loss, 10 epoch vision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(x,trtimes)\n",
    "plt.savefig('Plots/vMF_loss_10_epoch_vision.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = [1,2,3,4,5]\n",
    "print(g[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd919d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Just use a Gaussian first layer for the vMF?\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
